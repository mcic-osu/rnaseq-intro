[
  {
    "objectID": "modules/A04_shell2.html#overview-setting-up",
    "href": "modules/A04_shell2.html#overview-setting-up",
    "title": "The Unix Shell - Part II",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nIn this session, we will continue learning about the Unix shell. Specifically, we will focus on:\n\nCommands for file and directory organization, such as copying, moving, renaming, and deleting\nSelecting multiple files with wildcards (“globbing”)\nViewing text files\nRedirecting the output of commands to files\nKeyboard shortcuts, counting lines, and environment variables\n\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nFor now, your working directory doesn’t matter — we’ll change it in a bit."
  },
  {
    "objectID": "modules/A04_shell2.html#some-commands-and-features-to-get-you-up-to-speed",
    "href": "modules/A04_shell2.html#some-commands-and-features-to-get-you-up-to-speed",
    "title": "The Unix Shell - Part II",
    "section": "1 Some commands and features to get you up to speed",
    "text": "1 Some commands and features to get you up to speed\nWe’ll start with a few miscellaneous shell commands and features that we’ll either use again later on in today’s session, or that are just good to know about sooner rather than later.\n\n1.1 Counting lines with wc -l\nThe wc (short for wordcount) command can count things like the number of characters, words, and lines in a text file. Getting a line count for files is often especially useful, certainly for genomic data files: for example, the number of reads in a FASTQ file is simply the number of lines divided by 4, and other file types like GFF (an annotation format) have one entry per line.\nTo get just the line count, we should use the -l option of wc — let’s count the number of lines in the metadata file in /fs/ess/PAS0471/demo/202307_rnaseq/:\nwc -l /fs/ess/PAS0471/demo/202307_rnaseq/metadata/meta.tsv\n9 /fs/ess/PAS0471/demo/202307_rnaseq/metadata/meta.tsv\n\n\n\n1.2 Keyboard shortcuts\nUsing keyboard shortcuts can make your life in the Unix shell a lot easier. Below, we’ll see a couple particularly useful ones. For more, see the keyboard shortcuts reference page on this website.\n\nCommand history & faster cursor movement\nIt’s common to re-execute command line expressions, and even more common to make slight changes to a previously executed one. To cycle through your command history and retrieve previous commands1 you can ⇧ and ⇩.\nIf you hit the ⇧ (up arrow) once, you’ll get your most recent command, and if you keep hitting it, you’ll go further back. The⇩ (down arrow) will go the other way: towards the present.\nFor example, say that we wanted to see what the output of the wc command is when we omit the -l option. At the prompt, press ⇧ once to retrieve your previous wc command. Then, edit it to remove the -l, and press Enter:\nwc /fs/ess/PAS0471/demo/202307_rnaseq/metadata/meta.tsv\n9  27 236 /fs/ess/PAS0471/demo/202307_rnaseq/metadata/meta.tsv\nA few other tips while you’re trying this — perhaps practice this a few times:\n\nTo execute a command, you can press Enter regardless of where your cursor is on the line; it does not need to be at the end of the line.\nAfter pressing ⇧, you needed to move to almost the beginning of the line to make your edit (removing -l). There are two ways to speed up your cursor movements:\n\nIf you keep Ctrl (Windows) or Option (Mac) pressed while using the left (and right) arrows to move your cursor on the line, you will move one “word”, rather than one character, at a time!\nIf you press Ctrl+A, you will move all the way to the beginning of the line (and Ctrl+E will move you to the end of the line).\n\n\n\n\nUse man wc or wc --help to figure out what the three counts mean\n\n\nSolution\n\nThe three numbers returned by wc when we don’t give it any options are: the number of lines, words, and bytes, respectively.\nSometimes you might want to get the number of characters, which you can do with wc -c.\n\n\n\nCancel and kill\nIt is also common to want to cancel a running command or get out of a “hanging” prompt, and you can do so with Ctrl+C.\nIf you accidentally omit a file name with a command like wc that requires an argument2, something odd happens — you move to a new line in the terminal but don’t get your regular prompt back; it appears to “hang”. Try it:\nwc\nTo get out of this, press Ctrl+C to cancel the partial command, and get your prompt back. There are several other ways you might accidentally try to execute incomplete commands, such as forgetting to close parentheses or quotes, and in some cases, you don’t even know what the problem is. Just press Ctrl+C and then take a close look at your command to find the problem.\nAnother type of situation where Ctrl+C is useful is when you execute a command that does actually run, but you realize, for example, that there is a mistake in the command (wrong input file!), or that the command will take way too long to run interactively. Ctrl+C will then “kill” (cancel) the running process. A somewhat silly example is the sleep command, which you can use to make the computer wait between successive commands:\n# This will \"run\" for 60 seconds, after which you get your prompt back \nsleep 60s\n# Press Ctrl + C to get your prompt back sooner!\n^C\n\n\n\n\n1.3 Environment variables and the echo command\nIn computer languages, “variables” store information. When we reference a shell variable to retrieve its value, we have to put a $ in front of its name (unlike in say R or Python) — for instance, we reference a variable with the name x as $x3.\nIn later sessions, we’ll see how we can create our own variables, and we will be using these a lot! But the Unix shell also has a number of pre-existing variables, which are called environment variables and have ALL-CAPS names. For example, the variable $HOME contains the path to your Home directory, and the variable $USER contains your username.\nHow can we examine the value of variables? Just typing, e.g., $USER at the prompt will not work: as pointed out in the previous session, every command line expression should start with a command, and a variable is not a command. Instead, we should use the echo command, which simply prints whatever text you give it:\n# You don't have to use quotes (\" \"), at least here, but it's good practice\necho \"Hello there\"    \nHello there\nWe’ll also be using the echo command a lot, such as by letting our shell scripts report progress and the values of variables. If we use echo with a(n environment) variable like $USER, the variable’s value will be printed:\n# The shell will replace $USER by its value, so echo will just see 'jelmer'\necho \"$USER\"\njelmer\n\nYour Turn: echo and environment variables\n\nPrint “Hello there &lt;user&gt;” to the screen\n(i.e., &lt;user&gt; should be replaced by your actual username).\nUse an environment variable mentioned above to move to your Home directory, and then move back to where you were (as seen last time).\n\n\n\nHints for the second question (click here)\n\nRecall from the previous session that:\n\nThe command cd will change your working directory\ncd - will move back to your previous directory\n\n\n\n\nSolutions (click here)\n\n\nPrint “Hello there &lt;user&gt;” to the screen:\n\n# (This would also work without the \" \" quotes)\necho \"Hello there $USER\"\nHello there jelmer\n\n\nUse an environment variable to move to your Home directory, check that it worked, then move back to where you were:\n\n# Move to your Home directory\ncd $HOME\n\n# Print the working directory\npwd\n#&gt; /users/PAS0471/jelmer\n\n# Move back to your previous working directory\ncd -"
  },
  {
    "objectID": "modules/A04_shell2.html#commands-for-file-organization",
    "href": "modules/A04_shell2.html#commands-for-file-organization",
    "title": "The Unix Shell - Part II",
    "section": "2 Commands for file organization",
    "text": "2 Commands for file organization\nBelow, we’ll learn about the key commands for directory and file organization: mkdir (create dirs), cp (copy), mv (move and rename), and rm (remove). But first off, why do we need to learn this when VS Code and the OnDemand website itself have file browsers with this kind of functionality?\n\nWe’ll regularly want to use these commands non-interactively in our shell scripts\nThese commands are often a more efficient way to organize files\nThey provide a good way to get more familiar with the Unix shell and how Unix commands work\nIt’s good to know these basics and be prepared for situations where you do not have access to OnDemand/VS Code.\n\n\n2.1 Create dirs with mkdir (and files with touch)\nThe mkdir command creates new directories: you can give it one or more arguments to create one or more directories. Create one for yourself, with your username, in /fs/ess/PAS0471:\nmkdir /fs/ess/PAS0471/$USER\nNow, move to that directory:\ncd /fs/ess/PAS0471/$USER\nNext, we’ll create a directory for these introductory sessions and go there:\nmkdir rnaseq_intro\ncd rnaseq_intro       # Use Tab completion\nLet’s switch VS Code to this directory: click      =&gt;   File   =&gt;   Open Folder, and select /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro. You’ll have to open a Terminal window again, too.\n\n\n\n\n\n\nTip\n\n\n\n/fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro (with &lt;user&gt; replaced by your username) is going to be your working dir for upcoming sessions too, so you should always open/reload VS Code in that dir.\n\n\nFinally, let’s make several dirs at once — this is how I usually structure my research project directories:\nmkdir data metadata results scripts sandbox\n\nRecursive mkdir\nBy default, mkdir does not work recursively: it will refuse to make a dir inside a dir that does not (yet) exist. And if you try to do so, the resulting error might confuse you:\nmkdir sandbox/2023/08\nmkdir: cannot create directory ‘sandbox/2023/08’: No such file or directory\n\nWhy won’t you do your job, mkdir!?\n\nInstead, we need to use the -p option to mkdir:\nmkdir -p sandbox/2023/08\nIn general, Unix commands are designed to be powerful and to do exactly what you asked, but non-recursive default behaviors (we will see this with the commands to copy (cp) and delete (rm) files as well) are an exception to this. We can assume that the developers thought that recursive default behaviors were just too risky.\n\n\n\n\n\n\nWhen a dir already exists, -p will also suppress any errors (Click to expand)\n\n\n\n\n\nThe -p also changes mkdir’s behavior when you try to create a dir that already exists.\nBy default, it will give an error:\n# Note: we (should have) just created this dir above, so it already exists\nmkdir sandbox/2023/08\nmkdir: cannot create directory ‘sandbox/2023/08’: File exists\nWith -p, it doesn’t complain (but won’t do anything, like overwriting, either):\nmkdir -p sandbox/2023/08    # Should not give any output\nAs we’ll see later, all of this makes mkdir -p ideal for non-interactive usage in shell scripts.\n\n\n\n\n\nCreate new files with touch\nAs we have learned the command to make new dirs, you may be wondering what the equivalent command is to make new, empty files: it is touch.\n# Create three new, empty files:\ntouch file1 file2 file3\nThis command isn’t as commonly used as mkdir because we usually create new files on the fly with command output redirection, which we’ll learn about below. But it’s certainly useful to create dummy files to practice your shell skills.4\n\n\n\n\n2.2 Copy files and dirs with cp\nAbove, you created your own directory — now, let’s get you a copy of the partial RNAseq dataset in /fs/ess/PAS0471/demo/202307_rnaseq/ that we’ll be working with throughout these sessions.\nThe cp command copies files and/or directories from one location to another. It has two required arguments: what you want to copy (the source), and where you want to copy it to (the destination). So, we can summarize its basic syntax as cp &lt;source&gt; &lt;destination&gt;.\nWe’ll start by copying a file with some metadata on the RNAseq samples:\ncp -v /fs/ess/PAS0471/demo/202307_rnaseq/metadata/meta.tsv metadata/\n‘/fs/ess/PAS0471/demo/202307_rnaseq/metadata/meta.tsv’ -&gt; ‘metadata/meta.tsv’\nThis told cp to copy the metadata file into the metadata dir you created earlier.\n\n\n\n\n\n\n-v option for verbose\n\n\n\nWe used the -v option, short for verbose, to make cp tell us what it did:\n\nRecall that by default, Unix commands are generally silent when performing actions like this.\nBut many commands have a verbose option (nearly always -v) to make them tell us what they did, which can be very useful.\n\n\n\nLike mkdir, cp is not recursive by default. If you want to copy a directory and all of its contents, we need to use its -r option5. We’ll use that option to copy the dir with FASTQ files:\ncp -rv /fs/ess/PAS0471/demo/202307_rnaseq/data/fastq data/\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq’ -&gt; ‘data/fastq’\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/Miapaca2_A178V_R1.fastq.gz’ -&gt; ‘data/fastq/Miapaca2_A178V_R1.fastq.gz’\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/ASPC1_G31V_R2.fastq.gz’ -&gt; ‘data/fastq/ASPC1_G31V_R2.fastq.gz’\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/ASPC1_A178V_R2.fastq.gz’ -&gt; ‘data/fastq/ASPC1_A178V_R2.fastq.gz’\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/ASPC1_G31V_R1.fastq.gz’ -&gt; ‘data/fastq/ASPC1_G31V_R1.fastq.gz’\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/ASPC1_A178V_R1.fastq.gz’ -&gt; ‘data/fastq/ASPC1_A178V_R1.fastq.gz’\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/Miapaca2_A178V_R2.fastq.gz’ -&gt; ‘data/fastq/Miapaca2_A178V_R2.fastq.gz’\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/Miapaca2_G31V_R1.fastq.gz’ -&gt; ‘data/fastq/Miapaca2_G31V_R1.fastq.gz’\n‘/fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/Miapaca2_G31V_R2.fastq.gz’ -&gt; ‘data/fastq/Miapaca2_G31V_R2.fastq.gz’\nLet’s get an overview of the files we now have in our rnaseq_intro dir:\ntree -C                 # Your own output should have colors\n.\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n├── results\n├── sandbox\n│   └── 2023\n│       └── 08\n└── scripts\n\n8 directories, 9 files\n\n\n\n2.3 Moving with mv, and general cp/mv tips\nThe mv command is nearly identical to the cp command, except that it:\n\nMoves rather than copies files and/or dirs\nWorks recursively by default\n\nWe’ll use this section to learn a few more things about the usage of both these commands.\n\nBy default, both mv and cp will overwrite files without warning! Use the -i (for interactive) option to make it let you confirm before overwriting anything (that option also exists for rm, where you’ll practice with it).\nThere is no separate command for renaming, because both cp and mv allow you to provide a different name for the target. Thus, if used as follows, mv functions merely as a renamer:\nmv meta.tsv meta_version2.tsv\nAnd we can move and rename at the same time as well — let’s do that to restore our original location and name for the metadata file:\nmv meta_version2.tsv metadata/meta.tsv\nUse the . notation for the current working dir if you want to copy/move something there:\ncp -v /fs/ess/PAS0471/demo/202307_rnaseq/README.md .\n‘/fs/ess/PAS0471/demo/202307_rnaseq/README.md’ -&gt; ‘README.md’\n\n\n\n\n\n\n\nRenaming rules for both cp and mv\n\n\n\nIf the destination is:\n\nAn existing dir, the file(s) will keep their original names.\nNot an existing dir, the path specifies the new name of the file or dir, depending on what the source is.\nHowever, neither command will create “additional, intermediate” directories, so mv metadata/meta.tsv metadata/misc/meta.tsv would not work if metadata/misc doesn’t already exist.\n\n\n\n\nYour Turn: Practice with mv\nIn which directory (in terms of a relative path from your working dir) would the FASTQ files end up with each of the following commands?\n\nmv data/fastq data/fastq_files\nmv data/fastq fastq\nmv data/fastq .\n\nWhat if you wanted to move the FASTQ files directly into your current working directory (from data/fastq)?\n(You should know how to do this 1-by-1 but we have not yet seen the special character you can use to do this for all files at once — see the hint.)\n\n\nHint for the last question (click here)\n\nYou can use the asterisk (star) symbol * to select all files in a directory.\n(We’ll learn more about * later. In short, it is one of a few shell wildcards used to expand to filenames, and this one specifically matches any number of any character, and as such will match all possible filenames when used by itself.)\n\n\n\nSolutions (click here)\n\nIn which directory (in terms of relative path from your working dir) will the FASTQ files end up with each of the following commands?\n\nmv data/fastq data/fastq_files — in the dir fastq_files (we’ve really just renamed the dir fastq to fastq_files)\nmv data/fastq fastq — in fastq (because our source is a dir, so is the destination)\nmv data/fastq . — in fastq also! (we’d need the syntax shown below to move the individual files directly into our current dir)\n\nWhat if you wanted to move the FASTQ files directly into your current working directory?\nFor one file:\nmv data/fastq/ASPC1_A178V_R1.fastq.gz .\nFor all files:\nmv data/fastq/* .\n\n\n\n\n\n2.4 Intermezzo: the trouble with spaces in file names\nIn the Unix shell, spaces are essentially special characters that are used to separate commands, options, and arguments — with the latter often being one or more file names.\nFor example, one of our earlier mkdir commands, mkdir data metadata results scripts sandbox, created five separate dirs instead of one dir with four spaces in it.\nTherefore, to create or refer to a file or directory name with a space in it, you need some special notation. Two different methods are available: quoting the entire file name, and “escaping” the special meaning of the space:\n# Method 1: Put quotes around the entire file name:\nmkdir \"my new dir\"\n\n# Method 2: Escape the special meaning of spaces by preceding them with a backslash\nmkdir my\\ new\\ dir2\nLet’s see what we have now:\nls -1    # The \"-1\" (dash one) option to ls lists each entry on its own line\ndata\nmetadata\nmy new dir\nmy new dir2\nIn any situation where we refer to file or dir names that contain spaces, we’ll need quotes or escaping — consider this failing command:\ncd my new dir\nbash: cd: my: No such file or directory\nSimilarly, ls will fail as follows:\nls my new dir\nls: cannot access my: No such file or directory\nls: cannot access new: No such file or directory\nls: cannot access dir: No such file or directory\n\nWhy does cd produce one error while ls produces three?\n\n\nSolution (click here)\n\ncd tried to move to non-existing dir my, and ignored the arguments new and dir, since it only accepts one argument.\nls accepts multiple arguments and will try to list all 3 listed items, none of which happens to exist.\n\n\nFortunately, tab completion will insert backspaces for you — type cd my and press Tab:\n# After typing cd and pressing tab, the line should complete as follows:\ncd my\\ new\\ dir\n\n# Let's move back up:\ncd ..\nNevertheless, the moral of the story is that you should always avoid spaces in file names when working at OSC. In fact, you should do so in any project that uses the Unix shell or any coding. If you need to separate words, then use underscores (_), dashes (-), and/or capitalization. For example:\nmkdir my_new_dir     # \"snake\" case\nmkdir my-new-dir     # \"kebab\" case\nmkdir myNewDir       # \"camel\" case\n\n\n\n\n\n\nMore file and dir naming tips\n\n\n\nIt’s also a good idea to avoid characters other than letters, numbers, and underscores (_), dashes (-), and periods (.) in dir and file names.\nIdeally, you’d only use the . to separate the extension (i.e., the abbreviation that indicates the type of file, like txt or tsv) from the rest of the file name, but it is (unfortunately) also common to use . as a “word separator” — e.g. my_file.filtered.sorted.txt.\nFinally, avoid using a number as the first character in a file or dir name.\n\n\n\n\n\n2.5 Removing files with rm\nOur final command in this series is rm, which removes (deletes) files and directories.\nOne important thing to note upfront is that rm will permanently and irreversibly delete files without the typical “intermediate step” of placing them in a trash bin, like you are used to with your personal computer. With a healthy dosis of fear installed, let’s dive in.\nTo remove one or more files, you can simply pass the file names as arguments to rm as with previous commands. We will also use the -v (verbose) option to have it tell us what it did:\n# This assumes you ran `touch file1 file2 file3` earlier on; do so now if needed\nrm -v file1            # Remove 1 file\nremoved ‘file1’\nrm -v file2 file3      # Remove 2 files\nremoved ‘file2’\nremoved ‘file3’\n(If it seems cumbersome to type a bunch of file names, we’ll see below how you can use a Shell wildcard to select many files at once.)\n\nRecursive rm\nAs a safety measure, rm will by default only delete files and not directories or their contents — i.e., like mkdir and cp, it refuses to act recursively by default. To remove dirs and their contents, use the -r option:\n# You should have these from 2.4 (if not, run: mkdir \"my new dir\" \"my new dir2\")\nrm -r \"my new dir\" \"my new dir2\"\nremoved directory: ‘my new dir’\nremoved directory: ‘my new dir2’\nYou should obviously be quite careful with rm -r! If you’d run it on say your personal dir within PAS0471, /fs/ess/PAS0471/&lt;user&gt;, after working in there for months on different projects, it would remove all of those files in one fell swoop! (Though luckily, OSC does keep backups in the project dirs.)\n\n\nPreventing disaster\nTo make it less likely for bad things to happen, here are some measures you can take as well as a built-in feature worth pointing out:\n\nIf you’re worried about a specific removal you are about to perform, you could use rm -i (short for interactive) to have remove ask you to confirm the removal of every dir and file, though this is not practical for dirs with lots of files:\n\n# Make some files for this exercise\nmkdir sandbox/remove_test\ntouch sandbox/remove_test/a1 sandbox/remove_test/a2 sandbox/remove_test/a3\n\n# Now remove them interactively, press 'y' (or 'n') for every question\nrm -ri sandbox/remove_test\nrm: descend into directory sandbox/remove_test? y\nrm: remove regular empty file sandbox/remove_test/a1’? y\nrm: remove regular empty file sandbox/remove_test/a2’? y\nrm: remove regular empty file sandbox/remove_test/a3’? y\n\nYou can write-protect important files, as we’ll see in the next session.\nYou can use rmdir whenever a dir you want to remove should be empty. That command will only remove empty dirs, and fails on non-empty dirs!\nFinally, it may be assuring to learn that you will not be able to remove other people’s files at OSC (since you do not have “write permission” for them)6. For example, the file below was created by me and you won’t be able to remove (or edit) it:\n\nrm /fs/ess/PAS0471/demo/202307_rnaseq/README.md\nrm: cannot remove ‘/fs/ess/PAS0471/demo/202307_rnaseq/README.md’: Permission denied"
  },
  {
    "objectID": "modules/A04_shell2.html#globbing-with-shell-wildcard-expansion",
    "href": "modules/A04_shell2.html#globbing-with-shell-wildcard-expansion",
    "title": "The Unix Shell - Part II",
    "section": "3 Globbing with shell wildcard expansion",
    "text": "3 Globbing with shell wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing.\n\n3.1 Shell wildcards\nIn the term “wildcard expansion”, wildcard refers to a few symbols that have a special meaning: specifically, they match certain characters in file names. (We’ll see below what expansion refers to.)\nHere, we’ll only talk about the most-used wildcard, *, in detail. But for the sake of completeness, I list them all below:\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets\n\n\n\n\n\n\n3.2 Using the * wildcard\nHere are some file matching examples with * — if we are in our directory data/fastq, then:\n\n\n\n\n\n\n\n\nPattern\nMatches files whose names…\nMatches in our dir\n\n\n\n\n*fastq.gz\nEnd in “.fastq.gz”\nAll files (but this patterns is useful to select files by extension)\n\n\nA*\nStart with a capital “A”\nFiles for the ASPC1 samples\n\n\n*_R1*\nContain “_R1”\nFiles with forward reads\n\n\n*\nAnything7\nAll files\n\n\n\nIn the above, note that:\n\nBecause * matches “zero characters” as well, A* also matches a file whose name is just A, *fastq.gz also matches just fastq.gz, and *_R1* also matches just _R1.\nAs a whole, the expressions with a wildcard need to match the entire filename, which is why, for example, you need *s on both sides of _R1 in the last example to match any file that contains this string.\n\nIt’s also useful to realize that wildcard expansion (e.g., the expansion of *fastq.gz to all FASTQ files in the working dir) is done by the shell itself, and not by any specific command. As a consequence, we can use shell wildcards with any command that takes file names as arguments — for example:\necho data/fastq/A*R1.fastq.gz\ndata/fastq/ASPC1_A178V_R1.fastq.gz  data/fastq/ASPC1_G31V_R1.fastq.gz\nls data/fastq/A*R1.fastq.gz\ndata/fastq/ASPC1_A178V_R1.fastq.gz  data/fastq/ASPC1_G31V_R1.fastq.gz\n# Note that these are line-counts of the compressed file, not reflecting the # of reads\nwc -l data/fastq/A*R1.fastq.gz\nwc -l data/fastq/A*R1.fastq.gz\n  15358 data/fastq/ASPC1_A178V_R1.fastq.gz\n  14974 data/fastq/ASPC1_G31V_R1.fastq.gz\n  30332 total\n\nYour Turn: practice with *\nWhat pattern would you use if you wanted to select:\n\nFASTQ files for all A178V samples in our dir data/fastq\nIn a fictional example (we don’t have plain FASTQ files), all gzipped (.fastq.gz) and plain FASTQ files (.fastq) at the same time?\n\n\n\nSolutions (click here)\n\n\ndata/fastq/*A178V* (We’ll need a * on either side of our pattern, because the file names neither start not end with the pattern.)\ndata/fastq/*.fastq* (Recall that the * will also match nothing, so this will also match files whose name end in .fastq, i.e. plain FASTQ files)\n\n\n\n\n\n\n\n\n\nDon’t confuse shell wildcards with regular expressions! (Click to expand)\n\n\n\n\n\nFor those of you who know some regular expressions from coding in e.g. R, Python, or from advanced usage of GUI-based text editors like jEdit: wildcards are conceptually similar to these, but the * and ? symbols don’t have the same meaning, and there are way fewer shell wildcards than regular expression symbols.\nIn particular, note that . is not a shell wildcard and thus represents a literal period when globbing.\nWe can also use regular expressions in the Shell, but this is functionality included in several specific commands, like grep and sed, and is not a form of shell expansion.\n\n\n\n\n\n\n3.3 Everyday usage of globbing\nIt is very common to use globbing to move (mv), copy (cp), or remove (rm) multiple files at once.\nBecause a * by itself matches all files, it can for example be useful if you want to copy the entire contents of a directory into your working dir (recall the exercise in 2.3):\n# You can run this if you want, but then remove the copied FASTQ files afterwards\ncp data/fastq/* .\nA few more globbing examples:\ncp -v data/fastq/*Miapaca*_R1* .     # Copy 3 FASTQ files to your working dir \n‘data/fastq/Miapaca2_A178V_R1.fastq.gz’ -&gt; ‘./Miapaca2_A178V_R1.fastq.gz’\n‘data/fastq/Miapaca2_G31V_R1.fastq.gz’ -&gt; ‘./Miapaca2_G31V_R1.fastq.gz’\nrm -v *fastq.gz                  # Remove all FASTQ files in your working dir\nremoved ‘Miapaca2_A178V_R1.fastq.gz’\nremoved ‘Miapaca2_G31V_R1.fastq.gz’\nIn next sessions, we’ll also use globbing a lot to select files to loop over.\n\n\n\n\n\n\nRecursive globbing (Click to expand)\n\n\n\n\n\nGlobbing does not work recursively by default, so ls *fastq.gz would only return gzipped FASTQ files in your current working dir.\nAfter running some bionformatics program, it’s relatively common to have FASTQ files in a separate directory for each sample. In cases like that, you can use ** to match any directory. For example, in the pattern below, the ** would “traverse” the fastq dir within the data dir:\nls data/**/*fastq.gz\ndata/fastq/ASPC1_A178V_R1.fastq.gz  data/fastq/ASPC1_G31V_R1.fastq.gz  data/fastq/Miapaca2_A178V_R1.fastq.gz  data/fastq/Miapaca2_G31V_R1.fastq.gz\ndata/fastq/ASPC1_A178V_R2.fastq.gz  data/fastq/ASPC1_G31V_R2.fastq.gz  data/fastq/Miapaca2_A178V_R2.fastq.gz  data/fastq/Miapaca2_G31V_R2.fastq.gz\nBut ** itself is not recursive either by default and only “traverses a single level”, so you’d need the following to list any FASTQ files that are exactly two levels deep from your current working dir:\nls **/**/*fastq.gz\ndata/fastq/ASPC1_A178V_R1.fastq.gz  data/fastq/ASPC1_G31V_R1.fastq.gz  data/fastq/Miapaca2_A178V_R1.fastq.gz  data/fastq/Miapaca2_G31V_R1.fastq.gz\ndata/fastq/ASPC1_A178V_R2.fastq.gz  data/fastq/ASPC1_G31V_R2.fastq.gz  data/fastq/Miapaca2_A178V_R2.fastq.gz  data/fastq/Miapaca2_G31V_R2.fastq.gz\nWhile this would not find them:\nls **/*fastq.gz\nls: cannot access **/*fastq.gz: No such file or directory\nHowever, you can make ** recursive by turning on the globstar option:\nshopt -s globstar     # Turn on 'globstar'\n\nls **/*fastq.gz\ndata/fastq/ASPC1_A178V_R1.fastq.gz  data/fastq/ASPC1_G31V_R1.fastq.gz  data/fastq/Miapaca2_A178V_R1.fastq.gz  data/fastq/Miapaca2_G31V_R1.fastq.gz\ndata/fastq/ASPC1_A178V_R2.fastq.gz  data/fastq/ASPC1_G31V_R2.fastq.gz  data/fastq/Miapaca2_A178V_R2.fastq.gz  data/fastq/Miapaca2_G31V_R2.fastq.gz\nWith globstar turned on, the pattern above would find gzipped FASTQ files no matter how many dir levels deep they are (including when they are in your current working dir)."
  },
  {
    "objectID": "modules/A04_shell2.html#viewing-text-files",
    "href": "modules/A04_shell2.html#viewing-text-files",
    "title": "The Unix Shell - Part II",
    "section": "4 Viewing text files",
    "text": "4 Viewing text files\nHere, I’ll give a quick overview of the most common commands to view text files in the shell.\nSeveral of these commands are especially useful to look at (very) large files, such as genomic data files, and we’ll talk more about them in the next session.\n\n\n\n\n\n\nViewing files in VS Code\n\n\n\nFor smaller text files, you might also choose to open them in the editor pane in VS Code. You can do so by finding and clicking on them in the Explorer in the side bar. Additionally, when you list files in the shell in VS Code, you can hold Ctrl (Windows) or Command (Mac) and then click on a file name (you should see it being underlined before you click)!\nUnlike the shell, VS Code is also able to open image files (e.g. .png, .jpg) and PDFs this way — and as we’ll see later, you can even render HTML files in the editor pane.\nVS Code also handles the viewing of large text files better than most visual text editors, but I would still recommend to mostly view genomic-scale data files in the shell instead.\n\n\n\n4.1 cat\nThe cat (short for concatenate) command will print the full contents of a file. It is therefore mostly conventient for smaller files, since the contents of very large files will just stream by on your screen.\nFor example, let’s take a look at the README.md file in your working dir:\n# If you don't have this file, first run:\n# cp -v /fs/ess/PAS0471/demo/202307_rnaseq/README.md .\ncat README.md\n# README for /fs/ess/PAS0471/demo/202307_rnaseq\n\nThis directory contains files from an RNAseq project by the Montserrate-Cruz lab,\nmeant to demonstrate Unix shell basics.\n\nIn the `data/fastq` directory, there are 8 gzipped FASTQ files for 4 samples\n(the sequencing was done in a paired-end fashion and each sample has a file with\nforward reads (`_R1.fastq.gz`) and a file with reverse reads (`_R2.fastq.gz`)).\n\nTo be able to demonstrate Unix shell and bioinformatics basics,\nthose 8 files do not represent the full dataset for the experiment in question.\nFiles are present for only 4 of the 18 samples,\nand the individual files are much smaller than the originals:\n100,000 reads were randomly sub-sampled\n(the original files contained ~30 million reads each).\n\nA very simple metadata TSV (tab-separated values) text file is present in\n`metadata/meta.tsv`, with one row per input file,\nand columns specifying the read direction, sample ID, cell line, and variant.\n\n\n\n\n\n\n.md = Markdown (Click to expand)\n\n\n\n\n\n.md is the extension for “Markdown” files. Markdown is a very simple but still quite powerful text “Markup language” (e.g. HTML and LaTeX being much more complicated ones). This website is also made with a file type that is a Markdown variant.\nBut because of Markdown’s simplicity, a Markdown file can look very similar to a plain text file. The only Markdown features used in the README file are backticks around file names, so they will show up in code font, and #, which will create a Level 1 header.\n\n\n\n\n\n\n4.2 head and tail\nThe twin commands head and tail will print the first and last lines, respectively, of a file — this can be very useful to preview the contents of larger files.\nBy default, head and tail will print up to 10 lines of a file:\nhead metadata/meta.tsv\nsample_id       cell_line       variant\nASPC1_A178V     ASPC1   A178V\nASPC1_A178V     ASPC1   A178V\nASPC1_G31V      ASPC1   G31V\nASPC1_G31V      ASPC1   G31V\nMiapaca2_A178V  Miapaca2        A178V\nMiapaca2_A178V  Miapaca2        A178V\nMiapaca2_G31V   Miapaca2        G31V\nMiapaca2_G31V   Miapaca2        G31V\nBut meta.tsv only has 9 lines, so we got 9 instead. We can use the -n argument to modify the number of lines that is being printed:\ntail -n 3 metadata/meta.tsv\nMiapaca2_A178V  Miapaca2        A178V\nMiapaca2_G31V   Miapaca2        G31V\nMiapaca2_G31V   Miapaca2        G31V\n\n\n\n4.3 less\nAnother very handy command to view medium to large text files is less, which opens up a text file within your shell in a “pager”. That is, you will not get your prompt back until you press q to quit less, but you can e.g. scroll/move around in the file.\nBesides scrolling with your mouse, its easiest to move around with up and down arrows and, if you have them, PgUp and PgDn (also, u will move up half a page and d will move down half a page).\nIf you find yourself scrolling down and down to try and reach the end of the file, you can instead press G to go to the very end right away (and g to go back to the top).\nTry it with one of the FASTQ files (we’ll go into more detail next time!):\nless data/fastq/ASPC1_A178V_R1.fastq.gz"
  },
  {
    "objectID": "modules/A04_shell2.html#command-output-redirection",
    "href": "modules/A04_shell2.html#command-output-redirection",
    "title": "The Unix Shell - Part II",
    "section": "5 Command output redirection",
    "text": "5 Command output redirection\nAs mentioned earlier, Unix commands nearly always print their output to the screen. But you can also “redirect” the output to a file or to another command.\n\n5.1 Plain redirection with &gt;\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\necho \"My first line\" &gt; test.txt\ncat test.txt\nMy first line\nOf course, redirection works not just with echo, but with every single command (or bioinformatics program) that prints output to screen:\nls &gt; my_files_on_2023-08-04.txt\ncat my_files_on_2023-08-04.txt    # Use tab completion\ndata\nmetadata\nmy_files_on_20230803.txt\nREADME.md\ntest.txt\n\n\n\n5.2 Appending with &gt;&gt;\nWith “&gt;&gt;”, we append the output to a file (that is, it won’t overwrite any existing content like &gt;):\necho \"My second line\" &gt;&gt; test.txt\ncat test.txt\nMy first line\nMy second line\nAnd to circle back to &gt;, demonstrating how this will overwrite contents:\necho \"My third line overwrote the first two!\" &gt; test.txt\ncat test.txt\nMy third line overwrote the first two!\n\n\n\n5.3 The pipe\nA third way of redirecting command output is quite different from the previous two — the | (pipe) takes the output of one command and “pipes” it as input for a subsequent command.\n# This will tell us how many files and dirs are in our current working dir\nls -l | wc -l\n6\nIn the example above, the output of the ls command (i.e., a list of files and dirs) was not printed to screen but piped to the wc -l command, which then counted the number of lines in the input it received through the pipe.\nThis also reveals, then, that wc accepts input in two ways:\n\nOne ore more files, whose names are passed as arguments (wc -l README.md)\nSo-called “standard input”, a “stream” of input directly passed to the command, most commonly via the pipe (cat README.md | wc -l).\n\nAnd this is true not just for wc, but for almost any shell command that accepts input data to operate on.\nPipes are incredibly useful because they avoid the need write/read intermediate files — this saves typing and also makes the operation much quicker.\n\n\n\n\n\n\n\nThe power of the Unix shell\n\n\n\nAnother example, where we combine shell wildcards with output redirection, demonstrates how simple and powerful the Unix shell can be (and why the cat command is named after concatenation).\nImagine you had a thousand FASTA files in a dir fastas, and wanted to combine all of them, back-to-back, into a single file (“multi-FASTA” files, i.e. FASTA files with multiple sequences, indeed just consist of multiple FASTA entries concatenated). This simple command would do that — and would be very fast even for really large files:\n# (Fictitious example, you don't have a dir 'fastas')\ncat fastas/*.fa &gt; combined.fa\nImagine trying to create such a concatenated file in a regular text editor with a graphical user interface!\n\n\n\n\n\n\n\n\n“Standard output” vs “standard error” (Click to expand)\n\n\n\n\n\nWhile we have not noticed it so far, Unix commands –and many bioinformatics command-line tools alike– actually separate “regular” output (called “standard output”) from error messages (“standard error”).\nWithout redirection, this is not obvious because both types of output are then printed to screen. But look what happens when we run ls with a non-existing file name as an argument, and redirect the output:\nls -lh non_existent_file &gt; my_file_list.txt\nls: cannot access non_existent_file: No such file or directory\ncat my_file_list.txt\n \nThe error message (standard error) was not redirected to the output file, but printed to the screen!\nOr consider this example:\nls -lh README.md non_existent_file &gt; another_file_list.txt\nls: cannot access non_existent_file: No such file or directory\ncat another_file_list.txt\n-rw-r--r-- 1 jelmer PAS0471 964 Jul 27 17:48 README.md\nIn the example above, the ls command produced both standard out (the successful file listing of README.md) and standard error (the error message for non_existent_file), and:\n\nStandard output was redirected to the file\nStandard error was printed to screen.\n\nOf course, there is also a way to redirect standard error. We’ll at least see this in the context of standard out and standard error redirection for Slurm batch jobs."
  },
  {
    "objectID": "modules/A04_shell2.html#at-home-reading-reference-pages-further-resources",
    "href": "modules/A04_shell2.html#at-home-reading-reference-pages-further-resources",
    "title": "The Unix Shell - Part II",
    "section": "At-home reading: reference pages & further resources",
    "text": "At-home reading: reference pages & further resources\n\nShell reference pages on this site\n\nCommon shell commands\nShell keyboard shortcuts\n\n\n\nFurther resources for learning the Unix shell\n\nOSC’s UNIX Basics\nhttps://www.learnenough.com/command-line-tutorial\nhttps://cvw.cac.cornell.edu/Linux/\nhttp://www.ee.surrey.ac.uk/Teaching/Unix/\nhttps://www.udacity.com/course/linux-command-line-basics--ud595\nhttp://moo.nac.uci.edu/~hjm/How_Programs_Work_On_Linux.html"
  },
  {
    "objectID": "modules/A04_shell2.html#footnotes",
    "href": "modules/A04_shell2.html#footnotes",
    "title": "The Unix Shell - Part II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom now on, I will refer to command-line expressions, with which I mean everything you type in the shell until the next Enter as simply a command. “Command”, then, can refer to either a command’s name (e.g. ls) or a full command line expression (e.g. ls -lh path/to/dir) — this is potentially confusing but common usage and the context should usually make the meaning clear.↩︎\nThough we will see later that you can also “pipe in” input to wc and many other commands, in which case it does not need a file name as an argument, since you are providing data for it to operate on in a different way.↩︎\nThe advantage of this is that it makes it very easy to recognize variables inside scripts and so on!↩︎\nWhen you use touch on existing files, all it will do is change the timestamp that the file was most recently modified: hence the name of this command.↩︎\n-r is a common option name for recursive behaviour among Unix commands (mkdir is an exception with its -p).↩︎\nAt least not by default, the person who created the file can change this.↩︎\nHowever, it will not match so-called “hidden files” whose names start with a ..↩︎"
  },
  {
    "objectID": "modules/A02_vscode.html#overview-setting-up",
    "href": "modules/A02_vscode.html#overview-setting-up",
    "title": "The VS Code Text Editor",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nHere, we will learn the basics of a fancy text editor called VS Code (in full, Visual Studio Code). Conveniently, we can use a version of this editor (sometimes referred to as Code Server) in our browser via the OSC OnDemand website.\nWe will use VS Code throughout these sessions as practically a one-stop solution for our computing activities at OSC. This is also how I use this editor in my daily work.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. If you’ve ever worked with R, the RStudio program is another good example of an IDE. For our purposes, RStudio will be our IDE for R, and VS code will be our IDE for shell scripts and code.\n\nStarting a VS Code session in OSC OnDemand\nIn the previous session, I showed you how to start a VS Code session in OnDemand, but for the sake of completeness, instructions to do so are also shown below.\n\n\n\n\n\n\nStarting VS Code at OSC\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nThe starting directory doesn’t matter\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code."
  },
  {
    "objectID": "modules/A02_vscode.html#getting-started-with-vs-code",
    "href": "modules/A02_vscode.html#getting-started-with-vs-code",
    "title": "The VS Code Text Editor",
    "section": "1 Getting started with VS Code",
    "text": "1 Getting started with VS Code\n\n\n\n\n1.1 Side bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA      (“hamburger menu” icon) in the top, which has most of the standard menu items that you often find in a top bar, like File.\nA      (cog wheel icon) in the bottom, through which you can mainly access settings.\nA bunch of icons in the middle that serve to switch between different options for the (wide) Side Bar, which can show one of the following:\n\nExplorer: File browser (and, e.g., an outline for the active file)\nSearch: To search recursively across all files in the active folder\nSource Control: To work with version control systems like Git (not used in this workshop)\nRun and Debug: For debugging your code (not used in this workshop)\nExtensions: To install extensions (we’ll install one later)\n\n\n\n\n1.2 Editor pane and Welcome document\nThe main part of the VS Code is the editor pane. Whenever you open VS Code, a tab with a Welcome document is automatically opened. This provides some help for beginners, but also, for example, a handy overview of recently opened folders.\nWe can also use the Welcome document to open a new text file by clicking New file below Start (alternatively, click      =&gt;   File   =&gt;   New File), which open as a second “tab” in the editor pane. We’ll work with our own text files (scripts) starting tomorrow.\n\n\n\n\n\n\nRe-open the Welcome document\n\n\n\nIf you’ve closed the Welcome document but want it back, click      =&gt;   Help   =&gt;   Welcome.\n\n\n\n\n1.3 Terminal (with a Unix shell)\n By default, no terminal is open in VS Code – open one by clicking      =&gt; Terminal =&gt; New Terminal.\nThis opens up a terminal with a Unix shell. In the next session, we’ll start talking about actually using the Unix shell."
  },
  {
    "objectID": "modules/A02_vscode.html#a-folder-as-a-starting-point",
    "href": "modules/A02_vscode.html#a-folder-as-a-starting-point",
    "title": "The VS Code Text Editor",
    "section": "2 A folder as a starting point",
    "text": "2 A folder as a starting point\nConveniently, VS Code takes a specific folder (directory) as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nBy default, VS Code via OnDemand will open your Home directory.\nHere, we’ll change to the project dir for OSC project PAS0471, which is /fs/ess/PAS0471.\n Let’s open that folder. Click Open folder... in the Welcome tab (or      =&gt;   File   =&gt;   Open Folder).\nYou’ll notice that the program completely reloads. And You might also see a pop-up like this – you can check the box and click Yes:\n\n\n\n\n\n\n\n\n\n\n\nTaking off where you were\n\n\n\nWhen you reopen a folder you’ve had open before, VS Code will resume where you were before in terms of:\n\nReopening any files you had open\nIf you had an active terminal, it will re-open a terminal.\n\nThis is quite convenient, especially when you start working on multiple projects (different folders) in VS Code and frequently switch between those."
  },
  {
    "objectID": "modules/A02_vscode.html#some-vs-code-tips-and-tricks",
    "href": "modules/A02_vscode.html#some-vs-code-tips-and-tricks",
    "title": "The VS Code Text Editor",
    "section": "3 Some VS Code tips and tricks",
    "text": "3 Some VS Code tips and tricks\n\n3.1 Making use of your screen’s real estate\nSince we are using VS Code inside a browser window, we are unfortunately losing some screen space. Make sure to maximize the browser window and if you have a bookmarks bar, you should consider hiding it (for Chrome: Ctrl/⌘+Shift+B).\nYou may also opt to hide the side bars using the    =&gt;   View   =&gt;   Appearance menu (or Ctrl/⌘+B for the (wide) Side Bar).\n\n\n3.2 Resizing panes\nYou can resize panes (the terminal, editor, and side bar) by hovering your cursor over the borders and then dragging it.\n\n\n3.3 The Command Palette / Color themes\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for.\nTo access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P).\n\nOn Your Own: Try a few color themes\nOpen the Command Palette and start typing “color theme”, and you’ll see the relevant option pop up.\nThen, try out a few themes and see what you like!\n(You can also access the Color Themes option via      =&gt; Color Theme.)"
  },
  {
    "objectID": "modules/A02_vscode.html#at-home-reading-keyboard-shortcuts-local-installation",
    "href": "modules/A02_vscode.html#at-home-reading-keyboard-shortcuts-local-installation",
    "title": "The VS Code Text Editor",
    "section": "At-home reading: keyboard shortcuts & local installation",
    "text": "At-home reading: keyboard shortcuts & local installation\n\nKeyboard shortcuts\nWorking with keyboard shortcuts (also called “keybindings”) for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, replace Ctrl with ⌘).\n\n\n\n\n\n\nKeyboard shortcut cheatsheet\n\n\n\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\n\n\n\nOpen a terminal: Ctrl+` (backtick) or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nToggle the (wide) Side Bar: Ctrl+B\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+⬆/⬇ will move lines up or down.\n\nMultiple cursors: Press & hold Ctrl+Shift, then ⬆/⬇ arrows to add cursors upwards or downwards.\nToggle line comment (“comment out” code, and removing those comment signs): Ctrl+/\nSplit the editor window vertically: Ctrl+\\ (See also the options in      View =&gt; Editor Layout)\n\n\n\n\n\n\n\nBrowser interference\n\n\n\nUnfortunately, some VS Code and terminal keyboard shortcuts don’t work in this setting where we are using VS Code inside a browser, because existing browser keyboard shortcuts take precedence.\nIf you end up using VS Code a lot in your work, it is therefore worth switching to your own installation of the program — see the section below.\n\n\n\n\n\nLocal VS Code installation\nAnother nice feature of VS Code is that it is freely available for all operating systems (and even though it is made by Microsoft, it is also open source).\nTherefore, if you like the program, you can also install it on your own computer and do your local text editing / script writing in the same environment at OSC (it is also easy to install on OSU-managed computers, because it is available in the OSU “Self Service” software installer).\nEven better, the program can be “tunneled into” OSC, so that your working directory for the entire program can be at OSC rather than on your local computer. This gives the same experience as using VS Code through OSC OnDemand, except that you’re not working witin a browser window, which has some advantages (also: no need to fill out a form, and you’ll never run out of time).\nTo install VS Code on your own machine, follow these instructions from the VS Code website: Windows / Mac / Linux.\nTo SSH-tunnel VS Code into OSC, see these instructions on the SSH reference page on this website (they are a bit rudimentary, ask me if you get stuck)."
  },
  {
    "objectID": "modules/B01_fastqc.html",
    "href": "modules/B01_fastqc.html",
    "title": "Read QC with FastQC",
    "section": "",
    "text": "Under construction\n\n\n\nThis page is still under construction."
  },
  {
    "objectID": "modules/B01_fastqc.html#overview-setting-up",
    "href": "modules/B01_fastqc.html#overview-setting-up",
    "title": "Read QC with FastQC",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nSo far, we have covered all the building blocks to be able to run command-line programs at OSC:\n\nBasics of a supercomputer and of OSC specifically\nUnix (Bash) shell basics to work at a supercomputer, and learn the language used in our scripts\nThe bells and whistles needed to turn our commands into a shell script\nLoading and installing the software (command-line programs) that we want to run\nWorking with the Slurm job scheduler, so we can submit scripts as batch jobs\nThe ability to loop over commands, so that we can submit many scripts at once\n\nWith these skills, it’s relatively straightforward to create and submit scripts that run command-line programs to analyze our genomics data. In this session, we’ll apply them to run FastQC.\n\n\nFastQC: A program for quality control of FASTQ files\nFastQC is one the most ubiquitous pieces of genomics software. It allows you to assess the overall quality of, and potential problems with, the reads in your FASTQ files. It produces visualizations and assessments of for statistics such as per-base quality (below) and adapter content. Running FastQC or an equivalent program should always be the first analysis step after you receive your sequences.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser and which has about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\n\n\n\n\n\n\n\nFigure 1: A FastQC per-base quality score graph for files with fairly good (left) and poor (right) quality reads.\n\n\n\n\nStart VS Code and open your folder\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\nMake sure to open your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir, either by using the Open Folder menu item, or by clicking on this dir when it appears in the Welcome tab.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nIn the Welcome tab under Recent, you should see your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir listed: click on that to open it. Alternatively, use      =&gt;   File   =&gt;   Open Folder to open that dir in VS Code.\n\n\n\n\n\n\n\n\n\n\nDon’t have your own dir with the data? (Click to expand)\n\n\n\n\n\nIf you missed the last session, or deleted your rnaseq_intro dir entirely, run these commands to get a (fresh) copy of all files you should have so far:\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\nAnd if you do have an rnaseq_intro dir, but you want to start over because you moved or removed some of the files while practicing, then delete the dir before your run the commands above:\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\nYou should have at least the following files in this dir:\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── md5sums.txt\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md\n│   └── ref\n│       ├── GCF_000001405.40.fna\n│       ├── GCF_000001405.40.gtf"
  },
  {
    "objectID": "modules/B01_fastqc.html#a-script-to-run-fastqc",
    "href": "modules/B01_fastqc.html#a-script-to-run-fastqc",
    "title": "Read QC with FastQC",
    "section": "1 A script to run FastQC",
    "text": "1 A script to run FastQC\n\n1.1 FastQC syntax\nTo analyze one (optionally gzipped) FASTQ file with FastQC, the syntax can be as simple as:\n\nfastqc &lt;fastq-file&gt;\n\nHowever, we’ll always want to specify the output directory, because the unfortunate default for FastQC is to put them in the directory that the FASTQ files are in — we can do so as follows:\n\nfastqc --outdir=&lt;output-dir&gt; &lt;fastq-file&gt;\n\nFor instance, if we wanted output files to go to the directory results/fastqc and wanted the program to analyze the file data/fastq/ASPC1_A178V_R1.fastq.gz, a functional command would like like this:\n\nfastqc --outdir=results/fastqc data/fastq/ASPC1_A178V_R1.fastq.gz\n\n\n\n\n\n\n\nFastQC’s output file names are automatically determined\n\n\n\nFastQC allows us to specify the output directory, but not the output file names, which will be automatically determined based on the input file name.\nFor one FASTQ file, FastQC will output one HTML file and one ZIP archive. The latter contains files with the summary statistics that were computed and on which the figures are based — we generally don’t need to look at that.\n\n\n\n\n1.2 A basic script to run FastQC\nHere is what a basic script to run FastQC could look like:\n#!/bin/bash\n\n# Strict Bash settings\nset -euo pipefail\n\n# Copy the placeholder variables\ninput_file=$1\noutput_dir=$2\n\n# Run FastQC\nfastqc --outdir=\"$output_dir\" \"$input_file\"\n\n# (Don't run this in your terminal, this is an example script)\n\n\n1.3 A more well-developed FastQC script\nWe’ll add a few things to this script to e.g. make it run it smoothly as a batch job at OSC:\n\nA line to load the relevant OSC software module:\n\nmodule load fastqc/0.11.8\n\nA few sbatch options:\n\n#SBATCH --account=PAS0471\n#SBATCH --output=slurm-fastqc-%j.out\n\nA few echo statements to report what’s going on\nA line to create the output directory if it doesn’t yet exist:\nmkdir -p \"$output_dir\"\n\n\n\n\n\n\n\nRefresher: the -p option to mkdir (Click to expand)\n\n\n\n\n\nUsing the -p option does two things at once for us, both of which are necessary for a foolproof inclusion of this command in a script:\n\nIt will enable mkdir to create multiple levels of directories at once (i.e., to act recursively): by default, mkdir errors out if the parent directory/ies of the specified directory don’t yet exist.\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: No such file or directory\nmkdir -p newdir1/newdir2    # This successfully creates both directories\nIf the directory already exists, it won’t do anything and won’t return an error (by default, mkdir would return an error in this case, which would in turn lead the script to abort at that point with our set settings):\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: File exists\nmkdir -p newdir1/newdir2   # This does nothing since the dirs already exist\n\n\n\n\nHere is what our script looks like with those additions:\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n#SBATCH --output=slurm-fastqc-%j.out\n  \n# Strict Bash settings\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\ninput_file=\"$1\"\noutput_dir=\"$2\" \n\n# Initial reporting\necho \"# Starting script fastqc.ch\"\ndate\necho \"# Input FASTQ file:   $input_file\"\necho \"# Output dir:         $output_dir\"\necho\n\n# Create the output dir if needed\nmkdir -p \"$output_dir\"\n\n# Run FastQC\nfastqc --outdir=\"$output_dir\" \"$input_file\"\n\n# Final reporting\necho\necho \"# Listing the output files:\"\nls -lh \"$output_dir\"\n\necho\necho \"# Done with script fastqc.sh\"\ndate\n\n# (Don't run this in your terminal, but copy it into a .sh text file)\n\n Open a new file in VS Code (     =&gt;   File   =&gt;   New File) and save it as fastqc.sh within your scripts/ directory. Paste in the code above and save the file.\nNotice that this script is very similar to our toy scripts from the previous sessions: mostly standard (“boilerplate”) code with just a single command to run our program of interest. Therefore, you can adopt this script as a template for scripts that run other command-line programs, and will generally only need minor modifications!"
  },
  {
    "objectID": "modules/B01_fastqc.html#a-master-runner-script",
    "href": "modules/B01_fastqc.html#a-master-runner-script",
    "title": "Read QC with FastQC",
    "section": "2 A master / runner “script”",
    "text": "2 A master / runner “script”\nAbove, we created a fastqc.sh script, which we’ll eventually want to submit using a for loop. The code with that loop and the sbatch command could be directly typed in the terminal. But it’s better to save the commands used for job submission in a file/script as well.\nWe will now create such a file, which has the overall purpose of documenting the steps we took and the batch jobs we submitted. You can think of this file as your analysis lab notebook, or perhaps more accurately, your notebook entry that contains the final protocol you followed.\nThis kind of script is sometimes called a “master” or “runner” script. Because it will contain shell code, we will save it as a shell script (.sh) just like the script to run fastqc.sh and other individual analysis steps. However, it is important to realize that the runner script is not like the scripts to run individual steps. The latter are meant to be run/submitted in their entirety by the runner script, whereas a basic runner script that contains sbatch compute job commands for multiple steps has to be run step-by-step (see the box below).\n\n\n\n\n\n\nThe runner script can’t itself be run at once in its entirety\n\n\n\nOnce we’ve added multiple batch job steps, and the input of a later step uses the output of an earlier step, we won’t be able to just run the script as is. This is because the runner script would then submit jobs from different steps all at once, and that later step would start running before the earlier step has finished.\nTODO include an example\nIt is possible to make sbatch batch jobs wait for earlier steps to finish (e.g. with the --dependency option), but this quickly gets tricky. If you want a fully automatically rerunnable workflow / pipeline, you should consider using a workflow management system like Snakemake or NextFlow.\n\n\nSo, we’ll separate our code into two hierarchical levels of scripts, which we’ll also save in separate dirs to make this division clear:\n\nThe scripts that run individual steps of your analysis, like fastqc.sh. We’ll save these in a directory called scripts.\nA “runner” script that orchestrates the batch job submission of these individual steps. This script is not run at once in its entirety (unless you turn it into a formal workflow, which is beyond the scope of this material). We’ll save this script in a directory called run.\n\n Let’s go ahead and open a new text file, and save it as run/run.sh (VS Code should create that directory on the fly as needed).\n\n\n\n\n\n\nKeep the scripts for individual steps simple\n\n\n\nIt is a good idea to keep the shell scripts you will submit (e.g., fastqc.sh) simple in the sense that they should generally just run one program, and not a sequence of programs.\nOnce you get the hang of writing these scripts, it may seem appealing to string a number of programs together in a single script, so that it’s easier to rerun everything at once — but in practice, that will often end up leading to more difficulties than convenience. Once again, if you do want to develop a workflow that can run from start to finish, you’ll have to bite the bullet and learn a workflow management system."
  },
  {
    "objectID": "modules/B01_fastqc.html#running-fastqc-using-batch-jobs",
    "href": "modules/B01_fastqc.html#running-fastqc-using-batch-jobs",
    "title": "Read QC with FastQC",
    "section": "3 Running FastQC using batch jobs",
    "text": "3 Running FastQC using batch jobs\n\n3.1 Submitting the script for one FASTQ file\nLet’s submit our fastqc.sh script to the Slurm queue with sbatch:\nsbatch scripts/fastqc.sh data/fastq/ASPC1_A178V_R1.fastq.gz results/fastqc\nSubmitted batch job 12521308\n\n\n\n\n\n\nOnce again: Where does our output go? (Click to expand)\n\n\n\n\n\n\nOutput that would have been printed to screen if we had run the script directly, such as our echo statements and FastQC’s progress logging, will go into the Slurm log file slurm-fastqc-&lt;job-nr&gt;.out in our working dir.\nFastQC’s main output files (HTML and zip) will end up in the output directory we specified, in this case results/fastqc.\n\n\n\n\nIf we take a look at the queue, you may catch the job while it’s still pending (note below that the job’s NAME will by default be the filename of the script):\nFri Aug 25 12:07:48 2023\n    JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n  23666218 serial-40 fastqc.s   jelmer  PENDING       0:00   1:00:00      1 (None)\n…and then it should start running:\nFri Aug 25 12:07:54 2023\n    JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n  23666218 condo-osu fastqc.s   jelmer  RUNNING       0:06   1:00:00      1 p0133\nThe job will be finished within 10 seconds, though, and you might miss its listing in the squeue output entirely: as soon as it’s done, it will be removed from the list.\nOf course, just because a job has finished does not mean that it has ran successfully, and we should always check this. Let’s start by taking a look at the Slurm log file:\n\ncat slurm-fastqc-23666218.out    # You'll have a different job number in the filename\n\n\n\n\n\n\n\nClick to see the contents of the Slurm log file\n\n\n\n\n\n# Starting script fastqc.ch\nFri Aug 25 12:07:50 EDT 2023\n# Input FASTQ file:   data/fastq/ASPC1_A178V_R1.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ASPC1_A178V_R1.fastq.gz\nApprox 5% complete for ASPC1_A178V_R1.fastq.gz\nApprox 10% complete for ASPC1_A178V_R1.fastq.gz\nApprox 15% complete for ASPC1_A178V_R1.fastq.gz\nApprox 20% complete for ASPC1_A178V_R1.fastq.gz\nApprox 25% complete for ASPC1_A178V_R1.fastq.gz\nApprox 30% complete for ASPC1_A178V_R1.fastq.gz\nApprox 35% complete for ASPC1_A178V_R1.fastq.gz\nApprox 40% complete for ASPC1_A178V_R1.fastq.gz\nApprox 45% complete for ASPC1_A178V_R1.fastq.gz\nApprox 50% complete for ASPC1_A178V_R1.fastq.gz\nApprox 55% complete for ASPC1_A178V_R1.fastq.gz\nApprox 60% complete for ASPC1_A178V_R1.fastq.gz\nApprox 65% complete for ASPC1_A178V_R1.fastq.gz\nApprox 70% complete for ASPC1_A178V_R1.fastq.gz\nApprox 75% complete for ASPC1_A178V_R1.fastq.gz\nApprox 80% complete for ASPC1_A178V_R1.fastq.gz\nApprox 85% complete for ASPC1_A178V_R1.fastq.gz\nApprox 90% complete for ASPC1_A178V_R1.fastq.gz\nApprox 95% complete for ASPC1_A178V_R1.fastq.gz\nApprox 100% complete for ASPC1_A178V_R1.fastq.gz\nAnalysis complete for ASPC1_A178V_R1.fastq.gz\n\n# Listing the output files:\ntotal 5.1M\n-rw-r--r-- 1 jelmer PAS0471 266K Aug 25 12:07 ASPC1_A178V_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 456K Aug 25 12:07 ASPC1_A178V_R1_fastqc.zip\n\n# Done with script fastqc.sh\nFri Aug 25 12:07:56 EDT 2023\n\n\n\nOur script already listed the output files, but let’s take a look at those too, and do so in the VS Code file browser in the side bar. To actually view FastQC’s HTML output file, we unfortunately need to download it with the older version of VS Code that’s installed at OSC.\nFind the HTML file in VS Code’s file Explorer on the left-hand side of the screen, right-click on it, and find the “Download…” entry towards the bottom:\n\n\n\n\n\n\n3.2 Submitting the script many times with a loop\nThe script that we wrote above will run FastQC for a single FASTQ file. Now, we will write a loop that iterates over all of our FASTQ files (only 8 files in our case, but this could be 100s of files just the same), and submits a batch job for each of them.\nLet’s type the following into our run.sh script, and then copy-and-paste it into the terminal to run the loop:\nfor fastq_file in data/fastq/*fastq.gz; do\n    sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\nSubmitted batch job 2451089  \nSubmitted batch job 2451090  \nSubmitted batch job 2451091  \nSubmitted batch job 2451092   \nSubmitted batch job 2451093  \nSubmitted batch job 2451094  \nSubmitted batch job 2451095  \nSubmitted batch job 2451096\n\n\nOn Your Own: Check if everything went well\n\nUse squeue to monitor your jobs.\nTake a look at the Slurm log files while the jobs are running and/or after the jobs are finished.\nA nice trick when you have many log files is to check the last few lines of all of them using tail with a wildcard. This is useful because recall that with our strict Bash settings, a script should only run until the end if it did not encounter errors. And tail will helpfully include file name markers when you run it on multiple files as follows:\ntail slurm-fastqc*\nTake a look at FastQC’s output files: are you seeing 8 HTML files?\n\n\n\n\n\n\n\n\nAdd keyboard shortcut to run shell commands from the editor\n\n\n\nTODO - expand on this\n\nClick the  (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "modules/B01_fastqc.html#interpreting-the-fastqc-output",
    "href": "modules/B01_fastqc.html#interpreting-the-fastqc-output",
    "title": "Read QC with FastQC",
    "section": "4 Interpreting the FastQC output",
    "text": "4 Interpreting the FastQC output\nTODO\nFor now see, https://biodash.github.io/tutorials/2021-01_rnaseq/03-fastqc-output.html"
  },
  {
    "objectID": "modules/A06_ref-files.html#overview-setting-up",
    "href": "modules/A06_ref-files.html#overview-setting-up",
    "title": "Reference genome files",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nIn this session, we’ll talk about the two main types of reference genome files that you need in a reference-based RNAseq analysis (and other reference-genome based genomics analysis like variant calling): FASTA and GTF/GFF.\n\nFASTA files: Simple sequence files, where each entry contains just a header and a DNA or protein sequence. Your reference genome assembly will be in this format.\nGTF (& GFF) files: These contain annotations in a tabular format, e.g. the start & stop position of each gene.\n\nWe will download these files from the internet using wget, and will also see a few new shell commands that can be used to summarize tabular data like that in GTF files: cut, sort, and uniq (in combination with grep).\n\nStart VS Code and open your folder\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\nMake sure to open your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir, either by using the Open Folder menu item, or by clicking on this dir when it appears in the Welcome tab.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nIn the Welcome tab under Recent, you should see your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir listed: click on that to open it. Alternatively, use      =&gt;   File   =&gt;   Open Folder to open that dir in VS Code.\n\n\n\n\n\n\n\n\n\n\nDon’t have your own dir with the data? (Click to expand)\n\n\n\n\n\nIf you missed the last session, or deleted your rnaseq_intro dir entirely, run these commands to get a (fresh) copy of all files:\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\nAnd if you do have an rnaseq_intro dir, but you want to start over because you moved or removed some of the files while practicing, then delete the dir before your run the commands above:\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\nYou should have at least the following files in this dir:\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md"
  },
  {
    "objectID": "modules/A06_ref-files.html#downloading-reference-genome-files",
    "href": "modules/A06_ref-files.html#downloading-reference-genome-files",
    "title": "Reference genome files",
    "section": "1 Downloading reference genome files",
    "text": "1 Downloading reference genome files\n\n1.1 Finding genome files at NCBI\nTo analyze our RNAseq data, we’ll need two files related to our reference genome. This is the genome that we will align our reads to, and whose gene annotations will form the basis of the gene counts we’ll eventually get.\nSpecifically, we’ll need the nucleotide FASTA file with the genome assembly itself, and a GTF file, which is a tabular file with the genomic coordinates and other information for genes and other so-called genomic “features”.\nWe can download these files from NCBI. For human, many genome assemblies are available on NCBI, but the current reference genome is “GRCh38.p14” (see this overview). There are several ways to download genomes from the NCBI — here, we will keep it simple and directly download just the two files that we need from the NCBI FTP site for this genome.\n\n\n\n\n\n\nGetting to the FTP site\n\n\n\nYou can get to this FTP site by clicking on the link for “GRCh38.p14” on the overview page, which will bring you here, then clicking on “View the legacy Assembly page”, which will bring you here, and then clicking on “FTP directory for RefSeq assembly” on the right-hand side of the page.\n\n\nOn the FTP site, right-click on GCF_000001405.40_GRCh38.p14_genomic.fna.gz and then click “Copy link address” (the URL to this file is also shown in the command box below).\n\n\n\n1.2 Downloading files to OSC with wget\nTo download a file to OSC, you can’t just open a web browser and download it there directly. You could download it to your own computer and then transfer it to OSC. A more direct approach is to use a download command in your OSC shell. wget is one command that allows you to download files from the web1.\nTo download a file to your working directory, you just need to tell wget about the URL (web address) to that file — type “wget”, press Space, and paste the address you copied:\n\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\n\n--2023-08-08 13:46:35--  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.11, 130.14.250.10, 2607:f220:41e:250::12, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.11|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 972898531 (928M) [application/x-gzip]\nSaving to: ‘GCF_000001405.40_GRCh38.p14_genomic.fna.gz’\n65% [============================================================================================&gt;                                                   ] 633,806,848 97.7MB/s \nThe wget command is quite chatty, as you can see above, and its output to the screen includes a progress bar for the download.\nNext, let’s download one of the annotation files. On the NCBI genome assembly FTP page, both a GFF and a GTF file are available. These are two very similar formats that contain the same data. For our specific analysis workflow, the GTF format is preferred, so we will download the GTF file — right-click on GCF_000001405.40_GRCh38.p14_genomic.gtf.gz and copy the link address.\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.gtf.gz\n# (Command output not shown)\nNext, let’s see if the files are indeed in our current working dir:\nls -lh \n# (Output should include:)\n-rw-r--r-- 1 jelmer PAS0471 928M Mar 21 10:15 GCF_000001405.40_GRCh38.p14_genomic.fna.gz\n-rw-r--r-- 1 jelmer PAS0471  49M Mar 21 10:15 GCF_000001405.40_GRCh38.p14_genomic.gtf.gz\n\n\n\n1.3 Uncompressing and renaming the genome files\nBoth the FASTA and the GTF file are gzip-compressed. While it’s preferable to keep FASTQ files compressed (as mentioned above), it’s often more convenient to store your reference genome files as uncompressed files2.\nWe can uncompress/unzip these files with the gunzip (“g-unzip”) command as follows (and notice the subsequent increase in file size in the ls output):\n# Will take several seconds, esp. for the first file, and not print output to screen\ngunzip GCF_000001405.40_GRCh38.p14_genomic.fna.gz\ngunzip GCF_000001405.40_GRCh38.p14_genomic.gtf.gz\n\nls -lh\n# (Output should include:)\n-rw-r--r-- 1 jelmer PAS0471 3.2G Mar 21 10:15 GCF_000001405.40_GRCh38.p14_genomic.fna\n-rw-r--r-- 1 jelmer PAS0471 1.6G Mar 21 10:15 GCF_000001405.40_GRCh38.p14_genomic.gtf\nWe’ll also want to move these files to a dedicated directory. Also, it will be convenient for our purposes to shorten their file names while retaining the necessary identifying information (the “GCF_” RefSeq number).\nAfter creating a directory, we can move and rename the files at once as follows:\n# We'll use a dir 'data/ref' for reference genome files\nmkdir -p data/ref\n\nmv -v GCF_000001405.40_GRCh38.p14_genomic.fna data/ref/GCF_000001405.40.fna\nmv -v GCF_000001405.40_GRCh38.p14_genomic.gtf data/ref/GCF_000001405.40.gtf\n‘GCF_000001405.40_GRCh38.p14_genomic.fna’ -&gt; ‘data/ref/GCF_000001405.40.fna’\n‘GCF_000001405.40_GRCh38.p14_genomic.gtf’ -&gt; ‘data/ref/GCF_000001405.40.gtf’"
  },
  {
    "objectID": "modules/A06_ref-files.html#fasta",
    "href": "modules/A06_ref-files.html#fasta",
    "title": "Reference genome files",
    "section": "2 FASTA",
    "text": "2 FASTA\nOur reference genome assembly is stored in the FASTA format, so we’ll learn more about this format and take a look at our own file.\n\n2.1 The FASTA format\nThe FASTA format is in essence simpler than FASTQ, but it is also less standardized and more generic.\nThe following example FASTA file contains two entries:\n&gt;unique_sequence_ID Optional description (free form!)\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n&gt;unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\nLike FASTQ files, each entry contains a header and the sequence itself, but:\n\nHeader lines start with a &gt; and are otherwise basically “free form”, but they usually provide some sort of identifier (and sometimes some metadata) for the sequence in the entry.\nThere are no quality scores, and there is no + line.\nThe sequences can be DNA, RNA, or amino acids.\nA FASTA entry can represent a number of different types of sequencs, such as a read, a contig/scaffold, a chromosome, or a gene, as well as an aligned sequence with gaps in it.\nBecause the individual sequences in a FASTA file can be very long, sequences are often not on a single line, but may be spread across multiple/many lines with some fixed width, say 50 or 80 characters. (As such, line counts are not that informative!)\n\nGeneric FASTA file extensions are .fasta and .fa, but also used are extensions that explicitly indicate whether the sequences are nucleotides (.fna, like our downloaded genome assembly file) or amino acids (.faa).\n\n\n\n2.2 Exploring our FASTA file\nLet’s take a look at the first lines of our reference genome FASTA:\nhead data/ref/GCF_000001405.40.fna\n&gt;NC_000001.11 Homo sapiens chromosome 1, GRCh38.p14 Primary Assembly\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nWell … apparently not all bases have been determined: as you probably know, an N represents an unknown base. We can also see that the sequence for each entry is spread across multiple lines, with a line width of 80 characters:\n# 'head -n 2 | tail -n 1' prints the second line; 'wc -c' counts characters\nhead -n 2 data/ref/GCF_000001405.40.fna | tail -n 1 | wc -c\n81              # 81 - 1 = 80 characters\nLet’s take a more extensive look at the file with less:\nless data/ref/GCF_000001405.40.fna\n# (Remember: press 'q' to quit less)\n\nWe can also look at all or some of the entry headers by greping for &gt; (which is only allowed in the header) — in this file, we can see that the entries represent the scaffolds:\ngrep \"&gt;\" data/ref/GCF_000001405.40.fna | head\n&gt;NC_000001.11 Homo sapiens chromosome 1, GRCh38.p14 Primary Assembly\n&gt;NT_187361.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG1_UNLOCALIZED\n&gt;NT_187362.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG2_UNLOCALIZED\n&gt;NT_187363.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG3_UNLOCALIZED\n&gt;NT_187364.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG4_UNLOCALIZED\n&gt;NT_187365.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG5_UNLOCALIZED\n&gt;NT_187366.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG6_UNLOCALIZED\n&gt;NT_187367.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG7_UNLOCALIZED\n&gt;NT_187368.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG8_UNLOCALIZED\n&gt;NT_187369.1 Homo sapiens chromosome 1 unlocalized genomic scaffold, GRCh38.p14 Primary Assembly HSCHR1_CTG9_UNLOCALIZED\nFinally, let’s count the number of scaffolds simply by counting the number of lines with a &gt;:\n# (This will take several seconds)\ngrep -c \"&gt;\" data/ref/GCF_000001405.40.fna\n705\nDoes this match with what the NCBI website told us?"
  },
  {
    "objectID": "modules/A06_ref-files.html#annotation-files-gtfgff",
    "href": "modules/A06_ref-files.html#annotation-files-gtfgff",
    "title": "Reference genome files",
    "section": "3 Annotation files (GTF/GFF)",
    "text": "3 Annotation files (GTF/GFF)\nOur reference genome annotation is stored in the GTF format, so we’ll learn about this format, too, and take a look at our own file.\n\n3.1 The GTF format\nThe GTF format and the very similar GFF format3 are tabular files, with one row for each individual “genomic feature” (gene, exon, intron, etc.) that has been annotated in the genome, and with columns that contain information like the genomic coordinates of the features.\nSee the small sample below, with an added header line (not normally present) with column names for clarity:\nseqname     source  feature start   end     score  strand  frame    attributes\nNC_000001   RefSeq  gene    11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"\"; db_xref \"GeneID:100287102\"; db_xref \"HGNC:HGNC:37102\"; description \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; gbkey \"Gene\"; gene \"DDX11L1\"; gene_biotype \"transcribed_pseudogene\"; pseudo \"true\"; \nNC_000001   RefSeq  exon    11874   12227   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; \nHere are some more details on what’s in each column, with the more important/interesting columns in bold:\n\nseqname — Name of the chromosome, scaffold, or contig\nsource — Name of the program that generated this feature, or the data source (e.g. database)\nfeature — Name of the feature type, e.g. “gene”, “exon”, “intron”, “CDS”\nstart — Start position of the feature (sequence numbering starts at 1; start position is included)\nend — End position of the feature (end position is also included)\nscore — A confidence score for the feature, often absent (in which case it is .)\nstrand — Whether the feature is on the + (forward) or - (reverse) strand\nframe — 0, 1 or 2 (or . for no frame). 0 means the 1st base of the feature is the 1st base of a codon, etc.\nattribute — A semicolon-separated list of tag-value pairs with additional information about each feature.\n\nAs we’ll see below, GTF files also have a header section, whose lines start with a # and contain some metadata for the file.\n\n\n\n\n\n\nAnnotation and assembly versions\n\n\n\nBecause GTF/GFF files contain genomic coordinates, it is essential that the FASTA and GTF/GFF you are working with correspond to the same assembly. For example, if you want to use an updated assembly FASTA for your focal genome, you’re going to need an updated annotation file as well. (On the other hand, it is possible to update just the annotation for the same assembly.)\n\n\n\n\n\n3.2 Exploring our GTF file\nLet’s take a look at the first lines of the file with head, which shows us that there are five header lines, followed by the tabular part of the file:\nhead data/ref/GCF_000001405.40.gtf\n#gtf-version 2.2\n#!genome-build GRCh38.p14\n#!genome-build-accession NCBI_Assembly:GCF_000001405.40\n#!annotation-date 03/15/2023\n#!annotation-source NCBI RefSeq GCF_000001405.40-RS_2023_03\nNC_000001.11    BestRefSeq      gene    11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"\"; db_xref \"GeneID:100287102\"; db_xref \"HGNC:HGNC:37102\"; description \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; gbkey \"Gene\"; gene \"DDX11L1\"; gene_biotype \"transcribed_pseudogene\"; pseudo \"true\"; \nNC_000001.11    BestRefSeq      transcript      11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gbkey \"misc_RNA\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; transcript_biotype \"transcript\"; \nNC_000001.11    BestRefSeq      exon    11874   12227   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; transcript_biotype \"transcript\"; exon_number \"1\"; \nNC_000001.11    BestRefSeq      exon    12613   12721   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; transcript_biotype \"transcript\"; exon_number \"2\"; \nNC_000001.11    BestRefSeq      exon    13221   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; transcript_biotype \"transcript\"; exon_number \"3\"; \nThe final column (“attributes”) contains lots of data, so the line width will almost certainly exceed your screen width, and the output you see in your shell will be line-wrapped. It’s easier to see what’s going on on this website (where we get one line for each line of the file, and can scroll sideways), or with less -S:\nless -S data/ref/GCF_000001405.40.gtf\n\nOne nice trick for if we only want to only see the tabular lines (for some files, the header can be quite long), or if we want to summarize the data in the tabular lines, is to omit the header lines using grep -v. The -v option will invert grep’s behavior and only print non-matching lines — since only the header lines contain a #, we can use that as the pattern to search for:\ngrep -v \"#\" data/ref/GCF_000001405.40.gtf | head -n 3\nNC_000001.11    BestRefSeq      gene    11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"\"; db_xref \"GeneID:100287102\"; db_xref \"HGNC:HGNC:37102\"; description \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; gbkey \"Gene\"; gene \"DDX11L1\"; gene_biotype \"transcribed_pseudogene\"; pseudo \"true\"; \nNC_000001.11    BestRefSeq      transcript      11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gbkey \"misc_RNA\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; transcript_biotype \"transcript\"; \nNC_000001.11    BestRefSeq      exon    11874   12227   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; transcript_biotype \"transcript\"; exon_number \"1\";\nThis will also allow us to count the number of annotated features in the genome (4,684,284):\ngrep -cv \"#\" data/ref/GCF_000001405.40.gtf\n4684284\n\nLet’s see if we can find a gene of interest, “RASD1” in this annotation:\ngrep \"RASD1\" data/ref/GCF_000001405.40.gtf\nNC_000017.11    BestRefSeq      gene    17494437        17496395        .       -       .       gene_id \"RASD1\"; transcript_id \"\"; db_xref \"GeneID:51655\"; db_xref \"HGNC:HGNC:15828\"; db_xref \"MIM:605550\"; description \"ras related dexamethasone induced 1\"; gbkey \"Gene\"; gene \"RASD1\"; gene_biotype \"protein_coding\"; gene_synonym \"AGS1\"; gene_synonym \"DEXRAS1\"; gene_synonym \"MGC:26290\"; \nNC_000017.11    BestRefSeq      transcript      17494437        17496395        .       -       .       gene_id \"RASD1\"; transcript_id \"NM_001199989.2\"; db_xref \"GeneID:51655\"; gbkey \"mRNA\"; gene \"RASD1\"; product \"ras related dexamethasone induced 1, transcript variant 2\"; transcript_biotype \"mRNA\"; \nNC_000017.11    BestRefSeq      exon    17495896        17496395        .       -       .       gene_id \"RASD1\"; transcript_id \"NM_001199989.2\"; db_xref \"GeneID:51655\"; gene \"RASD1\"; product \"ras related dexamethasone induced 1, transcript variant 2\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_000017.11    BestRefSeq      exon    17494437        17495610        .       -       .       gene_id \"RASD1\"; transcript_id \"NM_001199989.2\"; db_xref \"GeneID:51655\"; gene \"RASD1\"; product \"ras related dexamethasone induced 1, transcript variant 2\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_000017.11    BestRefSeq      CDS     17495896        17496181        .       -       0       gene_id \"RASD1\"; transcript_id \"NM_001199989.2\"; db_xref \"CCDS:CCDS58519.1\"; db_xref \"GeneID:51655\"; gbkey \"CDS\"; gene \"RASD1\"; note \"isoform 2 is encoded by transcript variant 2\"; product \"dexamethasone-induced Ras-related protein 1 isoform 2\"; protein_id \"NP_001186918.1\"; exon_number \"1\"; \nNC_000017.11    BestRefSeq      CDS     17495531        17495610        .       -       2       gene_id \"RASD1\"; transcript_id \"NM_001199989.2\"; db_xref \"CCDS:CCDS58519.1\"; db_xref \"GeneID:51655\"; gbkey \"CDS\"; gene \"RASD1\"; note \"isoform 2 is encoded by transcript variant 2\"; product \"dexamethasone-induced Ras-related protein 1 isoform 2\"; protein_id \"NP_001186918.1\"; exon_number \"2\"; \nNC_000017.11    BestRefSeq      start_codon     17496179        17496181        .       -       0       gene_id \"RASD1\"; transcript_id \"NM_001199989.2\"; db_xref \"CCDS:CCDS58519.1\"; db_xref \"GeneID:51655\"; gbkey \"CDS\"; gene \"RASD1\"; note \"isoform 2 is encoded by transcript variant 2\"; product \"dexamethasone-induced Ras-related protein 1 isoform 2\"; protein_id \"NP_001186918.1\"; exon_number \"1\"; \nNC_000017.11    BestRefSeq      stop_codon      17495528        17495530        .       -       0       gene_id \"RASD1\"; transcript_id \"NM_001199989.2\"; db_xref \"CCDS:CCDS58519.1\"; db_xref \"GeneID:51655\"; gbkey \"CDS\"; gene \"RASD1\"; note \"isoform 2 is encoded by transcript variant 2\"; product \"dexamethasone-induced Ras-related protein 1 isoform 2\"; protein_id \"NP_001186918.1\"; exon_number \"2\"; \nNC_000017.11    BestRefSeq      transcript      17494437        17496395        .       -       .       gene_id \"RASD1\"; transcript_id \"NM_016084.5\"; db_xref \"Ensembl:ENST00000225688.4\"; db_xref \"GeneID:51655\"; gbkey \"mRNA\"; gene \"RASD1\"; product \"ras related dexamethasone induced 1, transcript variant 1\"; tag \"MANE Select\"; transcript_biotype \"mRNA\"; \nNC_000017.11    BestRefSeq      exon    17495896        17496395        .       -       .       gene_id \"RASD1\"; transcript_id \"NM_016084.5\"; db_xref \"Ensembl:ENST00000225688.4\"; db_xref \"GeneID:51655\"; gene \"RASD1\"; product \"ras related dexamethasone induced 1, transcript variant 1\"; tag \"MANE Select\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_000017.11    BestRefSeq      exon    17494437        17495684        .       -       .       gene_id \"RASD1\"; transcript_id \"NM_016084.5\"; db_xref \"Ensembl:ENST00000225688.4\"; db_xref \"GeneID:51655\"; gene \"RASD1\"; product \"ras related dexamethasone induced 1, transcript variant 1\"; tag \"MANE Select\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_000017.11    BestRefSeq      CDS     17495896        17496181        .       -       0       gene_id \"RASD1\"; transcript_id \"NM_016084.5\"; db_xref \"CCDS:CCDS11185.1\"; db_xref \"Ensembl:ENSP00000225688.3\"; db_xref \"GeneID:51655\"; gbkey \"CDS\"; gene \"RASD1\"; note \"isoform 1 proprotein is encoded by transcript variant 1\"; product \"dexamethasone-induced Ras-related protein 1 isoform 1 proprotein\"; protein_id \"NP_057168.1\"; tag \"MANE Select\"; exon_number \"1\"; \nNC_000017.11    BestRefSeq      CDS     17495128        17495684        .       -       2       gene_id \"RASD1\"; transcript_id \"NM_016084.5\"; db_xref \"CCDS:CCDS11185.1\"; db_xref \"Ensembl:ENSP00000225688.3\"; db_xref \"GeneID:51655\"; gbkey \"CDS\"; gene \"RASD1\"; note \"isoform 1 proprotein is encoded by transcript variant 1\"; product \"dexamethasone-induced Ras-related protein 1 isoform 1 proprotein\"; protein_id \"NP_057168.1\"; tag \"MANE Select\"; exon_number \"2\"; \nNC_000017.11    BestRefSeq      start_codon     17496179        17496181        .       -       0       gene_id \"RASD1\"; transcript_id \"NM_016084.5\"; db_xref \"CCDS:CCDS11185.1\"; db_xref \"Ensembl:ENSP00000225688.3\"; db_xref \"GeneID:51655\"; gbkey \"CDS\"; gene \"RASD1\"; note \"isoform 1 proprotein is encoded by transcript variant 1\"; product \"dexamethasone-induced Ras-related protein 1 isoform 1 proprotein\"; protein_id \"NP_057168.1\"; tag \"MANE Select\"; exon_number \"1\"; \nNC_000017.11    BestRefSeq      stop_codon      17495125        17495127        .       -       0       gene_id \"RASD1\"; transcript_id \"NM_016084.5\"; db_xref \"CCDS:CCDS11185.1\"; db_xref \"Ensembl:ENSP00000225688.3\"; db_xref \"GeneID:51655\"; gbkey \"CDS\"; gene \"RASD1\"; note \"isoform 1 proprotein is encoded by transcript variant 1\"; product \"dexamethasone-induced Ras-related protein 1 isoform 1 proprotein\"; protein_id \"NP_057168.1\"; tag \"MANE Select\"; exon_number \"2\"; \nThat worked! We are getting quite a few lines, but these are all features for one single gene (there is only a single gene feature, in the first line)."
  },
  {
    "objectID": "modules/A06_ref-files.html#summarizing-tabular-data-with-shell-tools",
    "href": "modules/A06_ref-files.html#summarizing-tabular-data-with-shell-tools",
    "title": "Reference genome files",
    "section": "4 Summarizing tabular data with shell tools",
    "text": "4 Summarizing tabular data with shell tools\n\n4.1 Introduction to cut, uniq, and sort\nUsing more grep as well as a few new shell commands, we can quickly get some useful information about the GTF file. Let’s learn about cut, sort, and uniq using our metadata file, which has the following content:\n# (Column -t is useful for displaying tabular files with columns aligned)\ncolumn -t metadata/meta.tsv \nsample_id       cell_line  variant\nASPC1_A178V     ASPC1      A178V\nASPC1_A178V     ASPC1      A178V\nASPC1_G31V      ASPC1      G31V\nASPC1_G31V      ASPC1      G31V\nMiapaca2_A178V  Miapaca2   A178V\nMiapaca2_A178V  Miapaca2   A178V\nMiapaca2_G31V   Miapaca2   G31V\nMiapaca2_G31V   Miapaca2   G31V\nFirst, the cut command can extract columns from tabular files, and you can use its -f option to specify the column(s) you want to extract — for example, -f 3 extracts column 3, while -f 3,5 would extract columns 3 and 5:\ncut -f 3 metadata/meta.tsv\nvariant\nA178V\nA178V\nG31V\nG31V\nA178V\nA178V\nG31V\nG31V\nSecond, the sort command will sort its input:\ncut -f 3 metadata/meta.tsv | sort\nA178V\nA178V\nA178V\nA178V\nG31V\nG31V\nG31V\nG31V\nvariant\n\n\n\n\n\n\nOther sort functionality\n\n\n\nWhile in the example below, we’re sorting a single column, sort can also sort multi-column files on one or more columns, while keeping the rows intact. Additionally, it can sort numerically, in reverse, and much more.\n\n\nHmmm… maybe it isn’t appropriate to include the header line (variant in the output above). A nice little trick to exclude the first line of a file is tail -n +2:\ntail -n +2 metadata/meta.tsv | cut -f 3 | sort\nA178V\nA178V\nA178V\nA178V\nG31V\nG31V\nG31V\nG31V\nThird, the uniq command will remove consecutive duplicate lines — for that reason, its input needs to be sorted if we want to remove all duplicates:\ntail -n +2 metadata/meta.tsv | cut -f 3 | sort | uniq\nA178V\nG31V\nFor large input files, it’s most often useful to either:\n\nSimply get the number of unique values that occur in a column by piping into wc -l:\n\ntail -n +2 metadata/meta.tsv | cut -f 3 | sort | uniq | wc -l\n2\n\nGet a “count table” showing how many times each value in a column occurs using uniq’s -c option:\n\ntail -n +2 metadata/meta.tsv | cut -f 3 | sort | uniq -c\n      4 A178V\n      4 G31V\n\n\n\n4.2 Getting GTF summary stats with cut, uniq, and grep\nHow many snRNAs are in the annotation?\n# Because the 'gene_biotype' attribute only occurs in 'gene' entries,\n# we can simply count the number of lines found by grep:\ngrep -c 'gene_biotype \"snRNA\"' data/ref/GCF_000001405.40.gtf\n172\nHow many “RASD” genes are in the annotation?\n# After getting all RASD entries (for different feature types), we can select\n# only 'gene' entries by cutting column 3 (feature types) and grep for 'gene':\ngrep 'gene_id \"RASD' data/ref/GCF_000001405.40.gtf | cut -f 3 | grep -c \"gene\"\n2\nWhich feature types are present and what are their counts?\n# Column 3 contains the feature types\ngrep -v \"^#\" data/ref/GCF_000001405.40.gtf | cut -f 3 | sort | uniq -c\n1835339 CDS\n2291763 exon\n  67127 gene\n 145185 start_codon\n 144753 stop_codon\n 200121 transcript\n\nYour turn: more GTF exploration\n\nHow many features are derived from each of the annotation sources (column 2)?\n\n\n\nHint\n\nThe code to do this is nearly identical to the last example above, you just have to cut a different column.\n\n\n\nSolution\n\ngrep -v \"#\" $gtf | cut -f 2 | sort | uniq -c\n2141089 BestRefSeq\n  11308 BestRefSeq%2CGnomon\n   2955 cmsearch\n  21999 Curated Genomic\n2504737 Gnomon\n    150 RefSeq\n   2050 tRNAscan-SE\n\n\nAbove, we got counts of features by type (gene, exon, etc., in column 3). Now, get separate counts of each feature type on each strand.\n\n\n\nHint\n\nThe only thing you need to do differently compared to a single-column summary is to select two columns in your cut command (uniq will then automatically process the two columns).\n\n\n\nSolution\n\ngrep -v \"#\" data/ref/GCF_000001405.40.gtf | cut -f3,7 | sort | uniq -c\n 916540 CDS     -\n 918799 CDS     +\n1143757 exon    -\n1148006 exon    +\n  33435 gene    -\n  33692 gene    +\n  71561 start_codon     -\n  73624 start_codon     +\n  71473 stop_codon      -\n  73280 stop_codon      +\n  98976 transcript      -\n 101145 transcript      +"
  },
  {
    "objectID": "modules/A06_ref-files.html#footnotes",
    "href": "modules/A06_ref-files.html#footnotes",
    "title": "Reference genome files",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ncurl is another very commonly used one, with much the same functionality.↩︎\nThis is because reference genome files don’t take up as much space, and bioinformatics tools may expect them to be uncompressed.↩︎\nIn fact, GTF is identical to GFF v2. But the current GFF version is the slightly different GFF v3, which sometimes has the extension .gff3 to make that clear.↩︎"
  },
  {
    "objectID": "modules/A07_overview.html#overview-setting-up",
    "href": "modules/A07_overview.html#overview-setting-up",
    "title": "Overview of next steps, and shell variables & for loops",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nIn this session, we’ll start by looking ahead a bit. You will get an bird’s eye view of how, during the first part of the RNAseq analysis (going from reads to a count table). we will be running bioinformatics programs with command-line interfaces (CLIs) This meant to give an overview of the fundamentals we still need to cover before we can start running these analyses.\nThen, we’ll talk about the first of those remaining fundamental topics: shell variables and for loops. The next few sessions will cover the remaining ones:\n\nWriting shell scripts\nUsing software at OSC\nSubmitting scripts to the OSC job queue.\n\n\nStart VS Code and open your folder\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\nMake sure to open your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir, either by using the Open Folder menu item, or by clicking on this dir when it appears in the Welcome tab.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nIn the Welcome tab under Recent, you should see your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir listed: click on that to open it. Alternatively, use      =&gt;   File   =&gt;   Open Folder to open that dir in VS Code.\n\n\n\n\n\n\n\n\n\n\nDon’t have your own dir with the data? (Click to expand)\n\n\n\n\n\nIf you missed the last session, or deleted your rnaseq_intro dir entirely, run these commands to get a (fresh) copy of all files you should have so far:\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\nAnd if you do have an rnaseq_intro dir, but you want to start over because you moved or removed some of the files while practicing, then delete the dir before your run the commands above:\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\nYou should have at least the following files in this dir:\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── md5sums.txt\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md\n│   └── ref\n│       ├── GCF_000001405.40.fna\n│       ├── GCF_000001405.40.gtf"
  },
  {
    "objectID": "modules/A07_overview.html#running-cli-bioinformatics-programs-at-osc-an-overview",
    "href": "modules/A07_overview.html#running-cli-bioinformatics-programs-at-osc-an-overview",
    "title": "Overview of next steps, and shell variables & for loops",
    "section": "1 Running CLI bioinformatics programs at OSC: an overview",
    "text": "1 Running CLI bioinformatics programs at OSC: an overview\nAs pointed out earlier, bioinformatics software (programs) that we use to analyze genomic data are typically run from the command line. That is, they have a “command-line interface” (CLI) rather than a “graphical user interface” (GUI), and are run using command line expressions that are structurally very similar to how we’ve been using basic Unix shell commands.\nHere, we’ll use the program FastQC as an example, which performs quality control (QC) on a FASTQ file, and is often the very first step when analyzing a newly sequenced genomics dataset.\n\n1.1 Running a CLI program interactively\nWhen we run a bioinformatics program at OSC, we’ll always have to first make the program available to us. We will cover that in more in detail in the session Software at OSC — but here is how you can do this for FastQC:\n# Load the OSC \"module\" for FastQC, so we can use this program\nmodule load fastqc\nNext, we could run FastQC for one of our FASTQ files as follows:\n# First create the output dir\nmkdir -p results/fastqc\n\n# Tell FastQC about the output dir with --outdir, and pass the file name as an argument\nfastqc --outdir results/fastqc data/fastq/ASPC1_A178V_R1.fastq.gz\nStarted analysis of ASPC1_A178V_R1.fastq.gz\nApprox 5% complete for ASPC1_A178V_R1.fastq.gz\nApprox 10% complete for ASPC1_A178V_R1.fastq.gz\nApprox 15% complete for ASPC1_A178V_R1.fastq.gz\n# [...truncated]\nApprox 95% complete for ASPC1_A178V_R1.fastq.gz\nApprox 100% complete for ASPC1_A178V_R1.fastq.gz\nAnalysis complete for ASPC1_A178V_R1.fastq.gz\nls -lh results/fastqc\ntotal 736K\n-rw-r--r-- 1 jelmer PAS0471 266K Aug 15 10:39 ASPC1_A178V_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 456K Aug 15 10:39 ASPC1_A178V_R1_fastqc.zip\nThat seems to have worked! We have two output files, including an HTML file that contains figures with all the results for this one FASTQ file.\nSo should we simply repeat this for the rest of our FASTQ files? I.e., we could create and run a fastqc command 7 more times, each time passing a different input file as the argument. That seems a bit tedious, and remember that the full dataset contains even more files…\n\n\n\n\n\n\nNote\n\n\n\nFor completeness’ sake, I should mention that it is in fact possible to pass multiple files as argument to fastqc. If we did that, we’d only need to run FastQC once, and would not need the approach shown below, where we use a loop.\nHowever, using a loop is a very general approach, and is necessary for the many programs that can only run one FASTQ file or one sample at a time. When using a supercomputer like OSC, this is also by far the most time-efficient approach, because it allows each sample/file to be processed in parallel rather than consecutively.\n\n\n\n\n1.2 Looping over input files\nWe can avoid using a very similar command a whole bunch of times by using a loop.\nFurther down on this page, we will learn all about for loops. Here, we’ll again just see a quick example, and it is not yet important to understand the details of the code below, which will “loop over” all FASTQ files in the data/fastq dir, and run FastQC for one file at a time:\nfor fastq_file in data/fastq/*fastq.gz; do\n    fastqc --outdir results/fastqc \"$fastq_file\"\ndone\nStarted analysis of ASPC1_A178V_R1.fastq.gz\nApprox 5% complete for ASPC1_A178V_R1.fastq.gz\n# [...]\nStarted analysis of ASPC1_A178V_R2.fastq.gz\nApprox 5% complete for ASPC1_A178V_R2.fastq.gz\n# [...]\nStarted analysis of ASPC1_G31V_R1.fastq.gz\nApprox 5% complete for ASPC1_G31V_R1.fastq.gz\n# [...]\n# [Output for all remaining files follows...]\nThat’s a lot better than repeating a similar line of codes many times — and the loop code would be exactly the same regardless of whether we had 8 or 200 FASTQ files!\nHowever, recall that our full dataset also has much larger FASTQ files. A FastQC run on one of these will take several minutes, so our loop might take a while to finish (and note that some of the programs we’ll run later take much longer than this). Most of all, when we run the program interactively and consecutively like this, we are not making use of the supercomputer’s strengths at all: this is basically like running it on our own laptop, since each FastQC run happens consecutively.\n\n\n1.3 Running the program with a shell script\nWe should instead submit a separate compute job for each FastQC run, and these can then all run in parallel (i.e., at the same time) on different OSC compute nodes. Making the switch from interactively running FastQC to submitting each run as a compute job will involve two steps:\n\nPutting the code to run FastQC inside a shell script\nSubmitting that shell script to the OSC compute job queue with sbatch (rather than directly running it)\n\nWe will cover the basics of shell scripts in the session on shell scripts — for now, you just need to know that these scripts are small text files (with the extension .sh) that basically contain the same kind of shell code we’ve been running interactively, with some added bells and whistles.\nIf for now we just assume we had created such a script, then we can modify our loop code as follows to run FastQC through a script, instead of including the command to run the program directly in the loop:\n# Don't run this - meant as an example only (you don't have this script yet)\nfor fastq_file in data/fastq/*fastq.gz; do\n    bash scripts/fastqc.sh \"$fastq_file\"\ndone\nStarted analysis of ASPC1_A178V_R1.fastq.gz\nApprox 5% complete for ASPC1_A178V_R1.fastq.gz\n# [...]\nStarted analysis of ASPC1_A178V_R2.fastq.gz\nApprox 5% complete for ASPC1_A178V_R2.fastq.gz\n# [...]\nStarted analysis of ASPC1_G31V_R1.fastq.gz\nApprox 5% complete for ASPC1_G31V_R1.fastq.gz\n# [...]\n# [Output for all remaining files follows...]\nHowever, running that loop would still make FastQC run sequentially on the node that we are at, instead of in parallel on different compute nodes. With the fastqc.sh script, we’ve set ourselves up to be able to submit batch jobs, but just using this script in itself does not change much.\n\n\n1.4 Submitting the script to the job scheduler (many times)\nTo run our script in parallel, we will use the sbatch command, which will submit the script to the Slurm job scheduler in every iteration of the loop (i.e., as many times as we have input files):\n# Don't run this - meant as an example only (you don't have this script yet)\nfor fastq_file in data/fastq/*fastq.gz; do\n    sbatch --account=PAS0471 scripts/fastqc.sh \"$fastq_file\"\ndone\nSubmitted batch job 23510054\nSubmitted batch job 23510055\n# [Output truncated...]\nWe will learn about sbatch and associated commands, as well as the different options you can use to specify the resources you want to request (how much time, how many cores, etc), in the session on Slurm compute jobs.\n\n\n1.5 Summarizing what we need to learn about\n\nMaking software available\nAt OSC, a bunch of bioinformatics programs are installed system-wide, but they still need to be “loaded” via module load. This collection of programs is unfortunately not comprehensive and we also need another approach, as we’ll see in the session on Software at OSC.\nWriting shell scripts\nPutting the code to run individual bioinformatics programs inside small shell scripts allows us to submit them to the Slurm job scheduler. Using scripts (as opposed to typing or pasting commands into the terminal directly) is more generally useful, e.g. for long-running commands or programs, and is also a good way to save your code and keep it organized and clear. We’ll talk about shell scripts in the next session.\nSubmitting scripts to the Slurm job scheduler\nSubmitting scripts as Slurm batch jobs at OSC allows us, for example, to run the same analysis for different samples simultaneously. We’ll see this in more detail in the session on Slurm jobs.\nfor loops and variables\nfor loops will allow you to repeat operations — specifically, we will later use them to submit many scripts at the same time, one per input file or sample. Using variables will allow you to run scripts flexibly, with different input files and settings; they are also used in loops. We’ll talk about these in the sections below."
  },
  {
    "objectID": "modules/A07_overview.html#variables",
    "href": "modules/A07_overview.html#variables",
    "title": "Overview of next steps, and shell variables & for loops",
    "section": "2 Variables",
    "text": "2 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs. Using variables makes it easier to change such settings. We also need to understand variables to work with loops and scripts.\n\n2.1 Assigning and referencing variables\nTo assign a value to a variable in the shell (in short: to assign a variable), use the syntax variable_name=value:\n# Assign the value \"beach\" to a variable with the name \"location\":\nlocation=beach\n\n# Assign the value \"200\" to a variable with the name \"n_lines\":\nn_lines=200\n\n\n\n\n\n\nThere can not be any spaces around the equals sign (=)!\n\n\n\n\n\n\nTo reference a variable (i.e., to access its value), you need to put a dollar sign $ in front of its name. As before with the environment variable $HOME, we’ll use the echo command to see what values our variables contain:\necho $location\nbeach\necho $n_lines\n200\nConveniently, we can use variables in lots of contexts, as if we had directly typed their values:\ninput_file=data/fastq/ASPC1_A178V_R1.fastq.gz\n\nls -lh $input_file \n-rw-r--r-- 1 jelmer PAS0471 4.1M Aug  1 18:05 data/fastq/ASPC1_A178V_R1.fastq.gz\n\n\n\n\n\n\nRules and tips for naming variables\n\n\n\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file above, as opposed to say $x and $myvar.\nThere are multiple ways of distinguishing words in the absence of spaces, such as $inputFile and $input_file: I prefer the latter, which is called “snake case”.\n\n\n\n\n\n2.2 Quoting variables\nAbove, we learned that a variable name cannot contain spaces. But what happens if our variable’s value contains spaces? First off, when we try to assign the variable without using quotes, we get an error:\ntoday=Thu, Aug 18\nAug: command not found\n\n\n\n\n\n\nWhy do you think we got this error?\n\n\n\n\n\nThe shell tried to assign everything up to the first space (i.e., Thu,) to today. After that, since we used a space, it assumed the next word (Aug) was something else: specifically, it interpreted that as a command.\n\n\n\nBut it works when we quote (with double quotes, \"...\") the entire string that makes up the value:\ntoday=\"Thu, Aug 18\"\necho $today\nThu, Aug 18\n\nNow, let’s try to reference this variable in another context. For example, we can try make a new file with today’s date:\ncd sandbox\n\ntouch README_$today.txt\nls\n# The output should include the following files:\n18.txt  \nAug  \nREADME_Thu,\n\n\n\n\n\n\nWhat went wrong here?\n\n\n\n\n\nAgain, the shell split the value of $today in 3 parts separated by a space (this is called “field splitting”): as a result, three files were created.\n\n\n\nLike with assignment, our problems can be avoided by quoting a variable when we reference it. This tells the shell it shouldn’t perform so-called “field splitting” on the variable’s value. Put another way, the quotes takes away the special meaning of the space as a separator inside the variable’s value:\ntouch README_\"$today\".txt\n\nls -1\n# The output should include the following file:\nREADME_Thu, Aug 18.txt\nIt is good practice to quote variables when you reference them: it never hurts, and avoids unexpected surprises.\n\n\n\n\n\n\nWhere does a variable name end? (Click to expand)\n\n\n\n\n\nAnother issue we can run into when we don’t quote variables is that we can’t explicitly define where a variable name ends within a longer string of text:\necho README_$today_final.txt\nREADME_.txt\n\n\n\n\n\n\nWhat went wrong here? (Hint: check the coloring highlighting above)\n\n\n\n\n\n\nFollowing a $, the shell will stop interpreting characters as being part of the variable name only when it encounters a character that cannot be part of a variable name, such as a space or a period.\nSince variable names can contain underscores, it will look for the variable $today_final, which does not exist.\nImportantly, the shell does not error out when you reference a non-existing variable – it basically ignores it, such that README_$today_final.txt becomes README_.txt, as if we hadn’t referenced any variable.\n\n\n\n\nQuoting solves this issue, too:\n\necho README_\"$today\"_final.txt\n\nREADME_Thu, Aug 18_final.txt\n\n\n\n\n\n\n\n\n\nQuoting as “escaping” special meaning & and double vs. single quotes (Click to expand)\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, double quotes will escape other “special characters”, such as shell wildcards. Compare:\necho *     # This will echo/list all files in the current working dir (!)\n18.txt Aug README_Thu, README_Thu, Aug 18.txt\necho \"*\"   # This will simply print the \"*\" character \n*\nHowever, as we saw above, double quotes not turn off the special meaning of $ (which is to denote a string as a variable):\necho \"$today\"      # Double quotes do not escape the special meaning of $\nThu, Aug 18\n…but single quotes will:\necho '$today'     # Single quotes do escape the special meaning of $\n$today"
  },
  {
    "objectID": "modules/A07_overview.html#for-loops",
    "href": "modules/A07_overview.html#for-loops",
    "title": "Overview of next steps, and shell variables & for loops",
    "section": "3 For loops",
    "text": "3 For loops\nLoops are a universal element of programming languages, and are used to repeat operations, such as when you want to run the same script or command for multiple files.\nHere, we’ll only cover the most common type of loop: the for loop. A for loop iterates over a collection, such as a list of files: that is, is allows you to perform one or more actions for each element in the collection, one element at a time.\n\n3.1 for loop syntax and mechanics\nLet’s see a first example, where our “collection” is just a very short list of numbers (1, 2, and 3) that we define on the fly:\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nThe indented lines between do and done contain the code that is being executed as many times as there are items in the collection: in this case 3 times, as you can tell from the output above.\nOn the first and last, unindented lines, for loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name (an arbitrary name; above we used a_number)\n\n\nin\nAfter in, we specify the collection (list of items) we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\n\nA semicolon ; (as used before do) separates two commands on a single line (Click to expand)\n\n\n\n\n\nA semicolon separates two commands written on a single line – for instance, instead of:\nmkdir results\ncd results\n…you could equivalently type:\nmkdir results; cd results\nThe ; in the for loop syntax has the same function, and as such, an alternative way to format a for loop is:\nfor a_number in 1 2 3\ndo\n    echo \"In this iteration of the loop, the number is $a_number\"\ndone\nBut that’s one line longer and a bit awkwardly asymmetric.\n\n\n\n\n\nHere are two key things to understand about for loops — we touched on these above, but they are worth digging into a bit more:\nFirst, in each iteration of the loop, one element in the collection is being assigned to the variable specified after for. In the example above, we used a_number as the variable name, so that variable contained 1 when the loop ran for the first time, 2 when it ran for the second time, and 3 when it ran for the third and last time.\nThis variable even continues to exist outside of the loop, and will have the last value that was assigned to it in the loop:\necho $a_number\n\n3\n\nSecond, the loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\nThe following example, where we let the computer sleep for 1 second before printing the date and time with the date command, demonstrates that the loop is being executed sequentially (not in parallel). When you run it, you should notice a slight pause after the output of each iteration is printed, and you can see that the output of date differs by one second each time:\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    date              # Print the date and time\n    echo \"--------\"\n    sleep 1           # Let the computer sleep for 1 second\ndone\nIn this iteration of the loop, the number is 1  \nTue Aug 15 13:30:16 EDT 2023  \n--------  \nIn this iteration of the loop, the number is 2  \nTue Aug 15 13:30:17 EDT 2023  \n--------  \nIn this iteration of the loop, the number is 3  \nTue Aug 15 13:30:18 EDT 2023  \n\n\nOn Your Own: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\n\n\n\n\nHints (Click to expand)\n\n\n\n\n\n\nJust like we looped over 3 numbers above (1, 2, and 3), you want to loop over the three mushroom names, morel, destroying_angel, and eyelash_cup.\nNotice that when we specify the collection “manually”, like we did above with numbers, the elements are simply separated by a space.\n\n\n\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\n\nmorel is an Ohio mushroom\ndestroying_angel is an Ohio mushroom\neyelash_cup is an Ohio mushroom\n\n\n\n\n\n\n\n\n3.2 Looping over files with globbing\nIn practice, it is rare to manually list the collection of items we want to loop over, like we did above with the numbers 1-3 and a few types of mushroom in the exercise. Instead, we commonly loop over files directly using globbing:\ncd /fs/ess/PAS0471/$USER/rnaseq_intro\n\nfor fastq_file in data/fastq/*fastq.gz; do\n    echo \"Now processing FASTQ file $fastq_file...\"\n    # [Code to analyze the FASTQ file...]\ndone\nNow processing FASTQ file data/fastq/ASPC1_A178V_R1.fastq.gz...\nNow processing FASTQ file data/fastq/ASPC1_A178V_R2.fastq.gz...\nNow processing FASTQ file data/fastq/ASPC1_G31V_R1.fastq.gz...\nNow processing FASTQ file data/fastq/ASPC1_G31V_R2.fastq.gz...\nNow processing FASTQ file data/fastq/Miapaca2_A178V_R1.fastq.gz...\nNow processing FASTQ file data/fastq/Miapaca2_A178V_R2.fastq.gz...\nNow processing FASTQ file data/fastq/Miapaca2_G31V_R1.fastq.gz...\nNow processing FASTQ file data/fastq/Miapaca2_G31V_R2.fastq.gz...\nThis technique is extremely useful. Take a moment to realize that to get the list of files, we’re not running a separate ls and storing the results: as mentioned before, a globbing pattern directly selects files. You could also use your globbing / wild card skills to narrow down the file selection:\n\nPerhaps we only want to select R1 FASTQ files (forward reads):\nfor fastq_file in data/raw/*_R1.fastq.gz; do\n    # [Some file processing...]\ndone\nOr our FASTQ files are tucked away in sample-specific folders:\n# With default shell settings, '**' will traverse one dir level\n# E.g., files are in 'results/trim/sampleA/', 'results/trim/sampleB/', etc. \nfor fastq_file in results/trim/**/*_R1.fastq.gz; do\n    # [Some file processing...]\ndone"
  },
  {
    "objectID": "modules/A07_overview.html#at-home-reading-command-substitution",
    "href": "modules/A07_overview.html#at-home-reading-command-substitution",
    "title": "Overview of next steps, and shell variables & for loops",
    "section": "4 At-home reading: Command substitution",
    "text": "4 At-home reading: Command substitution\nIf you want to, for excample, store the result of a command in a variable, or be able to print this result in an echo statement, you can use a construct called “command substitution”. To do so, you just need to wrap the command inside $().\nLet’s see an example. As you know, the date command will print the current date and time:\n\ndate\n\nFri Aug 25 14:26:12 EDT 2023\n\n\nIf we try to store the date in a variable directly, it doesn’t work: the literal string “date” is stored, not the output of the command:\n\ntoday=date\necho \"$today\"\n\ndate\n\n\nThat’s why we need command substitution with $():\n\ntoday=$(date)\necho \"$today\"\n\nFri Aug 25 14:26:12 EDT 2023\n\n\n\nOne practical example of using command substitution is when you want to automatically include the current date in a file name. First, note that we can use date +%F to print the date in YYYY-MM-DD format, and omit the time:\n\ndate +%F\n\n2023-08-25\n\n\nLet’s use that in a command substitution — but a bit differently than before: we use the command substitution $(date +%F) directly in our touch command, rather than first assigning it to a variable:\ncd sandbox\n\n# Create a file with our $today variable:\ntouch README_\"$(date +%F)\".txt\n\nls\n# Output should include a file like this (the actual date will vary, of course!):\nREADME_2023-08-15.txt\nAmong many other uses, command substitution is handy when you want your script to report some results, or when a next step in the script depends on a previous result. We’ll come across it a few times in our shell scripts later on.\n\nOn Your Own: Command substitution\nSay we wanted to store and report the number of lines in a FASTQ file, which, as discussed before, tells us how many reads are in it (there are 4 lines per read).\nAs we’ve also seen before, wc -l gets you the number of lines, and if you pipe input into wc -l, it won’t include the filename — this is useful for what you’ll want to do below:\ncd /fs/ess/PAS0471/$USER/rnaseq_intro\n\nzcat data/fastq/ASPC1_A178V_R1.fastq.gz | wc -l\n400000\nUse command substitution to store the output of the last command in a variable, and then use an echo command to print:\nThe file has 30387 lines\n\n\n\n\n\n\nSolution\n\n\n\n\n\nn_lines=$(zcat data/fastq/ASPC1_A178V_R1.fastq.gz | wc -l)\necho \"The file $n_lines lines\"\nThe file 400000 lines\nNote: You don’t have to quote variables inside a quoted echo call, since it’s, well, already quoted. If you also quote the variables, you will in fact unquote it, although that shouldn’t pose a problem inside echo statements."
  },
  {
    "objectID": "modules/B03_trimgalore.html#introduction",
    "href": "modules/B03_trimgalore.html#introduction",
    "title": "Trimming with TrimGalore",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this step, we will remove:\n\nAny adapter sequences that are present in the reads\nPoor-quality bases at the start and of the reads\nReads that have become excessively short after the prior two steps\n\n[Discuss a bit how this step is sometimes considered optional: mappers should be able to deal with poor-quality bases and adapter sequences.]\n[Discuss how there are several programs commonly used for this, including TrimGalore, Trimmomatic, and fastp – and that in general, these are exchangeable.]\n[Discuss how TrimGalore is “just” a wrapper around CutAdapt, and how it will can also run FastQC on the processed sequences.]"
  },
  {
    "objectID": "modules/B03_trimgalore.html#usinggetting-trimgalore-at-osc",
    "href": "modules/B03_trimgalore.html#usinggetting-trimgalore-at-osc",
    "title": "Trimming with TrimGalore",
    "section": "2 Using/Getting TrimGalore at OSC",
    "text": "2 Using/Getting TrimGalore at OSC\nWe can use my Conda environment as follows:\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n```bash\ntrim_galore --version\n\n            Quality-/Adapter-/RRBS-/Speciality-Trimming\n                    [powered by Cutadapt]\n                        version 0.6.10\n\n                    Last update: 02 02 2023\nBut if you needed/wanted to install it for yourself, you could do so as follows:\nmodule load miniconda3/23.3.1-py310 # Load the latest version for installations\nconda create -y -n trimgalore -c bioconda trim-galore"
  },
  {
    "objectID": "modules/B03_trimgalore.html#trimgalore-syntax",
    "href": "modules/B03_trimgalore.html#trimgalore-syntax",
    "title": "Trimming with TrimGalore",
    "section": "3 TrimGalore syntax",
    "text": "3 TrimGalore syntax\ntrim_galore --help\n\n# NOTE: Below I am only showing (truncated) output for the key options!\n USAGE:\ntrim_galore [options] &lt;filename(s)&gt;\n\n-o/--output_dir &lt;DIR&gt;   If specified all output will be written to this directory instead of the current\n                        directory. If the directory doesn't exist it will be created for you.\n-j/--cores INT          Number of cores to be used for trimming [default: 1].\n--fastqc                Run FastQC in the default mode on the FastQ file once trimming is complete.\n--fastqc_args \"&lt;ARGS&gt;\"  Passes extra arguments to FastQC.\n-a/--adapter &lt;STRING&gt;   Adapter sequence to be trimmed. If not specified explicitly, Trim Galore will\n                        try to auto-detect whether the Illumina universal, Nextera transposase or Illumina\n                        small RNA adapter sequence was used.\n-q/--quality &lt;INT&gt;      Trim low-quality ends from reads in addition to adapter removal.\n--length &lt;INT&gt;          Discard reads that became shorter than length INT because of either\n                        quality or adapter trimming. A value of '0' effectively disables\n                        this behaviour. Default: 20 bp.\nThe line under “USAGE:” tells us that the FASTQ file names should be specified as arguments at the end of the command. It is important to realize that we will run TrimGalore for one sample at a time and therefore for two FASTQ files (forward/R1 and reverse/R2) at a time. By comparison, we were able to run FastQC simply for one FASTQ file at a time. So, without any options, and fictional FASTQ files A_R1.fastq.gz and A_R2.fastq.gz, our command would be trim_galore A_R1.fastq.gz A_R2.fastq.gz.\nWe will use TrimGalore with default settings for trimming of adapters (i.e. auto-detect, seems to always work), quality-trimming (minimum Phred quality score of 20), and read length (minimum of 20 bp), but it’s good to know we could easily change those if we wanted to.\nWe do want to specify the output directory, since it’s pretty annoying to get output files in the current working dir: we could for instance use --output_dir results/trimgalore.\nWe’ll typically also want to have TrimGalore run FastQC, since it will be good to check if adapter sequences were successfully removed and so on. We’ll also have to tell FastQC about the output dir or its files, too, will end up in our working dir — we can do so via --fastqc_args and when we use that argument, FastQC will already be run (no need to also use --fastqc). We could use --fastqc_args --outdir results/trimgalore/fastqc.\nFinally, we’ll usually want to specify the number of cores, and it should correspond to what we have available for our compute job. Since we have 1 core in the VS Code session, we’ll use --cores 1 in the test-run, but something else in our final script.\nA final test command to run TrimGalore on our actual (but small, subsetted) FASTQ files in data/fastq could therefore look as follows:\n# For clarity, using one argument/option per line\ntrim_galore \\\n    --output_dir results/trimgalore \\\n    --cores 1 \\\n    --fastqc_args \"--outdir results/trimgalore/fastqc\" \\\n    --paired data/fastq/ASPC1_A178V_R1.fastq.gz data/fastq/ASPC1_A178V_R2.fastq.gz\nLet’s try that out.\nYou should have gotten a lot of logging output, which was also saved …\nLet’s look at what the output files are:\nls -lhR results/trimgalore"
  },
  {
    "objectID": "modules/B03_trimgalore.html#variables-in-a-script-to-run-trimgalore",
    "href": "modules/B03_trimgalore.html#variables-in-a-script-to-run-trimgalore",
    "title": "Trimming with TrimGalore",
    "section": "4 Variables in a script to run TrimGalore",
    "text": "4 Variables in a script to run TrimGalore\nTo run TrimGalore efficiently at OSC, we will submit a compute job to run TrimGalore separately for each sample.\nWe’ll have to make our TrimGalore flexible such that it is able to take one or two arguments to specify the input FASTQ files. This can be one in several ways, but we’ll do it as follows: we’ll only pass an R1 FASTQ file to the script, which will then infer the name of the R2 file (by replacing _R1 with _R2). This is generally easiest because it allows us to simply loop over the R1 FASTQ files in our “runner script”.\nWhile we’re at it, we can also let the script accept an argument with the output directory name, something that is also good practice not to hardcode.\nLet’s start by just thinking about we would define this variables for a single sample, and how the TrimGalore command would change:\n# Variables defined upfront - passed as arguments to the script\nR1=data/fastq/ASPC1_A178V_R1.fastq.gz\noutdir=results/trimgalore\n\n# \"Derived\" variables\nn_cores=$SLURM_CPUS_ON_NODE\nR2=${R1/_R1/_R2}\n\n# The Trimgalore call:\ntrim_galore \\\n    --output_dir \"$outdir\" \\\n    --cores \"$n_cores\" \\\n    --fastqc_args \"--outdir \"$outdir\"/fastqc\" \\\n    --paired \"$R1\" \"$R2\""
  },
  {
    "objectID": "modules/B03_trimgalore.html#a-script-to-run-trimgalore",
    "href": "modules/B03_trimgalore.html#a-script-to-run-trimgalore",
    "title": "Trimming with TrimGalore",
    "section": "5 A script to run TrimGalore",
    "text": "5 A script to run TrimGalore\n#!/bin/bash\n#SBATCH --account=PAS0471\n#SBATCH --time=1:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --job-name=trimgalore\n#SBATCH --output=slurm-trimgalore-%j.out\n\n# Re-assign positional parameters\nR1=$1\noutdir=$2\n\n# Load the Conda environment\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# Use strict Bash settings\nset -euo pipefail\n\n# Infer derived variables\nn_cores=$SLURM_CPUS_ON_NODE\nR2=${R1/_R1/_R2}\n\n# Report\necho \"# Starting script trimgalore.sh\"\ndate\necho \"# Input R1 FASTQ file:      $R1\"\necho \"# Output dir:               $outdir\"\necho\n\n# Check that the output dir isn't the same as the input dir\n# This is because we will let the output files have the same name as the input files\nif [[ $(dirname \"$R1\") == \"$outdir\" ]]; then\n    echo \"# ERROR: Input dir is the same as the output dir ($outdir)\" \n    exit 1\nfi\n\n# Create the output dir\nmkdir -p \"$outdir\" \"$outdir\"/fastqc\n\n# Run TrimGalore\ntrim_galore \\\n    --output_dir \"$outdir\" \\\n    --cores \"$n_cores\" \\\n    --fastqc_args \"--outdir $outdir/fastqc\" \\\n    --paired \"$R1\" \"$R2\"\n\n# Rename output files\necho -e \"\\n# Renaming the output files:\"\nfile_id=$(basename \"$R1\" _R1.fastq.gz)\nR1_out=\"$outdir\"/\"$file_id\"_R1_val_1.fq.gz\nR2_out=\"$outdir\"/\"$file_id\"_R2_val_2.fq.gz\nmv -v \"$R1_out\" \"$outdir\"/\"$file_id\"_R1.fastq.gz\nmv -v \"$R2_out\" \"$outdir\"/\"$file_id\"_R2.fastq.gz\n\n# Report\necho -e \"\\n# Done with script trimgalore.sh\"\ndate\necho -e \"\\n# Listing files in the output dir:\"\nls -lh \"$outdir\""
  },
  {
    "objectID": "modules/A05_fastq.html#overview-setting-up",
    "href": "modules/A05_fastq.html#overview-setting-up",
    "title": "FASTQ files",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nOver the next two sessions, you’ll be introduced to three very common types of genomic data files, all of which you should have when you start your analysis of, for example, a reference-based RNAseq dataset.\nWe’ll talk about the following three types of genomic data file:\n\nFASTQ files: When you get your sequence data back, it will be in this format, which contains one entry per read, and has per-base quality scores along with the sequence itself.\nFASTA files: Simpler sequence files, where each entry contains just a header and a DNA or protein sequence. Your reference genome assembly will be in this format.\nGTF (& GFF) files: These contain annotations in a tabular format, e.g. the start & stop position of each gene.\n\nThis session will focus on FASTQ files, which you should already have in your practice directory (if not, see the instructions below). In the next session, we will download and explore reference genome FASTA and GFF files.\nYou’ll also learn a number of new shell concepts and commands, such as commands to work with gzip-compressed files and grep to search for text.\nThe at-home reading below will cover how you can make important data read-only, and how you can check whether your data files were not accidentally modified upon transfer.\n\nStart VS Code and open your folder\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\nMake sure to open your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir, either by using the Open Folder menu item, or by clicking on this dir when it appears in the Welcome tab.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nIn the Welcome tab under Recent, you should see your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir listed: click on that to open it. Alternatively, use      =&gt;   File   =&gt;   Open Folder to open that dir in VS Code.\n\n\n\n\n\n\n\n\n\n\nDon’t have your own dir with the data? (Click to expand)\n\n\n\n\n\nIf you missed the last session, or deleted your rnaseq_intro dir entirely, run these commands to get a (fresh) copy of all files:\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\nAnd if you do have an rnaseq_intro dir, but you want to start over because you moved or removed some of the files while practicing, then delete the dir before your run the commands above:\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\nYou should have at least the following files in this dir:\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md"
  },
  {
    "objectID": "modules/A05_fastq.html#the-fastq-format",
    "href": "modules/A05_fastq.html#the-fastq-format",
    "title": "FASTQ files",
    "section": "1 The FASTQ format",
    "text": "1 The FASTQ format\nFASTQ is a very common output format of high-throughput sequencing machines — at least from Illumina sequencing, you will almost always receive the sequences in this format. Like most genomic data files, these are plain text files, and each sequence that is read by the sequencer (i.e., each “read”) forms one FASTQ entry represented by four lines. The lines contain, respectively:\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe sequence itself\nA + (plus sign)\nOne-character quality scores for each base in the sequence\n\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than regular Illumina read lengths.\n\n\n\nThe “Q” in FASTQ stands for “quality”, to contrast this format with FASTA, a more basic and generic format that does not include base quality scores. FASTQ files have the extension .fastq or .fq, but they are very commonly gzip-compressed, in which case their name ends in .fastq.gz or .fq.gz.\n\n\n\n\n\n\nUnderstanding FASTQ quality scores (Click to expand)\n\n\n\n\n\nThe bottom line (starting with JJJJ) in the figure above contains a quality score for each base of the sequence in the second line. Specifically, the characters on that line correspond to a numeric Phred quality score (Q), which is defined as:\n\nQ = -10 * log10(P)\n\nIn the equation above, P is the estimated probability that a base call is erroneous — see the table below for some specific probabilities and their rough qualitative interpretation for Illumina data:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent\n\n\n\nWith Illumina data, these Phred quality scores typically go up to 42 (though newer platforms like NovaSeq used binned quality scores). This numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character”, which allows for a single-character representation of each possible score.\nAs a consequence, each quality score character can conveniently correspond to (i.e., line up with) a base in the sequence: e.g., the 30th base number in a DNA sequence line corresponds to the 30th quality score character two lines below that.\nIn practice, you almost never have to manually check the quality scores of bases in FASTQ files, but if you do want to know which character corresponds to which quality, this Illumina webpage has a table (though note that Q41 = J and Q42 = K are missing).\n\n\n\n\nIllumina sequencing is by far the most common for RNAseq1, and can be done in two “modes”: single-end and paired-end. With paired-end sequencing, each individual (c)DNA fragment is sequenced from two ends in opposite directions (with the total size of the fragment –“insert size”– determining whether these two reads will overlap). The resulting two, “paired”, reads for each fragment are inextricably linked, with the first called the forward read and the second called the reverse read.\n\n\n\n\nHow a DNA fragment prepared for paired-end Illumina sequencing will be sequenced. In this example, the insert size is such that the forward and reverse reads do not overlap.\n\n\n\nMost commonly for paired-end data, forward and reverse reads are placed in separate files. As such, you’ll usually have two files2 for each sample that was sequenced:\n\nThe file with forward reads has _R1 (or sometimes _1) in its name, e.g. ASPC1_A178V_R1.fastq.gz\nThe file with reverse reads has _R2 (or sometimes _2) in its name, e.g. ASPC1_A178V_R2.fastq.gz (i.e., the file name should be identical to that for the forward read, except for the read direction identifier).\n\nBecause the forward and reverse reads are so tightly linked, R1 and R2 files for one sample should always contain the same number of reads. (It is possible to end up with “orphan” reads, such as after one member of the pair is removed by quality filtering, but these are either discarded or saved in separate files.)"
  },
  {
    "objectID": "modules/A05_fastq.html#working-with-compressed-fastq-files",
    "href": "modules/A05_fastq.html#working-with-compressed-fastq-files",
    "title": "FASTQ files",
    "section": "2 Working with compressed (FASTQ) files",
    "text": "2 Working with compressed (FASTQ) files\nLet’s start by reminding ourselves about our own FASTQ files — recall that this is a subset of the full dataset, with way fewer reads reads per file than the originals, and with files for only four of the samples:\nls -lh data/fastq\ntotal 38M\n-rw-r--r-- 1 jelmer PAS0471 4.1M Jul 27 11:53 ASPC1_A178V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.2M Jul 27 11:53 ASPC1_A178V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.1M Jul 27 11:53 ASPC1_G31V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.3M Jul 27 11:53 ASPC1_G31V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.1M Jul 27 11:53 Miapaca2_A178V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.3M Jul 27 11:53 Miapaca2_A178V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.1M Jul 27 11:53 Miapaca2_G31V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.3M Jul 27 11:53 Miapaca2_G31V_R2.fastq.gz\nThe FASTQ files all have a .gz extension (and should listed in red in your terminal), indicating they are “gzip-compressed”. This is a common type of compression for large genomic files.\nNext, we’ll take a peak inside one of these files. It’s worth mentioning that during actual FASTQ file analysis (QC, trimming, alignment), you’re at most very rarely interacting with the raw contents of the file, since bioinformatics programs take care of the details. While it is therefore possible to run your analyis while treating FASTQ (and other genomic) files as “black boxes”, it is helpful to at least have a rough idea of what a FASTQ file looks like, and how you can get some quick stats on them using basic shell commands.\nLet’s consider our options to look inside one of these files. cat will print the contents of the entire file to screen, which doesn’t sound like a good idea for such a large file. Other than less, perhaps head will work well here? We’ll try to print 8 lines, which should show us two reads:\nhead -n 8 data/fastq/ASPC1_A178V_R1.fastq.gz\n\n\n\nOuch, what was that?! What happened here is that we are directly seeing the contents of the compressed file, which is simply not human-readable. Other shell commands, like those that count the number of lines (wc -l), or search for text (grep, see below), would also run into trouble with compressed files3.\nTo get around all of this, we might be inclined to uncompress these files, which we could do with the gunzip command. However, uncompressed files take up several times as much disk storage space as compressed ones, and this can add up to a lot when we’re talking about FASTQ files in particular. Fortunately, there are other approaches available. First of all, almost any bioinformatics tool will accept compressed FASTQ files. Additionally, the following commands and strategies should help you to do some basic exploration of your FASTQ files without uncompressing them.\n\n\n\n\n\n\nConcatenating compressed files (Click to expand)\n\n\n\n\n\nLike with FASTA files, multiple FASTQ files can simply be concatenated to produce a valid, larger FASTQ file.\nIt generally doesn’t make sense to concatenate files from different samples, but a relatively common situation in which you might want to concatenate FASTQs is when you have multiple files for each sample, originating from different Illumina “lanes”.\nIn the previous session, we saw that it is very easy to concatenate files in the shell using cat, even if there are many of them and/or the files are very large.\nConcatenating compressed files is also really straightforward — the example below would concatenate two files from different Illumina lanes (L001 and L002), separately for the forward and reverse reads:\n# Fictional example; note that we concatenate the R1 and R2 files separately\ncat sampleA_L001_R1.fastq.gz sampleA_L002_R1.fastq.gz &gt; sampleA_R1.fastq.gz\ncat sampleA_L001_R2.fastq.gz sampleA_L002_R2.fastq.gz &gt; sampleA_R2.fastq.gz\n\n\n\n\n\n2.1 less / zless\nAs you may recall from the previous session, we were actually able to directly view a compressed FASTQ file with less! less also has a zless counterpart that is explicitly meant to display gzip-compressed files, but at least at OSC, less itself will also work:\nless data/fastq/ASPC1_A178V_R1.fastq.gz\n# (Recall, press 'q' to quit less)\n\n\n\n\n\n\nAvoid line-wrapping by less\n\n\n\nDepending on your zoom level and the length of reads in your FASTQ file, some lines may contain too many characters to fit on your screen. If that’s the case, less will by default “wrap” those lines onto the next line on your screen, so characters won’t run off the screen on the right-hand side. That may be useful when the file contains text you’re trying to read in full, but it is often confusing for files like FASTQ as well as for tabular files.\nTo turn off line-wrapping, call less with the -S option:\nless -S data/fastq/ASPC1_A178V_R1.fastq.gz\n\n\n\n\n\n2.2 zcat\nLike the less/zless pair, cat has a counterpart that will print the uncompressed contents of a compressed file: zcat.\nOf course, we just established that cat’s behavior of printing all the contents of a file isn’t great for large FASTQ files. But zcat is nevertheless very useful for FASTQ files when we combine it with the pipe, |. For example, we can pipe the output of zcat to the head command to view the first 8 lines (2 reads) as follows:\nzcat data/fastq/ASPC1_A178V_R1.fastq.gz | head -n 8\n@A00195:432:HC7JWDRX3:2:1241:25093:24627 1:N:0:CCGATCGT+GACTGTTT\nGAACTAACCAAAGTGTGCTGAACATTATTAGGCTCTCCAGACATGTCTAGTTTATACTGGTAAATGCATATCAATTGTAAATATAAAAATAAAATTTGCAT\n+\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFF:FFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFF:F:F,F:FF\n@A00195:432:HC7JWDRX3:1:1232:30716:27226 1:N:0:CCGATCGT+GACTGTTT\nGTCCGGACCTTCTTGACGTAAGTGACATCTGGGTGGTGTTTGGGCGGCATGAGCAGCAGATGCAGCCGCTCATAGAACTGGATCCCGTTAAGGGAGGTGAC\n+\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\nAlong the same lines, we can count the number of lines in a FASTQ file like so:\nzcat data/fastq/ASPC1_A178V_R1.fastq.gz | wc -l\n400000\n\nYour Turn: The number of reads in a FASTQ file\n\nGiven the output of the command above, how many reads are in this FASTQ file?\nWhat line count do you get when you run wc -l directly on the compressed file?\n\n\n\nSolutions\n\n\nThere are 400,000 / 4 = 100,000 reads in this file\nYou’ll get a line count of 15,358, quite a ways off from the 400,000 lines in the uncompressed file! So, don’t do this when you want to count lines/reads!\n\nwc -l data/fastq/ASPC1_A178V_R1.fastq.gz \n15358 data/fastq/ASPC1_A178V_R1.fastq.gz\n\n\n\nYour Turn: Getting the read length (advanced)\nWhat is the read length for these FASTQ files? Can you get the answer using the commands we have covered so far?\nTwo extra pieces of information:\n\nwc -c will return the number of characters in its input.\nNearly all reads in Illumina FASTQ files have the same length, so you can examine the length of a single read to get your answer.\n\n\n\nHints\n\n\nRecall that the number of characters in e.g. the second (and the fourth) line of a FASTQ file corresponds to the read length.\nThink about how you might be able to use head and tail, each with the -n option, and back-to-back, to get a single, specific line from a file.\nString all four commands together in a single line using the pipe |.\n\n\n\n\nSolution\n\nzcat data/fastq/ASPC1_A178V_R1.fastq.gz | head -n 2 | tail -n 1 | wc -l\n102\nYou can extract any line from a text file by its line number N using head -n N | tail -n 1: the last line from head’s output will be the line of interest, which you can then isolate with tail -n 1.\nOne (annoying) little twist here is that wc -c will include the newline “character” in its count: as such, the read length is 102 - 1 = 101."
  },
  {
    "objectID": "modules/A05_fastq.html#finding-text-with-grep-zgrep",
    "href": "modules/A05_fastq.html#finding-text-with-grep-zgrep",
    "title": "FASTQ files",
    "section": "3 Finding text with grep / zgrep",
    "text": "3 Finding text with grep / zgrep\ngrep allows you to search a file for any text or text patterns. By default, it will return the entire line whenever it finds a match, and it is often set up (including at OSC) to highlight, within that line, the matching text in bold red in its output.\nFor example, let’s say we wanted to print the sequences in our FASTQ file that contain “ACCGATACG”:\nzcat data/fastq/ASPC1_A178V_R1.fastq.gz | grep \"ACCGATACG\"\n\n\n\nIf we wanted to know how many reads contain a certain sequence (e.g, the shorter and therefore more common “CCAGTA”), we can simply pipe grep’s output into wc -l:\nzcat data/fastq/ASPC1_A178V_R1.fastq.gz | grep \"CCAGTA\" | wc -l\n1118\ngrep has many options — one of those is -c, which will directly count matching lines instead of printing them (i.e., the command below is an alternative to the command above where we piped greps output into wc -l):\nzcat data/fastq/ASPC1_A178V_R1.fastq.gz | grep -c \"CCAGTA\"\n1118\n\ngrep even has a zgrep counterpart for gzip-compressed files, so the above can be further shortened by omitting zcat and passing the FASTQ file name as an argument to zgrep:\nzgrep -c \"CCAGTA\" data/fastq/ASPC1_A178V_R1.fastq.gz\n1118\nWe could also create a new FASTQ file whose sequences match our search by printing one line before each match (-B1) and two lines after it (-A2):\nzgrep -B1 -A2 --no-group-separator \"CCAGTA\" data/fastq/ASPC1_A178V_R1.fastq.gz &gt; CCAGTA.fastq \nAmong many other options, grep can also search multiple files at once (simply pass multiple filenames as arguments like with other commands, e.g. using a glob pattern to select them), and it can even search directories recursively using the -R flag."
  },
  {
    "objectID": "modules/A05_fastq.html#at-home-reading-raw-data-management",
    "href": "modules/A05_fastq.html#at-home-reading-raw-data-management",
    "title": "FASTQ files",
    "section": "At-home reading: raw data management",
    "text": "At-home reading: raw data management\n\nMaking your valuable data read-only\nYour raw FASTQ data is extremely invaluable as it contains the result of your experiment and was produced by an expensive sequencing process. Even after you’ve produced “clean versions” of these files after quality and adapter trimming, or after you have generated your final gene count tables, you’ll always want to keep these files around. For example, these are the foremost files you need to make publicly available when you publish your results (they are typically deposited at the NCBI’s SRA), they ensure that your results can be reproduced by yourself and others, and they allow for a modified re-analysis of the data after e.g. new methods or relevant data become available.\nYou should therefore keep at least one copy of your FASTQ files in a Project dir (as opposed to a Scratch dir) at OSC — recall that these dirs are being backed up daily by OSC! You’ll also want to keep at least one copy of the data outside of OSC.\nEven with those automatic back-ups and extra copies, it’s still good practice to be careful with your copy of the data at OSC. In the previous session, we learned that file removal with (e.g.) rm is permanent and irreversible, and performing an accidental removal of some of your FASTQ files or the entire dir with a wrong command is not that far-fetched. To make this much less likely to happen, it’s a good idea to “write-protect” your FASTQ files (or, phrased differently, to make them “read-only”).\nFirst, let’s briefly recap and expand our knowledge of file permissions:\n\nRead permissions allow you to read and copy files/dirs\nWrite permissions allow you to move, rename, modify, overwrite, or delete\nExecute permissions allow you to directly execute a file (e.g. running a program, or a script as a command).\n\nThese permissions can be most easily set for three different groups of people:\n\nOwner (or “user”) — By default, this the person that created the file or dir. After you have copied or downloaded some FASTQ files, for example, you are the owner of these copies.\nGroup — When you create a file in the PAS0471 project dir, its “group” will include all members of the OSC project PAS0471.\nOther — In the example above, anyone with access to OSC that is not a member of PAS0471.\n\nWe can see what these permissions are for any file or dir by looking at the first column of the output of ls -l. Let’s practice write-protection and its consequences with a dummy file — first, we create the dummy file and check the default permissions:\ntouch permission_test_file\n\nls -l permission_test_file\n# (Note, the very first dash below is about file _type_, not permissions:)\n -rw-r--r-- 1 jelmer PAS0471 0 Aug  7 16:27 permission_test_file\n\n\n\n\n\nThe command to set (read/write/execute) permissions for these categories of people (user/group/others, or everyone together) is chmod. There are multiple ways of using this command, but a common one is along the lines of chmod &lt;who&gt;&lt;operation&gt;&lt;permission&gt;, with e.g.:\n\nchmod a-w meaning “all minus write”: remove write-permissions for all\nchmod o+r meaning “others plus read”: add read-permissions for others.\n\nTo remove write-permission for everyone, including yourself, we will use:\nchmod a-w permission_test_file\n\nls -l permission_test_file\n# (Notice that there is no longer a 'w' in the sequence below:)\n-r--r--r-- 1 jelmer PAS0471 0 Aug  7 16:27 permission_test_file\nNow, let’s see what happens when we try to remove this file:\nrm permission_test_file\nrm: remove write-protected regular empty file ‘permission_test_file’? n\nNote that we can still remove this file by answering y, but we will hopefully not act that carelessly, and the question will make us reconsider and press n (also, if you made a mistake in a script that you run non-interactively, it will fail to remove write-protected files).\nFinally, let’s actually write-protect our FASTQ files, and the reference genome files:\nchmod a-w data/fastq/* data/ref/*\n\n\n\n\n\n\nAccess permissions for directories (Click to expand)\n\n\n\n\n\nOne tricky aspects of file permissions is that to list a directory’s content, or to cd into it, you need execute permissions for the dir (and not read permissions as you might have expected)! This is something to take into account when you want to grant others access to a directory at OSC.\nTo set execute permissions only for dirs throughout a dir hierarchy:\nchmod -R a+X my_large_dir       # Note the *uppercase* X\nMore intuitive is that when you don’t have write-permissions for a dir, it’s not just that you can’t delete or modify any of the files in it, but you’re also not able to create or copy your own files there. Therefore, if you want someone to obtain their own copy of one of your files, you’ll typically have to get them to run the cp command.\n\n\n\n\n\n\nChecking file integrity after transfer\nWhen you receive your FASTQ files from a sequencing facility, a small text file will usually accompany your FASTQ files, and will have a name along the lines of md5.txt, md5checksums.txt, or shasums.txt.\nSuch a file contains so-called checksums, a sort of digital fingerprints for your FASTQ files, which can be used to check whether your copy of these files is completely intact. Checksums are extremely compact summaries of files, computed so that even if just one character is changed in the data, the checksum will be different.\nFor example, in the dir with the original FASTQ files for our focal project, the following md5.txt file is present:\nls -lh /fs/ess/PAS0471/jelmer/assist/2023-08_hy/data/fastq | head -n 6\ntotal 48G\n-rw-r--r-- 1 jelmer PAS0471 3.4K Aug  9 16:33 md5.txt\n-rw-r--r-- 1 jelmer PAS0471 1.2G Aug  9 16:33 X10784_Cruz-MonserrateZ_ASPC1_vec_V1N_1_S25_R1_001.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 1.2G Aug  9 16:33 X10784_Cruz-MonserrateZ_ASPC1_vec_V1N_1_S25_R2_001.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 1.3G Aug  9 16:33 X10785_Cruz-MonserrateZ_ASPC1_RASD1_V1N_1_S26_R1_001.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 1.4G Aug  9 16:33 X10785_Cruz-MonserrateZ_ASPC1_RASD1_V1N_1_S26_R2_001.fastq.gz\n\n\n\n\n\n\nMore on checksums\n\n\n\nSeveral algorithms and their associated shell commands can compute checksums. Like in our case, you’ll most often see md5 checksums accompany genomic data files, which can be computed and checked with the md5sum command (the newer SHA-1 checksums can be computer and checked with the very similar shasum command).\nChecksums consist of hexadecimal characters only: numbers and the letters a-f.\nWe typically compute or check checksums for one or more files, but we can even do it for a string of text — the example below demonstrates that the slightest change in a string (or file alike) will generate a completely different checksum:\necho \"bioinformatics is fun\" | md5sum\n010b5ebf7e207330de0e3fb0ff17a85a  -\necho \"bioinformatic is fun\" | md5sum\n45cc2b76c02b973494954fd664fc0456  -\n\n\nLet’s take a look at our checksums — the file has one row per file and two columns, the first with the checksum and the second with the corresponding file name:\nhead -n 4 /fs/ess/PAS0471/jelmer/assist/2023-08_hy/data/fastq/md5.txt\n54224841f172e016245843d4a8dbd9fd        X10790_Cruz-MonserrateZ_Panc1_vec_V1N_1_S31_R2_001.fastq.gz\ncf22012ae8c223a309cff4b6182c7d62        X10790_Cruz-MonserrateZ_Panc1_vec_V1N_1_S31_R1_001.fastq.gz\n647a4a15c0d55e56dd347cf295723f22        X10797_Cruz-MonserrateZ_Miapaca2_RASD1_V1N_1_S38_R2_001.fastq.gz\nce5d444f8f9d87d325dbe9bc09ef0470        X10797_Cruz-MonserrateZ_Miapaca2_RASD1_V1N_1_S38_R1_001.fastq.gz\nThis file was created by the folks at the sequencing facility, and now that we have the data at OSC and are ready to analyze it, we can check if they are still fully intact and didn’t –for example– get incompletely transferred.\nI have done this check for the original files, but this takes a little while, and for a quick exercise, we can now do so with our subsampled FASTQ files. First, let’s copy a file md5.txt from the demo directory, which has the checksums for the subsampled FASTQ files as I created them:\ncp /fs/ess/PAS0471/demo/202307_rnaseq/data/fastq/md5sums.txt data/fastq/\nTo check whether the checksums for one or more files in a file correspond to those for the files, we can run mdsum -c &lt;mdsum-file&gt;, and should do so while inside the dir with the files of interest4. For example:\ncd data/fastq\nmd5sum -c md5sums.txt \nASPC1_A178V_R1.fastq.gz: OK\nASPC1_A178V_R2.fastq.gz: OK\nASPC1_G31V_R1.fastq.gz: OK\nASPC1_G31V_R2.fastq.gz: OK\nMiapaca2_A178V_R1.fastq.gz: OK\nMiapaca2_A178V_R2.fastq.gz: OK\nMiapaca2_G31V_R1.fastq.gz: OK\nMiapaca2_G31V_R2.fastq.gz: OK\nIf there were any differences, the md5sum command would clearly warn you about them, as you can see in the exercise below.\n\n\n\n\n\n\nMaking the checksum check fail (Click to expand)\n\n\n\n\n\nLet’s compute a checksum for the README.md file and save it in a file:\n# Assuming you went into data/fastq above;\n# you need to be in /fs/ess/PAS0471/$USER/rnaseq-intro\ncd ../..\n\nmd5sum README.md &gt; md5sum_for_README.txt\n\ncat md5sum_for_README.txt\nd4c4a8df4870f68808553ac0f5484aa3  README.md\nNow, let’s add a line to our README.md that says where the reference genome files are:\n# (You'll need single quotes like below, or the shell will interpret the backticks)\necho 'Files for the GRCh38.p14 human genome are in the `data/ref` dir' &gt;&gt; README.md\n\ntail -n 3 README.md\nand columns specifying the read direction, sample ID, cell line, and variant.\n\nFiles for the GRCh38.p14 human genome are in the `data/ref` dir\nFinally, let’s check the checksum, and watch it fail:\nmd5sum -c md5sum_for_README.txt\nREADME.md: FAILED\nmd5sum: WARNING: 1 computed checksum did NOT match"
  },
  {
    "objectID": "modules/A05_fastq.html#footnotes",
    "href": "modules/A05_fastq.html#footnotes",
    "title": "FASTQ files",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough long-read technologies like Oxford Nanopore Technologies and especially PacBio are picking up steam, since these can sequence transcripts in full.↩︎\nIn some cases, most often when you simply do a lot of sequencing for each sample, you’ll get more than one pair of files: one pair for each “lane” of the sequencer on which the sample was sequenced.↩︎\nAnd they would fail in a possibly worse way because you might overlook the problem: wc -l will return a count yet this is not the number of lines for the uncompressed file, whereas a text search with grep may “falsely” return nothing while that text is present in the uncompressed file.↩︎\nThis technically depends on how the file names are shown in the text file with the checksums: if there are just file names without directories (or ./&lt;filename&gt;, etc.), you’ll have to be in the dir with the files to run md5sum -c. (This in turn depends on from where the checksums were generated: if you generate them while in the dir with the focal files, which is the only sensible way to do this, that’s how they will be displayed.)↩︎"
  },
  {
    "objectID": "modules/A10_slurm.html#overview-setting-up",
    "href": "modules/A10_slurm.html#overview-setting-up",
    "title": "Slurm batch jobs",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nWe have so far been working interactively at OSC, issuing our commands directly in the terminal. But in order to run some actual genomics analyses, we will want to run scripts non-interactively and submit them to the compute job queue at OSC.\nAutomated scheduling software allows hundreds of people with different requirements to access compute nodes effectively and fairly. For this purpose, OSC uses the Slurm scheduler (Simple Linux Utility for Resource Management).\nA temporary reservation of resources on compute nodes is called a compute job. What are the options to start a compute job at OSC?\n\n“Interactive Apps” — We can start programs with GUIs, such as VS Code, RStudio or Jupyter Notebook on the OnDemand website, and they will run in a browser window.\nInteractive shell jobs — Start a interactive shell on a compute node.\nBatch (non-interactive) jobs — Run a script on a compute node without ever going to that node yourself.\n\nWe’ve already worked a lot with the VS Code Interactive App, and the at-home reading at the bottom of this page will talk about interactive shell jobs. But what we’re most interested in here is running batch jobs, which will be the focus of this session.\n\nStart VS Code and open your folder\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\nMake sure to open your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir, either by using the Open Folder menu item, or by clicking on this dir when it appears in the Welcome tab.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nIn the Welcome tab under Recent, you should see your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir listed: click on that to open it. Alternatively, use      =&gt;   File   =&gt;   Open Folder to open that dir in VS Code.\n\n\n\n\n\n\n\n\n\n\nDon’t have your own dir with the data? (Click to expand)\n\n\n\n\n\nIf you missed the last session, or deleted your rnaseq_intro dir entirely, run these commands to get a (fresh) copy of all files you should have so far:\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\nAnd if you do have an rnaseq_intro dir, but you want to start over because you moved or removed some of the files while practicing, then delete the dir before your run the commands above:\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\nYou should have at least the following files in this dir:\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── md5sums.txt\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md\n│   └── ref\n│       ├── GCF_000001405.40.fna\n│       ├── GCF_000001405.40.gtf"
  },
  {
    "objectID": "modules/A10_slurm.html#getting-started-with-slurm-batch-jobs",
    "href": "modules/A10_slurm.html#getting-started-with-slurm-batch-jobs",
    "title": "Slurm batch jobs",
    "section": "1 Getting started with Slurm batch jobs",
    "text": "1 Getting started with Slurm batch jobs\nIn requesting batch jobs, we are asking the Slurm scheduler to run a script on a compute node1. When doing so, we stay in our current shell at our current node (whether that’s a login or compute node), and the script will run on a (different) compute node “out of sight”. Also, as we’ll discuss in more detail below:\n\nScript output that would normally be printed to screen (“standard out”) will end up in a “log” file\nThere are commands for e.g. monitoring whether the job is already/stillrunning, and cancelling the job.\n\n\n\n1.1 The sbatch command\nWe use Slurm’s sbatch command to submit a batch job. Recall from the Bash scripting session that we can run a Bash script as follows:\nbash sandbox/printname.sh Jane Doe\nFirst name: Jane\nLast name: Doe\n\n\n\n\n\n\nCan’t find yesterday’s printname.sh script?\n\n\n\n\n\n\nOpen a new file in the VS Code editor (     =&gt;   File   =&gt;   New File) and save it as printname.sh\nCopy the code below into the script:\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\nThe above command ran the script on our current node. To instead submit the script to the Slurm queue, we start by simply replacing bash by sbatch:\nsbatch sandbox/printname.sh Jane Doe\nsrun: error: ERROR: Job invalid: Must specify account for job  \nsrun: error: Unable to allocate resources: Unspecified error\nHowever, that didn’t work. As the error message “Must specify account for job” tries to tell us, we need to indicate which OSC Project (or as Slurm puts it, “account”) we want to use for this compute job.\nTo specify the project/account, we can use the --account= option followed by the OSC Project number:\nsbatch --account=PAS0471 sandbox/printname.sh Jane Doe\nSubmitted batch job 12431935\nThis means that our job was successfully submitted (No further output will be printed to your screen - we’ll talk more about that below). The job has a unique identifier among all compute jobs by all users at OSC, and we can use this number to monitor and manage it. All of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nsbatch options and script arguments\n\n\n\nAs you perhaps noticed in the command above, we can use sbatch options and script arguments in one command, in the following order:\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\nBut, depending on the details of the script itself, all combinations of using sbatch options and script arguments are possible:\n\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS0471 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS0471 printname.sh Jane Doe  # Both sbatch option and script arguments\n\nNot using the --account option, as in the first two examples above, is possible when we specify this option inside the script, as we’ll see below.\n\n\n\n\n\n1.2 Adding sbatch options in scripts\nThe --account= option is just one of out of many options we can use when reserving a compute job, but is the only one that always has to be specified (including for batch jobs and for Interactive Apps).\nDefaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1). These options are all specified in the same way for interactive and batch jobs, and we’ll dive into them below.\nInstead of specifying Slurm/sbatch options on the command-line when we submit the script, we can also add these options inside the script.\nThis is handy because even though we have so far only seen the account= option, you often want to specify several options. That would lead to very long sbatch commands. Additionally, it can be practical to store a script’s typical Slurm options along with the script itself, so you don’t have to remember them.\nWe add the options in the script using another type of special comment line akin to the shebang line, marked by #SBATCH. The equivalent of adding --account=PAS0471 after sbatch on the command line is a line in a script that reads #SBATCH --account=PAS0471.\nJust like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one such line to the printname.sh script, such that the first few lines read:\n\n#!/bin/bash\n#SBATCH --account=PAS0471\n\nset -ueo pipefail\n\n# (This is a partial script, don't run this directly in the terminal)\n\nAfter having added this to the script, we can run our earlier sbatch command without options:\n\nsbatch printname.sh Jane Doe\n\n\nSubmitted batch job 12431942\n\nAfter we submit the batch job, we immediately get our prompt back. Everything else (job queuing and running) will happen out of our immediate view. This allows us to submit many jobs at the same time — we don’t have to wait for other jobs to finish (or even to start).\n\n\n\n\n\n\nsbatch option precedence\n\n\n\nAny sbatch option provided on the command line will override the equivalent option provided inside the script. This is sensible: we can provide “defaults” inside the script, and change one or more of those when needed on the command line.\n\n\n\n\n\n\n\n\nRunning a script with #SBATCH in other contexts\n\n\n\nBecause #SBATCH lines are special comment lines, they will simply be ignored and not throw any errors when you run a script that contains them in other contexts: when not running them as a batch job at OSC, or even when running them on a computer without Slurm installed.\n\n\n\n\n\n1.3 Where does the output go?\nAbove, we saw that when we ran the printname.sh script directly, its output was printed to the screen, whereas when we submitted it as a batch job, all that was sprinted to screen was Submitted batch job 12431942. So where did our output go?\nOur output ended up in a file called slurm-12431942.out: that is, slurm-&lt;job-number&gt;.out. Since each job number is unique to a given job, your file would have a different number in its name. We might call this type of file a Slurm log file.\n\n\n\n\n\n\nAny idea why we might not want batch job output printed to screen, even if we could? (Click to expand)\n\n\n\n\n\nThe power of submitting batch jobs is that you can submit many at once — e.g. one per sample, running the same script. If the output from all those scripts ends up on your screen, things become a big mess, and you have no lasting record of what happened.\n\n\n\nLet’s take a look at the contents of the Slurm log file with the cat command:\n# (Replace the number in the file name with whatever you got! - check with 'ls')\ncat slurm-12431942.out\nFirst name: Jane  \nLast name: Doe\nThis file simply contains the output of the script that was printed to screen when we ran it with bash — nothing more and nothing less.\nIt’s important to conceptually distinguish between two overall types of output that a script may have:\n\nOutput that is printed to screen when we directly run a script, such as what was produced by our echo statements, by any errors that may occur, and possibly by a program that we run in the script.2 As we saw, this output ends up in the Slurm log file when we submit the script as a batch job.\nOutput of commands inside the script that we redirect to a file (&gt; myfile.txt) or that automatically goes to an output file rather than being printed to screen. This type of output will always end up in the very same files regardless of whether we run the script directly (with bash) or as a batch job (with sbatch).\n\n\n\n\n\n\n\nThe working directory stays the same\n\n\n\nBatch jobs start in the directory that they were submitted from: that is, your working directory remains the same."
  },
  {
    "objectID": "modules/A10_slurm.html#monitoring-batch-jobs",
    "href": "modules/A10_slurm.html#monitoring-batch-jobs",
    "title": "Slurm batch jobs",
    "section": "2 Monitoring batch jobs",
    "text": "2 Monitoring batch jobs\n\n2.1 A sleepy script for practice\nLet’s use the following short script to practice monitoring and managing batch and other compute jobs. Open a new file in the VS Code editor (     =&gt;   File   =&gt;   New File) and save it as sandbox/sleep.sh, then copy the following into it:\n\n#!/bin/bash\n#SBATCH --account=PAS0471\n\necho \"I will sleep for 30 seconds\" &gt; sleep.txt\nsleep 30s\necho \"I'm awake!\"\n\n\n\nYour turn: Batch job output recap\nPredict what would happen if you submit the sleep.sh script as a batch job (using sbatch sandbox/sleep.sh):\n\nHow many output files will this batch job produce?\nWhat will be in each of those files?\nIn which directory will the file(s) appear?\nIn terms of output, what would have been different if we had run the script directly, i.e. using the command bash sandbox/sleep.sh?\n\nThen, can test your predictions by running the script.\n\n\n\n\n\n\nSolutions (Click to expand)\n\n\n\n\n\n\nThe script will produce 2 files:\n\nslurm-&lt;job-number&gt;.out: The Slurm log file, containing output that would have otherwise been printed to the screen\nsleep.txt: Containing the output that we redirected to this file in the script\n\nThey will contain:\n\nslurm-&lt;job-number&gt;.out: I’m awake!\nsleep.txt: “I will sleep for 30 seconds”\n\nBoth files will end up in your current working directory. Slurm log files always go to the directory from which you submitted the job. Slurm jobs also run from the directory from which you submitted your job, and since we redirected the output simply to sleep.txt, that file was created in our working directory.\nIf we had run the script directly, the slept.txt would have also been created with the same content, but “All done!” would have been printed to the screen.\n\n\n\n\n\n\n\n2.2 Checking the status of our job\nAfter we submit a job, it may be initially be waiting to be allocated resources: i.e., it may be “queued” or “pending”. Then, the job will start running, and at some point it will stop running, either because the script ran into and error, or because it completed.\nHow can we check the status of our batch job? We can do so using the Slurm command squeue -u $USER -l, in which:\n\nOur username is specified with the -u option (without this, we would see everyone’s jobs)\nWe used the environment variable $USER so that the very same code will work for everyone (you can also simply type your username if that’s shorter or easier).\nWe’ve added the -l (lowercase L, not the number 1!) option to get more verbose (“long”) output.\n\nLet’s try that:\nsqueue -u $USER -l\nMon Aug 21 15:47:42 2023\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          23640814 condo-osu ondemand   jelmer  RUNNING       6:34   2:00:00      1 p0133\nAfter a line with the date and time, and a header line, you should see some information about a single compute job, as shown above: this is the Interactive App job that runs VS Code. That’s not a “batch” job, but it is a compute job, and all compute jobs are listed here. In the table, we can see the following pieces of information about each job:\n\nJOBID — the job ID number,\nPARTITION — type of queue - here, we can tell it was submitted through OnDemand\nThe NAME of the job\nThe USER who submitted the job\nThe STATE of the job, which is usually either PENDING (queued) or RUNNING. (As soon as a job finished, it will disappear from this list!)\nTIME — for how long the job has been running (here as minutes:seconds)\nThe TIME_LIMIT — the amount of time you reserved for the job (here as hours:minutes:seconds)\nThe number of NODES reserved for the job\nNODELIST(REASON) — When a job is running, this will show the ID of the node on which it is running. When a job is pending, it will (sort of) say why it is pending.\n\nLet’s also try this after submitting our sleep.sh script as a batch job:\nsbatch sandbox/sleep.sh\nSubmitted batch job 12431945\nWe may be able to catch the STATE being PENDING before the job starts:\nsqueue -u $USER -l\nMon Aug 21 15:48:26 2023\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          12520046 serial-40 sleep.sh   jelmer  PENDING       0:00   1:00:00      1 (None)\n          23640814 condo-osu ondemand   jelmer  RUNNING       7:12   2:00:00      1 p0133\nBut soon enough it should say RUNNING in the STATE column:\nsqueue -u $USER -l\nMon Aug 21 15:48:39 2023\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          12520046 condo-osu sleep.sh   jelmer  RUNNING       0:12   1:00:00      1 p0133\n          23640814 condo-osu ondemand   jelmer  RUNNING       8:15   2:00:00      1 p0133\nThe script should finish after 30 seconds (our command was sleep 30s), after which the job will immediately disappear from the squeue listing — only pending and running jobs are shown:\nsqueue -u $USER -l\nMon Aug 21 15:49:26 2023\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          23640814 condo-osu ondemand   jelmer  RUNNING       9:02   2:00:00      1 p0133\nBut we need to check our output file(s) to see if our script ran successfully!\ncat sleep.txt\nI will sleep for 30 seconds\ncat slurm-12520046.out\nI'm awake!\n\n\n\n2.3 Cancelling jobs (and other monitoring/managing commands)\nSometimes, you want to cancel one or more jobs, because you realize you made a mistake in the script or you used the wrong input files. You can do so using scancel:\n\nscancel 2979968        # Cancel job number 2979968\nscancel -u $USER       # Cancel all your jobs\n\n\n\n\n\n\n\nOther job monitoring commands and options\n\n\n\n\nCheck only a specific job by specifying the job ID, e.g 2979968:\n\nsqueue -j 2979968\n\nOnly show running (not pending) jobs:\n\nsqueue -u $USER -t RUNNING\n\nUpdate Slurm directives for a job that has already been submitted (this can only been done before the job has started running):\n\nscontrol update job=&lt;jobID&gt; timeLimit=5:00:00\n\nHold and release a pending (queued) job, e.g. when needing to update input file before it starts running:\nscontrol hold jobID        # Job won't start running until released\nscontrol release jobID     # Job is free to start\nYou can see more details about any running or finished jobs, including the amount of time it ran for:\n\nscontrol show job 2526085   # For job 2526085\n\n# UserId=jelmer(33227) GroupId=PAS0471(3773) MCS_label=N/A\n# Priority=200005206 Nice=0 Account=pas0471 QOS=pitzer-default\n# JobState=RUNNING Reason=None Dependency=(null)\n# Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n# RunTime=00:02:00 TimeLimit=01:00:00 TimeMin=N/A\n# SubmitTime=2020-12-14T14:32:44 EligibleTime=2020-12-14T14:32:44\n# AccrueTime=2020-12-14T14:32:44\n# StartTime=2020-12-14T14:32:47 EndTime=2020-12-14T15:32:47 Deadline=N/A\n# SuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-12-14T14:32:47\n# Partition=serial-40core AllocNode:Sid=pitzer-login01:57954\n# [...]"
  },
  {
    "objectID": "modules/A10_slurm.html#common-sbatch-options",
    "href": "modules/A10_slurm.html#common-sbatch-options",
    "title": "Slurm batch jobs",
    "section": "3 Common sbatch options",
    "text": "3 Common sbatch options\nFirst off, note that many Slurm options have a corresponding long (--account=PAS0471) and short format (-A PAS0471), which can generally be used interchangeably. For clarity, we’ll stick to long format options here.\n\n3.1 --account: The OSC project\nAs seen above. Always specify the project when submitting a batch job.\n\n\n3.2 --time: Time limit (“wall time”)\nUse the --time option to specify the maximum amount of time your job will run for. Your job gets killed as soon as it hits the specified time limit!\nWall time is a term meant to distinguish it from, say “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16. Some notes:\n\nYou will only be charged for the time your job actually used, not what you reserved.\nThe default time limit is 1 hour. Acceptable time formats include:\n\nminutes (e.g. 60 =&gt; 60 minutes)\nhours:minutes:seconds (e.g. 1:00:00 =&gt; 60 minutes)\ndays-hours (e.g. 2-12 =&gt; two-and-a-half days)\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\n\nAn example, where we ask for 1 hour:\n\n#!/bin/bash\n#SBATCH --time=1:00:00\n\n\n\n\n\n\n\nWhen in doubt, ask for more time\n\n\n\nIf you are uncertain about how much time your job will take (i.e., how long it will take for your script / the program in your script to finish), then ask for more or even much more time than you think you will need. This is because queuing times are generally not long at OSC, and because you won’t be charged for reserved-but-not-used time.\nThat said, in general, the “bigger” (more time, more cores, more memory) your job is, the more likely it is that it will be pending for an appreciable amount of time. Smaller jobs (requesting up to a few hours and cores) will almost always start running nearly instantly. Even big jobs (requesting, say, a day or more and 10 or more cores) will often do so, but during busy times, you might have to wait for a while.\n\n\n\nYour turn: exceed the time limit\nModify the sleep.sh script to make it run longer than the time you request for it with --time. (Take into account that it does not seem to be possible to effectively request a job that runs for less than 1 minute.)\nIf you succeed in exceeding the time limit, an error message will be printed — where do you think it will go? After waiting for the job to be killed after 60 seconds, check if you were correct and what the exact error message is.\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\nThis script would do the trick, where we request 1 minute of walltime while we let the script sleep for 80 seconds:\n#!/bin/bash\n#SBATCH --account=PAS0471\n#SBATCH --time=1\n\necho \"I will sleep for 80 seconds\" &gt; sleep.txt\nsleep 80s\necho \"I'm awake!\"\nThis would result in the following type of error:\nslurmstepd: error: *** JOB 23641567 ON p0133 CANCELLED AT 2023-08-21T16:35:24 DUE TO TIME LIMIT ***\n\n\n\n\n\n\n\n3.3 --mem: RAM memory\nUse the --mem option to specify the maximum amount of RAM (Random Access Memory) that your job can use:\n\nThe default amount is 4 GB per core that you reserve. This is often enough, so it is fairly common to omit the --mem option.\nThe default unit is MB (MegaBytes) — append G for GB (i.e. 100 means 100 MB, 10G means 10 GB).\nLike with the time limit, your job gets killed when it hits the memory limit. Whereas you get a very clear Slurm error message when you hit the time limit (as seen in the exercise above), hitting the memory limit can result in a variety of errors, but look for keywords such as Killed, Out of Memory / OOM, and Core Dumped, as well as actual “dumped cores” in your working dir (large files with names like core.&lt;number&gt;, these can be deleted).\n\nFor example, to request 20 GB of RAM:\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n\n3.4 Cores (& nodes and tasks)\nThere are several options to specify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing!\n\nSlurm for the most part uses “core” and “CPU” interchangeably3. More generally, “thread” is also commonly used interchangeably with core/CPU4.\n\n\nRunning a program that uses multiple threads/cores/CPUs (“multi-threading”) is common. In such cases, specify the number of threads/cores/CPUs n with --cpus-per-task=n (and keep --nodes and --ntasks at their defaults of 1).\nThe program you’re running may have an argument like --cores or --threads, which you should then set to n as well.\n\n\n\n\n\n\n\nUncommon cases\n\n\n\n\nOnly ask for more than one node when a program is parallelized with e.g. “MPI”, which is uncommon in bioinformatics.\nFor jobs with multiple processes (tasks), use --ntasks=n or --ntasks-per-node=n — also quite rare!\n\n\n\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\n\nAn example, where we ask for 2 CPUs/cores/threads:\n\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n\n\n\n\n3.5 --output: Slurm log files\nAs we saw above, by default, all output from a script that would normally be printed to screen will end up in a Slurm log file when we submit the script as a batch job. This file will be created in the directory from which you submitted the script, and will be called slurm-&lt;job-number&gt;.out, e.g. slurm-12431942.out.\nBut it is possible to change the name of this file. For instance, it can be useful to include the name of the program that the script runs, so that it’s easier to recognize this file later.\nWe can do this with the --output option, e.g. --output=slurm-fastqc.out if we were running FastQC.\nHowever, you’ll generally want to keep the batch job number in the file name too5. Since we won’t know the batch job number in advance, we need a trick here — and that is to use %j, which represents the batch job number:\n\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n\n\n\n\n\n\n\nstdout and stderr\n\n\n\nBy default, two output streams “standard output” (stdout) and “standard error” (stderr) are printed to screen and therefore also both end up in the same Slurm log file, but it is possible to separate them into different files.\nBecause stderr, as you might have guessed, often contains error messages, it could be useful to have those in a separate file. You can make that happen with the --error option, e.g. --error=slurm-fastqc-%j.err.\nHowever, reality is more messy: some programs print their main output not to a file but to standard out, and their logging output, errors and regular messages alike, to standard error. Yet other programs use stdout or stderr for all messages.\nI therefore usually only specify --output, such that both streams end up in that file."
  },
  {
    "objectID": "modules/A10_slurm.html#at-home-reading-sbatch-option-overview-interactive-jobs",
    "href": "modules/A10_slurm.html#at-home-reading-sbatch-option-overview-interactive-jobs",
    "title": "Slurm batch jobs",
    "section": "At-home reading: sbatch option overview & interactive jobs",
    "text": "At-home reading: sbatch option overview & interactive jobs\n\nTable with sbatch options\nThis includes all the discussed options, and a couple more useful ones:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nProject to be billed\n-A PAS0471\n--account=PAS0471\nN/A\n\n\nTime limit\n-t 4:00:00\n--time=4:00:00\n1:00:00\n\n\nNr of nodes\n-N 1\n--nodes=1\n1\n\n\nNr of cores\n-c 1\n--cpus-per-task=1\n1\n\n\nNr of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr of tasks per node\n-\n--ntasks-per-node\n1\n\n\nMemory limit per node\n-\n--mem=4G\n(4G)\n\n\nLog output file (%j = job number)\n-o\n--output=slurm-fastqc-%j.out\n\n\n\nError output (stderr)\n-e\n--error=slurm-fastqc-%j.err\n\n\n\nJob name (displayed in the queue)\n-\n--job-name=fastqc\n\n\n\nPartition (=queue type)\n-\n--partition=longserial  --partition=hugemem\n\n\n\nGet email when job starts, ends, fails,  or all of the above\n-\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\n\nLet job begin at/after specific time\n-\n--begin=2021-02-01T12:00:00\n\n\n\nLet job begin after other job is done\n-\n--dependency=afterany:123456\n\n\n\n\n\n\n\nInteractive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. Working in an interactive shell job is operationally identical to working on a login node as we’ve been doing so far, but the difference is that it’s now okay to use significant computing resources. (How much and for how long depends on what you reserve.)\n\nUsing srun\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command6, which we can use with --pty /bin/bash added to get an interactive Bash shell.\n\nsrun --account=PAS0471 --pty /bin/bash\n\n\nsrun: job 12431932 queued and waiting for resources\nsrun: job 12431932 has been allocated resources\n[…regular login info, such as quota, not shown…]\n[jelmer@p0133 PAS0471]$\n\nThere we go! First some Slurm scheduling info was printed to screen: initially, the job was queued, and then it was “allocated resources”: that is, computing resources such as a compute node were reserved for the job. After that:\n\nThe job starts and because we’ve reserved an interactive shell job, a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nWe have now moved to the compute node at which our interactive job is running, so you should have a different p number in your prompt (And if you were on a login node before -but this is never the case if you are running VS Code through OnDemand-, your prompt switched from something like [jelmer@pitzer-login04 PAS0471]$)."
  },
  {
    "objectID": "modules/A10_slurm.html#footnotes",
    "href": "modules/A10_slurm.html#footnotes",
    "title": "Slurm batch jobs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther options: salloc works almost identically to srun, whereas sinteractive is an OSC convenience wrapper but with more limited options.↩︎\nTechnically, these are two different types of output, as we briefly touch on below: “standard output” and “standard error”.↩︎\nEven though technically, one CPU often contains multiple cores.↩︎\nEven though technically, one core often contains multiple threads.↩︎\nFor instance, we might be running the FastQC script multiple times, and otherwise those would all have the same name and be overwritten.↩︎\nOther options: salloc works almost identically to srun, whereas sinteractive is an OSC convenience wrapper but with more limited options.↩︎"
  },
  {
    "objectID": "info/about.html",
    "href": "info/about.html",
    "title": "About",
    "section": "",
    "text": "This website contains the material for a series of teaching sessions for grad students and postdocs of the Cruz-Monserrate lab in the fall of 2023.\nThese sessions focus on hands-on analysis of (short-read, bulk) RNAseq data with command-line tools and R using the computing resources at the Ohio Supercomputer Center (OSC).\nThis website and the teaching materials have been created by Jelmer Poelstra at the Molecular and Cellular Imaging Center (MCIC) of Ohio State University.\n\n\n\n Back to top"
  },
  {
    "objectID": "info/osc_ssh.html#introduction",
    "href": "info/osc_ssh.html#introduction",
    "title": "Connecting to OSC through SSH",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis page will first go through the basics of connecting to OSC with SSH, using the ssh command.\nIf you use ssh frequently, or plan to do so, the next two sections will show you two useful tricks:\n\nAvoid being prompted for your OSC password\nSet up a shortcut for your SSH connection name.\n\nFinally, something more specific but incredibly useful if you like VS Code, and have it installed locally, is to SSH-tunnel VS Code to OSC.\n\n\n\n\n\n\nSSH and Unix shells on Windows\n\n\n\n\n\nIf you have a Windows computer, you can instead use a GUI-based SSH client like PuTTY.\nAlternatively, for an experience more similar to that if you did have a Unix-based operating system, you can install either of the following, which will enable you to get a terminal program that does run a Unix shell:\n\nWindows Subsystem for Linux (WSL) — the more involved option, this will basically run a Linux Virtual Machine on your computer.\nGit for Windows, which comes with a Unix (Bash) shell — “Git Bash”. To install this, download it from this page and install it using all the default settings for the installation, except:\n\nIn “Adjusting Your PATH Environment”, select “Use Git from Git Bash Only”.\nIn the prompt “Configuring the Line Ending Conversions”, choose “Checkout as-is, commit as-is”."
  },
  {
    "objectID": "info/osc_ssh.html#basic-ssh-connection-in-a-terminal",
    "href": "info/osc_ssh.html#basic-ssh-connection-in-a-terminal",
    "title": "Connecting to OSC through SSH",
    "section": "2 Basic SSH connection in a terminal",
    "text": "2 Basic SSH connection in a terminal\nTo connect to OSC or other remote computers without using a web portal like OnDemand, you can use SSH. You can do so via the ssh command if you have a Linux or a Mac computer, since these two operating systems are both Unix-based and have built-in terminals with Unix shells.\nHere, I’ll briefly demonstrate how to use the ssh command. On your own computer, open a terminal application and input the command ssh &lt;user&gt;@&lt;host&gt;, where:\n\n&lt;user&gt; should be replaced by your OSC username, and\n&lt;host&gt; should be replaced by the name of the computer you want to connect to:\n\npitzer.osc.edu to connect to the Pitzer cluster\nowens.osc.edu to connect to the Owens cluster\n\n\nFor example, if I (username jelmer) wanted to log in to the Pitzer cluster, I would use:\nssh jelmer@pitzer.osc.edu\nThe authenticity of host 'pitzer.osc.edu' can't be established.\nRSA key fingerprint is 2a:b6:f6:8d:9d:c2:f8:2b:8c:c5:03:06:a0:f8:59:12.\nAre you sure you want to continue connecting (yes/no)?\nIf this is the first time you are connecting to Pitzer via SSH, you’ll encounter a message similar to the one above. While the phrase “The authenticity of host ‘pitzer.osc.edu’ can’t be established.” sounds ominous, you will always get this warning when you attempt to connect to a remote computer for the first time, and you should type yes to proceed (you then won’t see this message again).\nYou should now be prompted for your password. Type it in carefully because no characters or even *s will appear on the screen, and then press Enter.\njelmer@pitzer.osc.edu's password:\nIf you entered your password correctly, your shell is now connected to OSC rather than operating on your own computer. That is, you’ll have shell access very much in the same way as when using the “Pitzer Shell Access” button on OSC OnDemand. (The key difference between SSH-ing in this way rather than using OnDemand is that the terminal is not running inside your browser, which can be convenient.)\n\n\n\n\n\n\nSSH shortcuts\n\n\n\nIf you use SSH a lot to connect to OSC, typing ssh &lt;username&gt;@pitzer.osc.edu every time and then providing your password can get pretty tedious. The next two sections will show you how to make this go faster."
  },
  {
    "objectID": "info/osc_ssh.html#avoid-being-prompted-for-your-osc-password",
    "href": "info/osc_ssh.html#avoid-being-prompted-for-your-osc-password",
    "title": "Connecting to OSC through SSH",
    "section": "3 Avoid being prompted for your OSC password",
    "text": "3 Avoid being prompted for your OSC password\nIf you take the following steps, you will no longer be promoted for your OSC password every time you log in to OSC using SSH.\nBoth steps should be done in a terminal on your local machine:\n\nGenerate a public-private SSH key-pair:\nssh-keygen -t rsa\nYou’ll get some output and will then be asked several questions, but in each case, you can just press Enter to select the default answer.\nTransfer the public key to OSC’s clusters:\n# Replace &lt;user&gt; by your username, e.g. \"ssh-copy-id jelmer@owens.osc.edu\"\nssh-copy-id &lt;user&gt;@owens.osc.edu\nssh-copy-id &lt;user&gt;@pitzer.osc.edu\n\nTest if it works by runnning:\n# Try connecting to Owens (once again, replace '&lt;user&gt;' by your username):\nssh &lt;user&gt;@owens.osc.edu\n\n# Try connecting to Pitzer (once again, replace '&lt;user&gt;' by your username):\nssh &lt;user&gt;@owens.osc.edu\n\n\n\n\n\n\nMore instructions\n\n\n\nSee also this Tecmint post in case you’re struggling."
  },
  {
    "objectID": "info/osc_ssh.html#use-a-shorter-name-for-your-ssh-connection",
    "href": "info/osc_ssh.html#use-a-shorter-name-for-your-ssh-connection",
    "title": "Connecting to OSC through SSH",
    "section": "4 Use a shorter name for your SSH connection",
    "text": "4 Use a shorter name for your SSH connection\nYou can easily set up alternative ways of referring to you SSH connection (i.e., “aliases”), such as shortening jelmer@pitzer.osc.edu to jp, by saving these aliases in a text file ~/.ssh/config, as shown below.\nThese two steps should both be done on your local machine:\n\nCreate a file called ~/.ssh/config:\ntouch ~/.ssh/config\nOpen the file in a text editor and add your alias(es) in the following format:\nHost &lt;arbitrary-alias-name&gt;    \n    HostName &lt;remote-address&gt;\n    User &lt;username&gt;\nFor instance, my file contains the following so as to connect to Pizer with jp and to Owens with jo:\nHost jp\n    HostName pitzer.osc.edu\n    User jelmer\n\nHost jo\n    HostName owens.osc.edu\n    User jelmer\n\n\nNow, you just need to use your, preferably very short, alias to log in — and if you did the previous no-password setup, you won’t even be prompted for your password!\nssh jp\nPerhaps even more conveniently, these shortcuts will also work with scp and rsync! For example:\nrsync ~/scripts op:/fs/scratch/PAS0471"
  },
  {
    "objectID": "info/osc_ssh.html#set-up-your-local-vs-code-to-ssh-tunnel-into-osc",
    "href": "info/osc_ssh.html#set-up-your-local-vs-code-to-ssh-tunnel-into-osc",
    "title": "Connecting to OSC through SSH",
    "section": "5 Set up your local VS Code to SSH tunnel into OSC",
    "text": "5 Set up your local VS Code to SSH tunnel into OSC\nIf you want to use VS Code to write code, have a shell, and interact with files at OSC directly, you don’t necessarily need to use the VS Code (Code Server) in OSC OnDemand. You can also make your local VS Code installation “SSH tunnel” into OSC.\nThis is a more convenient way of working because it’s quicker to start, will never run out of alotted time, and because you are not working inside a browser, you have more screen space and no keyboard shortcut interferences.\nThe set-up is pretty simple (see also these instructions if you get stuck), and should also work on Windows:\n\nIf necessary, install VS Code (instructions for Windows / Mac / Linux) on your computer, and open it.\nInstall the VS Code “Remote Development extension pack”: open the Extensions side bar (click the icon with the four squared in the far left), search for “Remote Development extension pack”, and click “Install”.\nOpen the Command Palette (F1 or Ctrl+ShiftP) and start typing “Remote SSH”.\nThen, select Remote-SSH: Connect to Host… and specify your SSH connection: e.g. ssh jelmer@pitzer.osc.edu (you’ll have to do this separately for Pitzer and Owens if you want to be able to connect to both).\nIf you did the no-password setup described above (recommended!), you shouldn’t be prompted for a password and VS Code will connect to OSC!\nIf you’re asked about the operating system of the host, select Linux, which is the operating system of the OSC clusters.\n\n\n\n\n\n\n\nWarning\n\n\n\nJust be aware that you’ll now be on a Login node (and not on a Compute node like when you use VS Code through OnDemand), so avoid running analyses directly in the terminal, and so on."
  },
  {
    "objectID": "info/osc_transfer.html#introduction",
    "href": "info/osc_transfer.html#introduction",
    "title": "Transferring files to and from OSC",
    "section": "1 Introduction",
    "text": "1 Introduction\nThere are several ways of transferring files between your computer and OSC:\n\n\n\n\n\n\n\n\n\n\nMethod\nTransfer size\nCLI or GUI\nEase of use\nFlexibility/options\n\n\n\n\nOnDemand Files menu\nsmaller (&lt;1GB)\nGUI\nEasy\nLimited\n\n\nRemote transfer commands\nsmaller (&lt;1GB)\nCLI\nModerate\nExtensive\n\n\nSFTP\nlarger (&gt;1GB)\nEither\nModerate\nLimited\n\n\nGlobus\nlarger (&gt;1GB)\nGUI\nModerate 1\nExtensive\n\n\n\nThis page will cover each of those in more detail below.\n\n\n\n\n\n\nDownload directly from the web using commands at OSC\n\n\n\nIf you need files that are at a publicly accessible location on the internet (for example, NCBI reference genome data), you don’t need to download these to your computer and then upload them to OSC.\nInstead, you can use commands for downloading files directly to OSC, like wget or curl. This will be covered in one of the main sessions."
  },
  {
    "objectID": "info/osc_transfer.html#ondemand-files-menu",
    "href": "info/osc_transfer.html#ondemand-files-menu",
    "title": "Transferring files to and from OSC",
    "section": "2 OnDemand Files menu",
    "text": "2 OnDemand Files menu\nFor small transfers (below roughly 1 GB), you might find it easiest to use the Upload and Download buttons in the OSC OnDemand “Files” menu — their usage should be pretty intuitive."
  },
  {
    "objectID": "info/osc_transfer.html#remote-transfer-commands",
    "href": "info/osc_transfer.html#remote-transfer-commands",
    "title": "Transferring files to and from OSC",
    "section": "3 Remote transfer commands",
    "text": "3 Remote transfer commands\nFor small transfers, you can also use a remote transfer command like scp, or a more advanced one like rsync or rclone. Such commands can provide a more convenient transfer method than OnDemand if you want to keep certain directories synced between OSC and your computer.\nThe reason you shouldn’t use this for very large transfers is that the transfer will happen using a login node.\n\n3.1 scp\nOne option is scp (secure copy), which works much like the regular cp command, including that you’ll need -r for recursive transfers.\nThe key difference is that we have to somehow refer to a path on a remote computer, and we do so by starting with the remote computer’s address, followed by :, and then the path:\n# Copy from remote (OSC) to local (your computer):\nscp &lt;user&gt;@pitzer.osc.edu:&lt;remote-path&gt; &lt;local-path&gt;\n\n# Copy from local (your computer) to remote (OSC)\nscp &lt;local-path&gt; &lt;user&gt;@pitzer.osc.edu:&lt;remote-path&gt;\nHere are two examples of copying from OSC to your local computer:\n# Copy a file from OSC to a local computer - namely, to your current working dir ('.'):\nscp jelmer@pitzer.osc.edu:/fs/ess/PAS0471/jelmer/mcic-scripts/misc/fastqc.sh .\n\n# Copy a directory from OSC to a local computer - namely, to your home dir ('~'):\nscp -r jelmer@pitzer.osc.edu:/fs/ess/PAS0471/jelmer/mcic-scripts ~\nAnd two examples of copying from your local computer to OSC:\n# Copy a file from your computer to OSC --\n# namely, a file in from your current working dir to your home dir at OSC:\nscp fastqc.sh jelmer@pitzer.osc.edu:~\n\n# Copy a file from my local computer's Desktop to the Scratch dir for PAS0471:\nscp /Users/poelstra.1/Desktop/fastqc.sh jelmer@pitzer.osc.edu:/fs/scratch/PAS0471\nSome nuances for remote copying:\n\nAs the above code implies, in both cases (remote-to-local and local-to-remote), you will issue the copying commands from your local computer.\nFor the remote computer (OSC), the path should always be absolute, whereas that for your local computer can be either relative or absolute.\nSince all files can be accessed at the same paths at Pitzer and at Owens, it doesn’t matter whether you use @pitzer.osc.edu or @owens.osc.edu in the scp command.\n\n\n\n\n\n\n\nTransferring directly to and from OneDrive\n\n\n\nIf your OneDrive is mounted on or synced to your local computer (i.e., if you can see it in your computer’s file brower), you can also transfer directly between OSC and OneDrive.\nFor example, the path to my OneDrive files on my laptop is:\n/Users/poelstra.1/Library/CloudStorage/OneDrive-TheOhioStateUniversity.\nSo if I had a file called fastqc.sh in my top-level OneDrive dir, I could transfer it to my Home dir at OSC as follows:\nscp /Users/poelstra.1/Library/CloudStorage/OneDrive-TheOhioStateUniversity jelmer@pitzer.osc.edu:~\n\n\n\n\n\n3.2 rsync\nAnother option, which I can recommend, is the rsync command, especially when you have directories that you repeatedly want to sync: rsync won’t copy any files that are identical in source and destination.\nA useful combination of options is -avz --progress:\n\n-a enables archival mode (among other things, this makes it work recursively).\n-v increases verbosity — tells you what is being copied.\n-z enables compressed file transfer (=&gt; generally faster).\n--progress to show transfer progress for individual files.\n\nThe way to refer to remote paths is the same as with scp. For example, I could copy a dir_with_results in my local Home dir to my OSC Home dir as follows:\nrsync -avz --progress ~/dir_with_results jelmer@owens.osc.edu:~\n\n\n\n\n\n\nTrailing slashes in rsync\n\n\n\nOne tricky aspect of using rsync is that the presence/absence of a trailing slash for source directories makes a difference for its behavior. The following commands work as intended — to create a backup copy of a scripts dir inside a dir called backup2:\n# With trailing slash: copy the *contents* of source \"scripts\" into target \"scripts\":\nrsync -avz scripts/ backup/scripts\n\n# Without trailing slash: copy the source dir \"scripts\" into target dir \"backup\"\nrsync -avz scripts backup\nBut these commands don’t:\n# This would result in a dir 'backup/scripts/scripts':\nrsync -avz scripts backup/scripts\n\n# This would copy the files in \"scripts\" straight into \"backup\":\nrsync -avz scripts/ backup"
  },
  {
    "objectID": "info/osc_transfer.html#sftp",
    "href": "info/osc_transfer.html#sftp",
    "title": "Transferring files to and from OSC",
    "section": "4 SFTP",
    "text": "4 SFTP\nThe first of two options for larger transfers is SFTP. You can use the sftp command when you have access to a Unix shell on your computer, and this what I’ll cover below.\n\n\n\n\n\n\nSFTP with a GUI\n\n\n\nIf you have Windows without e.g. WSL or Git Bash (see the top of the SSH page on this site for more details), you can use a GUI-based SFTP client instead like WinSCP, Cyberduck, or FileZilla. CyberDuck also works on Mac, and FileZilla works on all operating systems, if you prefer to do SFTP transfers with a GUI, but I won’t cover their usage here.\n\n\n\n4.1 Logging in\nTo log in to OSC’s SFTP server, issue the following command in your local computer’s terminal, substituting &lt;user&gt; by your OSC username:\nsftp &lt;user&gt;@sftp.osc.edu   # E.g., 'jelmer@sftp.osc.edu'\nThe authenticity of host 'sftp.osc.edu (192.148.247.136)' can't be established.\nED25519 key fingerprint is SHA256:kMeb1PVZ1XVDEe2QiSumbM33w0SkvBJ4xeD18a/L0eQ.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nIf this is your first time connecting to OSC SFTP server, you’ll get a message like the one shown above: you should type yes to confirm.\nThen, you may be asked for your OSC password, and after that, you should see a “welcome” message like this:\n******************************************************************************\n\nThis system is for the use of authorized users only.  Individuals using\nthis computer system without authority, or in excess of their authority,\nare subject to having all of their activities on this system monitored\nand recorded by system personnel.  In the course of monitoring individuals\nimproperly using this system, or in the course of system maintenance,\nthe activities of authorized users may also be monitored.  Anyone using\nthis system expressly consents to such monitoring and is advised that if\nsuch monitoring reveals possible evidence of criminal activity, system\npersonnel may provide the evidence of such monitoring to law enforcement\nofficials.\n\n******************************************************************************\nConnected to sftp.osc.edu.\nNow, you will have an sftp prompt (sftp&gt;) instead of a regular shell prompt.\nFamiliar commands like ls, cd, and pwd will operate on the remote computer (OSC, in this case), and there are local counterparts for them: lls, lcd, lpwd — for example:\n# NOTE: I am prefacing sftp commands with the 'sftp&gt;' prompt to make it explicit\n#       these should be issued in an sftp session; but don't type that part.\nsftp&gt; pwd\nRemote working directory: /users/PAS0471/jelmer\nsftp&gt; lpwd\nLocal working directory: /Users/poelstra.1/Desktop\n\n\n\n4.2 Uploading files to OSC\nTo upload files to OSC, use sftp’s put command.\nThe syntax is put &lt;local-path&gt; &lt;remote-path&gt;, and unlike with scp etc., you don’t need to include the address to the remote (because in an stfp session, you are simultaneously connected to both computers). But like with cp and scp, you’ll need the -r flag for recursive transfers, i.e. transferring a directory and its contents.\n# Upload fastqc.sh in a dir 'scripts' on your local computer to the PAS0471 Scratch dir:\nsftp&gt; put scripts/fastqc.sh /fs/scratch/PAS0471/sandbox\n\n# Use -r to transfer directories:\nsftp&gt; put -r scripts /fs/scratch/PAS0471/sandbox\n\n# You can use wildcards to upload multiple files:\nsftp&gt; put scripts/*sh /fs/scratch/PAS0471/sandbox\n\n\n\n\n\n\nsftp is primitive\n\n\n\nThe ~ shortcut to your Home directory does not work in sftp!\nsftp is generally quite primitive and you also cannot use, for example, tab completion or the recalling of previous commands with the up arrow.\n\n\n\n\n\n4.3 Downloading files from OSC\nTo download files from OSC, use the get command, which has the syntax get &lt;remote-path&gt; &lt;local-path&gt; (this is the other way around from put in that the remote path comes first, but the same in that both use the order &lt;source&gt; &lt;target&gt;, like cp and so on).\nFor example:\nsftp&gt; get /fs/scratch/PAS0471/mcic-scripts/misc/fastqc.sh .\n\nsftp&gt; get -r /fs/scratch/PAS0471/sandbox/ .\n\n\n\n4.4 Closing the SFTP connection\nWhen you’re done, you can type exit or press Ctrl+D to exit the sftp prompt."
  },
  {
    "objectID": "info/osc_transfer.html#globus",
    "href": "info/osc_transfer.html#globus",
    "title": "Transferring files to and from OSC",
    "section": "5 Globus",
    "text": "5 Globus\nThe second option for large transfers is Globus, which has a browser-based GUI, and is especially your best bet for very large transfers. Some advantages of using Globus are that:\n\nIt checks whether all files were transferred correctly and completely\nIt can pause and resume automatically when you e.g. turn off your computer for a while\nIt can be used to share files from OSC directly with collaborators even at different institutions.\n\nGlobus does need some setup, including the installation of a piece of software that will run in the background on your computer.\n\nGlobus installation and configuration instructions: Windows / Mac / Linux\nGlobus transfer instructions\nOSC’s page on Globus"
  },
  {
    "objectID": "info/osc_transfer.html#footnotes",
    "href": "info/osc_transfer.html#footnotes",
    "title": "Transferring files to and from OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut the initial setup for Globus is quite involved and a bit counterintuitive.↩︎\nFor simplicity, these commands are copying between local dirs, which is also possible with rsync.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RNAseq data analysis",
    "section": "",
    "text": "Schedule\n\nBlock A: Computing skills and genomics data files\n\n\n\nDate\nTopic & link\n\n\n\n\n2023-07-14\nIntroduction\n\n\n2023-07-21\nThe Ohio Supercomputer Center (OSC)\n\n\n2023-07-28\nVS Code & Unix shell (part I)\n\n\n2023-08-04\nUnix shell (part II)\n\n\n2023-08-11\nFASTQ files\n\n\n2023-08-18\nReference genome files (FASTA and GTF)\n\n\n2023-08-25\nOverview of next steps, variables, and loops\n\n\n2023-09-01\nShell scripting\n\n\n2023-09-08\nSoftware at OSC & recap\n\n\n2023-09-15\nSlurm batch jobs\n\n\n\n\n\n\nBlock B: From reads to counts\n\n\n\nDate\nTopic & link\n\n\n\n\n2023-09-22\nRead quality control with FastQC\n\n\n2023-09-29\nSummarizing QC outputs with MultiQC\n\n\n2023-10-06\nQuality and adapter trimming with TrimGalore\n\n\n2023-10-13\nRead alignment with STAR\n\n\n2023-10-20\nQuantification with Salmon\n\n\n2023-10-27\nThe Nextflow/nf-core rnaseq workflow (Part I)\n\n\n2023-11-03\nThe Nextflow/nf-core rnaseq workflow (Part II)\n\n\n\n\n\n\nBlock C: Analyzing the counts\n\n\n\n\n\n\n\nDate\nTopic & link\n\n\n\n\n2023-11-10\nCreating a DESeq2 object\n\n\n2023-11-17\nPCA and overall exploration of the count data\n\n\n2023-12-01\nDifferential expression analysis (Part I)\n\n\n2023-12-08\nDifferential expression analysis (Part II)\n\n\n2023-12-15\nGO enrichment analysis\n\n\n\nNo meetings on Nov 24 (Thanksgiving), Dec 22 and Dec 28.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "info/shell_commands.html#common-shell-commands-and-notation",
    "href": "info/shell_commands.html#common-shell-commands-and-notation",
    "title": "Common shell commands",
    "section": "Common shell commands and notation",
    "text": "Common shell commands and notation\nBelow is a list of common and useful-for-us Unix shell commands and features.\nWhile it’s not an exhaustive list, getting a grasp on the commands and features above will go a long way in allowing you to work in the Unix shell. We should see nearly all of these in action during our sessions.\nThey are grouped into some general categories, with small examples between parentheses:\n\nNavigating in the Terminal\n\npwd — returns (prints) your working directory\ncd — change working directory\n\nViewing Files\n\ncat — print the entire contents of a file\nhead — print the first lines of a file\ntail — print the last lines of a file\nless — view the contents of a file in a “pager” (press q to quit/exit!)\n\nManaging/Organizing Files\n\nls — list contents of directory\nmkdir — create a new directory\nrm — remove/delete a file or directory\ncp — copy files/directories to a new location\nmv — move/rename files/directories to a new location\n\nWorking With Compressed Files\n\ngzip/gunzip — compress/uncompress a file with gzip compression (.gz)\nunzip — uncompress a zip (.zip) file\nzcat — print the contents of a compressed file to the screen\n\nAssessing Files\n\nmd5/shasum — check file integrity via “checksums” (fingerprints) for a file\ngrep — search a text file for lines containing a pattern of text\nwc — return number of lines, words, characters in a file\n\nEditing Files (or other data)\n\nsort — Sort data, can sort a file by one or more columns (sort metadata/meta.tsv)\ncut — Select one or more columns from a tabular file (cut -f 1,3 metadata/meta.tsv)\nuniq — Exclude duplicate entries in a list (cut -f1 metadata/meta.tsv | uniq)\ntr — Substitute a character (class) for another (echo acgt | tr A-Z a-z)\nsed — A powerful & flexible command, but most often used to find-and-replace text.\nawk — A powerful & flexible command, most useful to work with tabular (e.g., CSV, TSV, GFF/GTF) files\n\nMiscellaneous\n\nwget — download a file from online (wget &lt;URL&gt;)\nman — get help (manual) for a command (man ls)\n\nSpecial Notation\n\n| — The pipe, to use the output of a command as the input for another command (ls | wc -l)\n~ — The path to your Home directory (cp data/fastq/* ~)\n. — Your current working dir (cp data/fastq/* .)\n.. — One directory up from your current working dir (cd ..)\n$USER — Your user name (echo $USER)\n$HOME — The path to your Home directory (echo $HOME)\n\nShell wildcards\n\n* — matches any number of any character, including nothing (ls *fastq.gz)\n? — matches any single character (ls *fast?)\n[] — matches a single character of those listed between brackets, e.g. [012] matches 0, 1, or 2\n[^] — matches a single character not listed between brackets, e.g. [0-9] excludes any numbers"
  },
  {
    "objectID": "info/shell_shortcuts.html#shell-keyboard-shortcuts",
    "href": "info/shell_shortcuts.html#shell-keyboard-shortcuts",
    "title": "Shell keyboard shortcuts",
    "section": "Shell keyboard shortcuts",
    "text": "Shell keyboard shortcuts\n\n\n\n\n\n\n\nShortcut\nCommand\n\n\n\n\nTab\nTab completion (double Tab shows options when there are multiple)\n\n\n⇧ / ⇩\nCycle through command history\n\n\nCtrl + C\nCancel/abort/kill current process or “incomplete command” (get prompt back)\n\n\nAlt/Escape+.\nPaste last argument of last command\n\n\nCtrl + D\nExit the current shell (/ interactive job) (exit)\n\n\nCtrl + L\nClear your screen (clear)\n\n\nCtrl/Command+C\nCopy selected text (can also be Ctrl+Shift+C)\n\n\nCtrl/Command++V\nPaste copied text (can also be Ctrl+Shift+V)\n\n\nCtrl+A\nMove cursor to beginning of line\n\n\nCtrl+E\nMove cursor to end of line\n\n\nCtrl+U\nCut text to beginning of line\n\n\nCtrl+K\nCut text to end of line\n\n\nCtrl+W\nCut previous word\n\n\nCtrl+Y\nPaste (“yank”) previously cut element\n\n\nCtrl + R\nEnter characters to search for in the history (repeat CTRL + R to keep going back, ENTER to put command in prompt)\n\n\n\n\nCtrl+U actually cut the text: “Yank” it back with Ctrl+Y.\n\n\nGetting the last word from the previous command\nThis is (perhaps surprisingly) very useful!\nCreate a directory for yourself using mkdir:\nmkdir /fs/ess/PAS1855/users/$USER/rnaseq_intro/sandbox/testdir\nMove into this dir using cd – after typing cd and a space, press Alt+. on Windows or Esc+. on a Mac:\ncd /fs/ess/PAS1855/users/$USER/rnaseq_intro/sandbox/testdir\nAnother press will go back to the last word of the second-to-last command, and so on. On Windows, you can keep Alt pressed while repeatedly pressing . (convenient), but on a Mac, you’ll need to release Esc in between successive presses."
  },
  {
    "objectID": "info/glossary.html",
    "href": "info/glossary.html",
    "title": "Glossary / Abbreviations",
    "section": "",
    "text": "Under construction\n\n\n\nThis page is still under construction."
  },
  {
    "objectID": "info/glossary.html#computing",
    "href": "info/glossary.html#computing",
    "title": "Glossary / Abbreviations",
    "section": "Computing",
    "text": "Computing\n\nOSC\nHPC\nCPU\nGPU\nCLI\nGUI\nShell\nUnix\nLinux"
  },
  {
    "objectID": "info/glossary.html#genomics",
    "href": "info/glossary.html#genomics",
    "title": "Glossary / Abbreviations",
    "section": "Genomics",
    "text": "Genomics\n\nFASTA\nFASTQ\nBAM/SAM\nGFF"
  },
  {
    "objectID": "info/resources.html#general-applied-bioinformatics-resources",
    "href": "info/resources.html#general-applied-bioinformatics-resources",
    "title": "Further Resources",
    "section": "General applied bioinformatics resources",
    "text": "General applied bioinformatics resources\n\nBuffalo Book\nPractical Computing for Biologists books"
  },
  {
    "objectID": "info/resources.html#rnaseq-analysis",
    "href": "info/resources.html#rnaseq-analysis",
    "title": "Further Resources",
    "section": "RNAseq analysis",
    "text": "RNAseq analysis\n\nTranscript-level analysis\nTBA"
  },
  {
    "objectID": "modules/intro.html#workflow-overview",
    "href": "modules/intro.html#workflow-overview",
    "title": "RNAseq data analysis: introduction",
    "section": "Workflow overview",
    "text": "Workflow overview"
  },
  {
    "objectID": "modules/intro.html#rnaseq-data-analysis",
    "href": "modules/intro.html#rnaseq-data-analysis",
    "title": "RNAseq data analysis: introduction",
    "section": "RNAseq data analysis",
    "text": "RNAseq data analysis\nRNAseq data analysis can be divided into two main parts:\n\nA bioinformatics-heavy part in which you generate gene counts from the raw reads.\nA more statistical part in which you analyze the count table to create lists of differentially expressed genes and enriched functional categories.\n\n\n\nPart I: From reads to count table\nThis part starts with the raw reads from the (typically Illumina) sequencing machine to eventually generate a table with expression counts for each gene by each sample. This part:\n\nIs usually done by sequentially running a series of programs with a command-line interface (CLI). Therefore, you typically use the Unix shell (command line) and shell scripts to do this.\nProcesses large amounts of data, and is generally not suitable to be run on a laptop or a desktop computer: you should use a high-performance computing (HPC) center or cloud computing. (We will use the Ohio Supercomputer Center, OSC.)\nIs quite standardized and therefore, a “pipeline” written for one dataset can be run for another one with minor changes, even if the datasets are from completely different experiments or different species.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause of the required technical skills and computing infrastructure, in combination with the standardized execution, there are some alternatives available to doing this by yourself step-by-step1:\n\nCompanies and university bioinformatics core facilities may be able to simply run this part for you.\nServices with graphical user interfaces (GUIs) are available, such as Galaxy.\nThese run the same command-line programs, but wrap their execution in a more user-friendly way.\n\nSuch options are especially worth considering when you have no plans or ambitions to do much other genomics work in the future – in other words, it may not be worth learning all the required technical skills just for one project.\nWhen you plan to do multiple genomics projects and/or are generally interested in gaining computing skills, it’s better to go ahead and learn to run these command-line programs yourself.\n\n\n\n\n\nPart II: Analyzing the count table\nIn this part, you will analyze the table with gene counts for each sample, for example to test for differential expression among groups (e.g., different treatments) and to test whether certain functional (GO, KEGG) gene categories have distinct expression patterns as a whole.\nThis part:\n\nIs typically run entirely in R, using a number of specialized R “packages”.\nIs not particularly “compute-intensive”: your count table is a text file of typically only a few Mb, and the analyses you’re running do not need much time or computer memory. As such, you can run this on your laptop or desktop computer. (Though we will do it at OSC, mainly for the sake of continuity.)\nIs much less standardized across projects: the details of the analysis depend a lot on your experimental design and what you’re interested in; in addition, initial results may influence your next steps, and so on."
  },
  {
    "objectID": "modules/intro.html#what-well-cover",
    "href": "modules/intro.html#what-well-cover",
    "title": "RNAseq data analysis: introduction",
    "section": "What we’ll cover",
    "text": "What we’ll cover\n\nComputing skills and genomic file formats\nMany of these computing skills are needed only for part I below.\n\nIntroduction to the Ohio Supercomputer Center (OSC)\nThe VS Code (Code Server) text editor / IDE\nIntroduction to the Unix shell (“command line” / “Bash”)\nShell scripts and loops\nThe SLURM compute job scheduler\nUsing and installing software at OSC\nInput genomic file formats relevant to RNAseq: FASTQ, FASTA, GTF/GFF\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlong the way, we’ll also learn about project organization and ensuring reproducibility.\nI won’t include a full-blown introduction to R, but will provide some learning resources for those of you with little R experience before we get to the relevant part of the RNAseq analysis.\n\n\n\n\n\nAnalysis part I: From sequence reads to gene counts\n\nRaw read QC with FastQC and MultiQC\nRead pre-processing with TrimGalore and SortMeRNA\nRead alignment to a reference genome with STAR (and BAM/SAM files)\nAlignment QC with (at least) MultiQC\nGene expression quantification with Salmon\n\n\n\nAnalysis part II: Analyzing gene counts in R\n\nGetting an overview of sample/group distinctiveness with a PCA\nDifferential expression analysis with {DESeq2}\nKEGG and GO enrichment analysis with {ClusterProfiler}"
  },
  {
    "objectID": "modules/intro.html#data-type-and-workflow-variations",
    "href": "modules/intro.html#data-type-and-workflow-variations",
    "title": "RNAseq data analysis: introduction",
    "section": "Data type and workflow variations",
    "text": "Data type and workflow variations\n\nReference-based versus de novo workflows\nWe will cover a “reference-based” RNAseq workflow: one where your focal organism has a reference genome assembly and annotation. “De novo” RNAseq workflows are necessary when you don’t have a reference genome. They are overall similar, but more time-consuming and bioinformatics-heavy, since you will first have to assemble a transcriptome from the RNAseq data itself.\n\n\nGene-level versus transcript-level counts, and short versus long reads\nWe will focus on generating and analyzing gene-level counts rather than transcript-level counts: that is, for each sample, we will obtain a single count for each gene even if that gene has multiple transcripts (isoforms). However, the program which we’ll use for counting (Salmon) can also generate transcript-level counts, and downstream transcript-level analysis is fairly similar too, though this certainly adds a level of complexity.\nAdditionally, we will use short-read (Illumina) sequencing data, for which transcript-level counts have much greater levels of uncertainty, since most reads cannot directly be assigned to a specific transcript. Consider using long reads, such as PacBio IsoSeq, if you’re interested in transcript-level inferences.\n\n\n“Bulk” versus single-cell RNAseq\nWe will focus on “bulk” RNAseq, where RNA was extracted from a large mixture of cells and possibly cell types. Single-cell RNAseq analysis is similar for the first part (generating counts), but differs more in the second part (count analysis)."
  },
  {
    "objectID": "modules/intro.html#footnotes",
    "href": "modules/intro.html#footnotes",
    "title": "RNAseq data analysis: introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdditionally, you can run standardized pipelines yourself, which wrap many individual steps into a single executable workflow. This especially becomes a time-efficient option once you know the computing basics, and also aids with reproducibility and following best-practices. For example, for RNAseq there is a Nextflow nf-core RNAseq pipeline. The steps we will run fill follow this pipeline closely – but in my opinion, for initial learning, it is better to go step-by-step without a formalized pipeline.↩︎"
  },
  {
    "objectID": "modules/A08_scripts.html#overview-setting-up",
    "href": "modules/A08_scripts.html#overview-setting-up",
    "title": "Shell Scripting",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nShell scripts enable us to run sets of commands non-interactively, which is useful:\n\nWhen a set of commands takes a long time to run and/or should be run many times.\nTo help keep our code clean and clear by using separate scripts for individual parts of our analysis pipeline.\nTo run our analyses as Slurm batch jobs at OSC, which will for instance to allow us to run analyses simultaneously for different samples.\n\nScripts form the basis for analysis pipelines and if we code things cleverly, it should be straightforward to rerun much of our project workflow after removing or adding some samples, with different parameter settings, and possibly even for an entirely different dataset.\n\nStart VS Code and open your folder\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\nMake sure to open your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir, either by using the Open Folder menu item, or by clicking on this dir when it appears in the Welcome tab.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nIn the Welcome tab under Recent, you should see your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir listed: click on that to open it. Alternatively, use      =&gt;   File   =&gt;   Open Folder to open that dir in VS Code.\n\n\n\n\n\n\n\n\n\n\nDon’t have your own dir with the data? (Click to expand)\n\n\n\n\n\nIf you missed the last session, or deleted your rnaseq_intro dir entirely, run these commands to get a (fresh) copy of all files you should have so far:\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\nAnd if you do have an rnaseq_intro dir, but you want to start over because you moved or removed some of the files while practicing, then delete the dir before your run the commands above:\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\nYou should have at least the following files in this dir:\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── md5sums.txt\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md\n│   └── ref\n│       ├── GCF_000001405.40.fna\n│       ├── GCF_000001405.40.gtf"
  },
  {
    "objectID": "modules/A08_scripts.html#a-basic-shell-script",
    "href": "modules/A08_scripts.html#a-basic-shell-script",
    "title": "Shell Scripting",
    "section": "1 A basic shell script",
    "text": "1 A basic shell script\n\n1.1 Our first script\nWe’ll be writing our shell scripts in the editor pane of VS Code. To create our first one, open a new file in the VS Code editor (     =&gt;   File   =&gt;   New File) and save it as printname.sh within the sandbox dir (shell scripts most commonly have the extension .sh).\nThen, type or paste the following inside the script in your editor pane (and not in the terminal):\necho \"This script will print a first and a last name\"\n\n\n\n\n\n\nAuto Save in VS Code\n\n\n\nAny changes you make to this and other files in the editor pane should be immediately, automatically saved by VS Code. If that’s not happening for some reason, you should see an indication of unsaved changes like a large black dot next to the script’s file name in the editor pane tab header (if that’s the case, see the box below on how to fix this).\n\n\n\n\n\n\nAuto Save not happening? (Click to expand)\n\n\n\n\n\nIf the file is not auto-saving, you can always save it manually (including with Ctrl/Cmd+S) like you would do in other programs.\nHowever, it may be convenient to turn Auto Save on: press Ctrl/Cmd+Shift+S to open the Command Palette and type “Auto Save”. You should see an option “Toggle Auto Save”: click on that.\n\n\n\n\n\nShell scripts mostly contain the same regular Unix shell (specifically, Bash shell) code that we have gotten familiar with, but have so far directly typed in the terminal. As such, our single line with an echo command already consitutes a functional shell script! One way of running the script is by typing bash followed by the path to the script:\nbash sandbox/printname.sh\nThis script will print a first and a last name\nThat worked! Although of course, the script doesn’t yet print any names like it “promises” to do, but we will add that functionality in a little bit.\nFirst, though, we’ll learn about two header lines that are good practice to add to every shell script.\n\n\n\n1.2 Shebang line\nWe use a so-called “shebang” line as the first line of a script to indicate which language our script uses. More specifically, this line tell the computer where to find the binary (executable) that will run our script.\nSuch a line starts with #!, basically marking it as a special type of comment. After that, we provide the location to the relevant program: in our case Bash (which is the specific type of shell we are using), which itself is just a program with a binary (executable) file that is located at /bin/bash on Linux and Mac computers.\n#!/bin/bash\nWhile not always strictly necessary, adding a shebang line to every shell script is good practice, especially when we want to submit our script to OSC’s Slurm queue.\n\n\n\n1.3 Shell script settings\nAnother boilerplate line that is good practice to add to your shell scripts will change some default Bash settings to safer alternatives.\n\nBad default shell settings\nThe following two default settings of the Bash shell are bad ideas inside scripts:\nFirst, as we’ve seen previously, when you reference a variable that does not exist, the shell will just replace that with nothing, and will not complain:\necho \"Hello, my name is $myname ...\"\nHello, my name is  ...\nIn scripts, this can lead to all sorts of downstream problems, because you very likely tried and failed to do something with an existing variable (e.g. you misspelled its name, or forgot to assign it altogether). Even more problematically, this can lead to potentially very destructive file removal, as the box below illustrates.\n\n\n\n\n\n\nAccidental file removal with unset variables (Click to expand)\n\n\n\n\n\nThe shell’s default behavior of ignoring (instead of giving an error) the referencing of unset variables can make you accidentally remove files as follows — and this is especially likely to happen inside scripts where we more commonly use variables and are not working interactively:\n\nUsing a variable, we try to remove some temporary files whose names start with tmp_:\ntemp_prefix=\"temp_\"\n\nrm \"$tmp_prefix\"*     # DON'T TRY THIS!\nUsing a variable, we try to remove a temporary directory:\ntempdir=output/tmp\n\nrm -r $tmpdir/*      # DON'T TRY THIS!\n\n\n\n\n\n\n\nAbove, the text specified the intent of the commands. What would have actually happened?\n\n\n\n\n\nIn both examples, there is a similar typo: temp vs. tmp, which means that we are referencing a (likely) non-existent variable.\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nIn the second example, along similar lines, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem: recall that a leading / in a path is a computer’s root directory. (-r makes the removal recursive and -f makes forces removal).\n\n\n\n\nBefore you get too scared of creating terrible damage, note that at OSC, you would not be able to remove any essential files (and more generally, any files that are not yours unless you’ve explicitly been given permission for this), since you don’t have the permissions to do so. On your own computer, this could be more genuinely dangerous, though even there, you would not be able to remove operating system files without specifically requesting “admin” rights.\n\n\n\nSecond, a Bash script keeps running after encountering errors. That is, if an error is encountered when running, say, line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, you might not notice an error somewhere in the middle if it doesn’t produce more errors downstream. But the downstream results from what we at that point might call a “zombie script” can still be completely wrong.\n\n\nSafer settings\nThe following three settings will make your Bash scripts more robust and safer. With these settings, the script terminates1 if:\n\nset -u — An unset (non-existent) variable is referenced.\nset -e — Almost any error occurs.\nset -o pipefail — An error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\nset -e -u -o pipefail     # (For in a script - don't run in the terminal)\nOr even more concisely:\nset -euo pipefail         # (For in a script - don't run in the terminal)\n\n\n\n\n1.4 Adding the header lines to our script\nLet’s add these lines to our printname.sh script, so it will now contain the following:\n#!/bin/bash\nset -euo pipefail\n\necho \"This script will print a first and a last name\"\nAnd let’s run it again:\nbash sandbox/printname.sh\nThis script will print a first and a last name\nThat didn’t change anything to the output, but at least we confirmed that the script still works.\n\n\n\n\n\n\nRunning the script without using the bash command (Click to expand)\n\n\n\n\n\nBecause our script has a shebang line, we have taken one step towards being able to execute the script without the bash command, simply using:\nsandbox/printname.sh\n(Or if the script was in our current working dir, using ./printname.sh. In that case the ./ is necessary to make it explicit that we are referring to a file name: otherwise, when running just printname.sh, the shell would look for a command or program of that name, and wouldn’t be able to find it.)\nHowever, this would also require us to “make the script executable”, which is beyond the scope of this material. But I’m mentioning it here because you might see this way of running scripts being used elsewhere."
  },
  {
    "objectID": "modules/A08_scripts.html#command-line-arguments-for-scripts",
    "href": "modules/A08_scripts.html#command-line-arguments-for-scripts",
    "title": "Shell Scripting",
    "section": "2 Command-line arguments for scripts",
    "text": "2 Command-line arguments for scripts\n\n2.1 Calling a script with arguments\nWhen you call a script to run it, you can pass command-line arguments to it, such as a file to operate on.\nThis is much like when you provide a command like ls with arguments:\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\n\n# (No need to run any of this, they are just syntax examples)\nAnd here is what it looks like to pass arguments to scripts:\n# Run scripts without any arguments:\nbash fastqc.sh                            # (Fictional script)\nbash sandbox/printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash fastqc.sh data/sampleA.fastq.gz      # 1 argument, a filename\nbash sandbox/printname.sh John Doe        # 2 arguments, strings representing names\n\n# (No need to run any of this, they are just syntax examples)\nIn the next section, we’ll see what happens inside the script with the arguments we pass to it.\n\n\n\n2.2 Placeholder variables\nInside the script, any command-line arguments that you pass to it are automatically available in “placeholder” variables. Any first argument will be assigned to the variable $1, any second argument will be assigned to $2, any third argument will be assigned to $3, and so on.\n\n\n\n\n\n\nIn the calls to fastqc.sh and printname.sh above, what are the placeholder variables and their values?\n\n\n\n\n\n\nIn bash fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash sandbox/printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\n\nHowever, these placeholder variables are not automagically used — the arguments passed to a script are merely made available in these variables. So, unless we explicitly include code in the script to do something with these variables, nothing else happens.\nTherefore, let’s add some code to our printname.sh script to “process” any first and last name that are passed to the script. For now, our script will simply echo the placeholder variables, so that we can see what happens:\n#!/bin/bash\nset -euo pipefail\n\necho \"This script will print a first and a last name\"\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\nNext, we’ll run the script, passing the arguments John and Doe:\nbash sandbox/printname.sh John Doe\nThis script will print a first and a last name\nFirst name: John\nLast name: Doe\n\n\nOn Your Own: Command-line arguments\nIn each case below, think about what might happen before you run the script. Then, run it, and if you didn’t make a successful prediction, try to figure out what happened instead.\n\nRun the script (sandbox/printname.sh) without passing arguments to it. (Keep in mind that we have the set -euo pipefail line in the script.)\nDeactivate (“comment out”) the line with set settings by inserting a # as the first character of that line. Then, run the script again without passing arguments to it.\nDouble-quote John Doe when you run the script, i.e. run bash sandbox/printname.sh \"John Doe\"\n\nTo get back to where we were, remove the # you inserted in the script in step 2 above.\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\n\nbash sandbox/printname.sh\n\nprintname.sh: line 4: $1: unbound variable\n\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\n\nbash sandbox/printname.sh\necho \"First name:\"\necho \"Last name:\"\nBeing commented out, the set line should read:\n#set -euo pipefail\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\n\nbash sandbox/printname.sh \"John Doe\"\necho \"First name: John Doe\"\necho \"Last name:\"\n\n\n\n\n\n\n2.3 Copying placeholders to variables with descriptive names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables as follows:\n\n#!/bin/bash\nset -euo pipefail\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nUsing descriptively named variables in your scripts has several advantages. It will make your script easier to understand for others and for yourself. It will also make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name\n$# contains the number of command-line arguments passed to the script\n\n\n\n\n\nOn Your Own: A script to print a specific line\nWrite a script that prints a specific line (identified by line number) from a file.\n\nOpen a new file and save it as sandbox/printline.sh\nStart with the shebang and set lines\nYour script takes two arguments: a file name ($1) and a line number ($2)\nCopy the $1 and $2 variables to descriptively named variables\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the top solution box.\nTest the script by printing line 4 from metadata/meta.tsv.\n\n\n\n\n\n\n\nSolution: how to print a specific line number (Click to expand)\n\n\n\n\n\nFor example, to print line 4 of metadata/meta.tsv directly:\nhead -n 4 metadata/meta.tsv | tail -n 1\nHow this command works:\n\nhead -n 4 metadata/meta.tsv will print the first 4 lines of metadata/meta.tsv\nWe pipe those 4 lines into the tail command\nWe ask tail to just print the last line of its input, which will in this case be line 4 of the original input file.\n\n\n\n\n\n\n\n\n\n\nFull solution (Click to expand)\n\n\n\n\n\n#!/bin/bash\nset -euo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\nTo run the script and make it print the 4th line of meta.tsv:\nbash sandbox/printline.sh metadata/meta.tsv 4\nASPC1_G31V      ASPC1   G31V"
  },
  {
    "objectID": "modules/A08_scripts.html#script-variations-and-enhancements",
    "href": "modules/A08_scripts.html#script-variations-and-enhancements",
    "title": "Shell Scripting",
    "section": "3 Script variations and enhancements",
    "text": "3 Script variations and enhancements\nIn this section, we will change our printline.sh script and a similar small utility script to make their behavior more like scripts that run a bioinformatics program: the script’s main output will end up in a file, but it prints extensive “logging” notes to the screen, so we can monitor what we’re doing.\n\n3.1 A script to serve as a starting point\nWe’ve learned that the head command prints the first lines of a file, whereas the tail command prints the last lines. Sometimes it’s nice to be able to quickly see “both ends” of a file, so let’s write a little script that can do that for us, as a starting point for the next few modifications.\nOpen a new file, save it as sandbox/headtail.sh, and add the following code to it:\n\n#!/bin/bash\nset -euo pipefail\n\ninput_file=$1\n\n# Print the first and last 2 lines of a file, separated by a line with \"---\"\nhead -n 2 \"$input_file\"\necho \"---\"\ntail -n 2 \"$input_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNext, let’s run our headtail.sh script:\nbash sandbox/headtail.sh metadata/meta.tsv\nsample_id       cell_line       variant\nASPC1_A178V     ASPC1   A178V\n---\nMiapaca2_G31V   Miapaca2        G31V\nMiapaca2_G31V   Miapaca2        G31V\n\n\n\n3.2 Redirecting output to a file\nSo far, the output of our scripts was printed to screen:\n\nIn printname.sh, we simply echo’d, inside sentences, the arguments passed to the script.\nIn headtail.sh, we printed the first and last few lines of a file with the head and tail commands.\n\nAll this output was printed to screen because that is the default output mode of Unix commands, and this works the same way regardless of whether those commands are typed and run interactively in the shell, or are run inside a script.\nAlong those same lines, we have already learned that we can “redirect” output to a file using &gt; (write/overwrite) and &gt;&gt; (append) when we run shell commands — and this, too, works exactly the same way inside a script.\n\nWhen working with genomics data, we commonly have files as input, and new/modified files as output. Let’s practice with this and modify our headtail.sh script so that it writes output to a file.\nWe’ll make the following changes:\n\nWe will have the script accept a second argument: the output file name2.\nWe will redirect the output of our head, echo, and tail commands to the output file. We’ll have to append (using &gt;&gt;) in the last two commands.\n\n#!/bin/bash\nset -euo pipefail\n\ninput_file=$1\noutput_file=$2\n\n# Print the first and last 2 lines of a file, separated by a line with \"---\"\nhead -n 2 \"$input_file\" &gt; \"$output_file\"\necho \"---\" &gt;&gt; \"$output_file\"\ntail -n 2 \"$input_file\" &gt;&gt; \"$output_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\nNow we run the script again, this time also passing the name of an output file:\nbash sandbox/headtail.sh metadata/meta.tsv sandbox/samples_headtail.txt\nThe script will no longer print any output to screen, and our output should instead be in sandbox/samples_headtail.txt:\n# Check that the file exists and was just modified:\nls -lh sandbox/samples_headtail.txt\n-rw-r--r-- 1 jelmer PAS0471 112 Aug 16 16:45 sandbox/samples_headtail.txt\n# Print the contents of the file to screen\ncat sandbox/samples_headtail.txt\nsample_id       cell_line       variant\nASPC1_A178V     ASPC1   A178V\n---\nMiapaca2_G31V   Miapaca2        G31V\nMiapaca2_G31V   Miapaca2        G31V\n\n\n\n3.3 Report what’s happening\nIt is often useful to have your scripts “report” or “log” what is going on. Let’s keep thinking about a script that has one or more files as its main output (again, like most bioinformatics programs do). But instead of having no output printed to screen at all, we’ll print some logging output to screen. For instance:\n\nAt what date and time did we run this script\nWhich arguments were passed to the script\nWhat are the output files\nPerhaps even summaries of the output.\n\nAll of this can help with troubleshooting and record-keeping3. Let’s try this with our headtail.sh script.\n#!/bin/bash\nset -euo pipefail\n\n# Copy placeholder variables\ninput_file=$1\noutput_file=$2\n\n# Initial logging \necho \"# Starting script headtail.sh\" # Print name of script\ndate                                 # Print date & time\necho \"# Input file:   $input_file\"\necho \"# Output file:  $output_file\" \necho                                 # Empty line to separate initial & final logging\n\n# Print the first and last 2 lines of a file, separated by a line with \"---\"\nhead -n 2 \"$input_file\" &gt; \"$output_file\"\necho \"---\" &gt;&gt; \"$output_file\"\ntail -n 2 \"$input_file\" &gt;&gt; \"$output_file\"\n\n# Final logging\necho \"# Listing the output file:\"\nls -lh \"$output_file\"\necho \"# Done with script headtail.sh\"\ndate\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\nA couple of notes about the lines that were added to the script above:\n\nRunning date at the end of the script (as well as at the beginning) allows you to check for how long the script ran\nPrinting the input and output files (and the command-line arguments more generally) can be particularly useful for troubleshooting\nWe printed a “marker line” like Done with script, indicating that the end of the script was reached. This is handy due to our set settings: seeing this line printed means that no errors were encountered.\nI also added some comment lines like “Initial logging” to make the script easier to read, and such comments can be made more extensive to really explain what is being done.\n\nLet’s run the script again:\nbash sandbox/headtail.sh metadata/meta.tsv sandbox/tmp.txt\n# Starting script sandbox/headtail.sh\nWed Aug 16 21:12:28 EDT 2023\n# Input file:   metadata/meta.tsv\n# Output file:  sandbox/tmp.txt\n\n# Listing the output file:\n-rw-r--r-- 1 jelmer PAS0471 112 Aug 16 21:12 sandbox/tmp.txt\n# Done with script sandbox/headtail.sh\nWed Aug 16 21:12:28 EDT 2023\nThe script printed some details for the output file, but not its contents. Let’s take a look at the output file, though, to make sure the script worked:\ncat sandbox/tmp.txt\nsample_id       cell_line       variant\nASPC1_A178V     ASPC1   A178V\n---\nMiapaca2_G31V   Miapaca2        G31V\nMiapaca2_G31V   Miapaca2        G31V\n\n\n\n\n\n\necho, echo?\n\n\n\nThe extensive logging output (echo statements) may seem silly for our little headtail.sh script, and it is at some level: a tiny utility script like this would ideally work much like a regular Unix shell command, and just print the main output and no logging output.\nHowever, this kind of fairly extensive logging is in fact useful when running scripts that execute long-running bioinformatics programs, and can eventually be a time-saver because it makes it easier to spot problems and helps with record-keeping. This is especially true for long-running scripts, or scripts that you often reuse and perhaps share with others.\n\n\n\n\nOn Your Own: A fanciful script\nModify your printline.sh script to:\n\nRedirect the main output (the printed line) to a file\nThe name of this output file should not be “hardcoded” in the script, but should be passed as an argument to the script, like we did above with headtail.sh\nAdd a bit of logging — echo statements, date, etc, similar to what we did above with headtail.sh\nAdd some comments to describe what the code in the script is doing\n\n\n\n\n\n\n\nThe original printline.sh script (Click to expand)\n\n\n\n\n\ncat sandbox/printline.sh\n#!/bin/bash\nset -euo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\n\n\n\n\n\n\n\n\n(One possible) solution (Click to expand)\n\n\n\n\n\n\n#!/bin/bash\nset -euo pipefail\n\n# Copy placeholder variables\ninput_file=$1\noutput_file=$2\nline_nr=$3\n\n# Initial logging \necho \"# Starting script printline.sh\"\ndate\necho \"# Input file:   $input_file\"\necho \"# Output file:  $output_file\"\necho \"# Line number:  $line_nr\"\necho\n\n# Print 1 specific line from the input file and redirect to an output file\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1 &gt; $output_file\n\n# Final logging\necho \"# Listing the output file:\"\nls -lh \"$output_file\"\necho \"# Done with script printline.sh\"\ndate\n\nTo run the script with the additional argument:\nbash sandbox/printline.sh metadata/meta.tsv sandbox/meta_line.tsv 4\n# Starting script printline.sh\nWed Aug 16 21:27:48 EDT 2023\n# Input file:   metadata/meta.tsv\n# Output file:  sandbox/meta_line.tsv\n# Line number:  4\n\n# Listing the output file:\n-rw-r--r-- 1 jelmer PAS0471 22 Aug 16 21:27 sandbox/meta_line.tsv\n# Done with script printline.sh\nWed Aug 16 21:27:48 EDT 2023"
  },
  {
    "objectID": "modules/A08_scripts.html#footnotes",
    "href": "modules/A08_scripts.html#footnotes",
    "title": "Shell Scripting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith an appropriate error message↩︎\nOf course, we could also simply write the output to a predefined (“hardcoded”) file name such as out.txt, but in general, it’s better practice to keep this flexible via an argument.↩︎\nWe’ll see in the upcoming Slurm module that we when submit scripts to the OSC queue (rather than running them directly), the output of scripts that is normally printed to screen, will instead go to a sort of “log” file. So, your script’s reporting will end up in this file.↩︎"
  },
  {
    "objectID": "modules/A03_shell1.html#overview-setting-up",
    "href": "modules/A03_shell1.html#overview-setting-up",
    "title": "The Unix Shell - Part I",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nMany of the things you typically do by pointing and clicking can alternatively be done by typing commands. The Unix shell allows you to interact with computers via commands. It is natively available through a Terminal app in computers with Unix-like operating systems like Linux (on which OSC runs) or MacOS, and can also be installed on Windows computers with relatively little trouble these days (see the SSH reference page on this website).\nWorking effectively on a remote supercomputer tends to simply require using a command line interface. But there are more advantages to doing command line computing than just allowing you to work on a supercomputer, such as:\n\nWorking efficiently with large files\nAchieving better reproducibility in research\nPerforming general computing tasks more efficiently, once you get the hang of it\nMaking it much easier to repeat similar tasks across files, samples, and projects, with the possibility of true automation\nFor bioinformatics, being able to use (the latest) command-line programs directly without having to depend on “GUI wrappers” written by third parties, that often cost money and also lag behind in functionality\n\n\nStarting a VS Code session in OSC OnDemand\nIn these sessions, we’ll use a Unix shell at OSC inside VS Code1. For this session, specifically, I will assume you still have an active VS Code session as setup in the previous one, have VS Code located at /fs/ess/PAS0471, and with an open Terminal — if not, see the instructions in the dropdown box right below.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nType pwd to check where you are. If you are not in /fs/ess/PAS0471, click Open folder... in the Welcome tab, or      =&gt;   File   =&gt;   Open Folder, then type/select /fs/ess/PAS0471 and press Ok.\n\n\n\n\n\n\n\n\n\n\nSome Unix shell terminology\n\n\n\nWe’re going to focus on the practice of doing command line computing here, and not get too bogged down in terminology, but let’s highlight a few interrelated terms you’re likely to run across:\n\nCommand Line — the most general term, an interface where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\nBash Shell — a Unix shell that uses the Bash language\n\nWhile it might not fly for a computer science class, for day-to-day computing/bioinformatics, you’ll probably hear all these terms used somewhat interchangably. Basically, we’re talking about the process of interacting with your computer by giving it commands as opposed to the point-and-click way you’re likely more familiar with."
  },
  {
    "objectID": "modules/A03_shell1.html#first-steps",
    "href": "modules/A03_shell1.html#first-steps",
    "title": "The Unix Shell - Part I",
    "section": "1 First steps",
    "text": "1 First steps\n\n1.1 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies a bit across shells and can also be customized, but our prompts at OSC should show the following:\n[&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example:\n[jelmer@p0080 PAS0471]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n\n\n\nHow shell code is presented on this website\n\n\n\nThe pale gray boxes like the ones shown above will be used to represent your command prompt, or rather, to show the command line expressions that you will type.\nIn upcoming boxes, the prompt itself ([...]$) will not be shown, but only the command line expressions that you type. This is to save space and also because if we omit the prompt, you will be able to directly copy and paste commands from the website to your shell.\nAlso, in a notation like &lt;username&gt;, the &lt; &gt; are there to indicate this is not an actual, functional example, but a descriptive generalization, and should not be part of the final code. In this case, then, it should be replaced merely by a username (e.g. jelmer), and not by &lt;jelmer&gt;, as you can see in the example with the prompt above.\n\n\n\n\n\n1.2 A few simple commands: date, whoami, pwd\nThe Unix shell comes with hundreds of commands. Let’s start with a few simple ones.\nThe date command prints the current date and time:\n\ndate\n\nMon Aug 21 11:59:41 EDT 2023\n\n\n\n\n\n\n\n\nCopying code from the website, and code output\n\n\n\nWhen you hover your mouse above the top box with the command (sometime you have to click in it first), you should see a copy icon appear on the far right, which will copy the command to your clipboard: for longer expressions, this can be handy so you can paste this right into your shell and don’t have to type. Generally speaking, though, learning works better when you type the commands yourself!\nAlso, the darker gray box below, with italic text, is intended to show the output of commands as they are printed to the screen in the shell.\n\n\nThe whoami (who-am-i) command prints your username:\n\nwhoami\n\njelmer\nThe pwd (Print Working Directory) commands prints the path to the directory you are currently located in:\n\npwd\n\n/fs/ess/PAS0471\nAll 3 commands that we just used provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n\n\n\nWorking directories, and paths part I\n\n\n\nOn Unix systems, all the files on a computer exist within a single hierarchical system of directories (folders). When working in the Unix shell, you are always “in” one of these directories. The directory you’re “in” at any given time is referred to as your working directory.\nIn a path (specification of a file or directory location) such as that output by pwd, directories are separated by forward slashes /.\nA leading forward slash in a path indicates the root directory of the computer, and as such, the path provided by pwd is an absolute path (or: full path), and not a relative path — more on that later.\nWhile not shown in the cd output, if you happen to see a trailing forward slash in a path (eg. /fs/ess/PAS0471/), you can be sure that the path points to a directory and not a file."
  },
  {
    "objectID": "modules/A03_shell1.html#cd-and-command-actions-defaults-and-arguments",
    "href": "modules/A03_shell1.html#cd-and-command-actions-defaults-and-arguments",
    "title": "The Unix Shell - Part I",
    "section": "2 cd and command actions, defaults, and arguments",
    "text": "2 cd and command actions, defaults, and arguments\nIn the above three command line expressions:\n\nWe merely typed the name of a command and nothing else\nThe main function of the command was to provide some information, which was the output printed to screen\n\nBut many commands perform an action other than printing output. For example, the very commonly used command cd (Change Directory) will, you guessed it, change your working directory. And as it happens, it normally has no output at all.\nWe start by simply typing cd:\n[jelmer@p0080 PAS0471]$ cd\n[jelmer@p0080 ~]$\nDid anything happen? You might expect a command like cd to report what it did, but it does not. As a general rule for Unix commands that perform actions, and one that also applies to cd: if the command does not print any output, this means it was successful.\nSo where did we change our working directory to, given that we did not tell cd where to move? Our prompt (as shown in the code box below) actually did give us a clue: PAS0471 was changed to ~ But what does ~ mean?\n\nYour Turn: Check what directory we moved to\n\n\nSolution (click here)\n\npwd\n/users/PAS0471/jelmer\nIt appears that we moved to our Home directory! (Remember, we were in the Project directory /fs/ess/PAS0471.)\nAnd as it turns out, ~ is a shell shortcut to indicate your Home directory — more on that later.\n\n\nFrom this, we can infer that the default behavior of cd, i.e. when it is not given any additional information, is to move to a user’s home directory. This is actually a nice trick to remember!\nNow, let’s move to another directory, one that contains some files we can explore to learn our next few commands. We can do so by specifying the path to that directory after the cd command (make sure to leave a space after cd!):\ncd /fs/ess/PAS0471/demo/202307_rnaseq/\npwd\n/fs/ess/PAS0471/demo/202307_rnaseq\nIn more abstract terms, what we did above was to provide cd with an argument (namely, the path to the dir to move to). Arguments generally tell commands what file or directory to operate on, and come at the end of a command line expression. There should always be a space between the command and its argument(s)!\n\n\n\n\n\n\nTab completion!! (Click to expand)\n\n\n\n\n\n\nAfter typing /fs/e, press the Tab key!\n/fs/ess/\nAfter typing /fs/ess/P, press the Tab key. Nothing will happen, so now press it quickly twice in succession.\nDisplay all 709 possibilities? (y or n)\nType n. Why does this happen?\nAfter typing /fs/ess/PAS04, press the Tab key twice quickly in succession (“Tab-tab”).\nPAS0400/ PAS0409/ PAS0418/ PAS0439/ PAS0453/ PAS0456/ PAS0457/ PAS0460/ PAS0471/ PAS0472/ PAS0498/ \nAfter typing /fs/ess/PAS0471/demo/2, press the Tab key!\n/fs/ess/PAS0471/demo/202307_rnaseq/\n\nThe tab completion feature will check for files/dirs present in the location you’re at, and based on the characters you’ve typed so for, complete the path as far as it can.\nSometimes it can’t move forward at all because there are multiple files or dirs that have the same next character. Pressing “Tab-tab” will then show your options, though in unusual circumstances like one above, there are so many that it asks for confirmation. In such cases, it’s usually better to just keep typing assuming that you know where you want to go.\nIn general, though, Tab completion is an incredibly useful feature that you should try to get accustomed to using right away!\n\n\n\nAs we’ve seen, then, cd gives no output when it succesfully changed the working directory (“silence is golden”!). But let’s also see what happens when it does not succeed — it gives the following error:\ncd /fs/Ess/PAS0471\nbash: cd: /fs/Ess/PAS0471: No such file or directory\n\nYour Turn: What was the problem with the path we specified?\n\n\nSolutions (click here)\n\nWe used a capital E in /Ess/ — this should have been /ess/.\nIn other words, paths (dir and file specifications) are case-sensitive on Unix systems!\n\n\nIn summary, in this section we’ve learned that:\n\nThe cd command can be used to change your working directory\nUnix commands like cd that perform actions will by default only print output to screen when something goes wrong (i.e., errors)\nCommands can have default behaviours when they are not given specific directions\nWe can give commands like cd arguments to tell them what to do / operate on.\n\nNext, we’ll learn about options to commands in the context of the ls command."
  },
  {
    "objectID": "modules/A03_shell1.html#ls-and-command-options",
    "href": "modules/A03_shell1.html#ls-and-command-options",
    "title": "The Unix Shell - Part I",
    "section": "3 ls and command options",
    "text": "3 ls and command options\n\n3.1 The default behavior of ls\nThe ls command, short for “list”, is a quite flexible command to list files and directories:\nls\ndata  metadata  README.md\n(You should still be in /fs/ess/PAS0471/demo/202307_rnaseq. If not, cd there first.)\n\n\n\n\n\n\nls output colors\n\n\n\nUnfortunately, the ls output shown above does not show the different colors you should see in your shell — here are some of the most common ones:\n\nEntries in blue are directories (like data and metadata above)\nEntries in black are regular files (like README.md above)\nEntries in red are compressed files (we’ll see an example soon).\n\n\n\nThe default behavior of ls includes that it will:\n\nList files and dirs inside our current working directory, and do so non-recursively: it won’t list files inside those directories, and so on.\nShow as many files and dirs as it can on one line, each separated by a few spaces\nSort files and dirs alphabetically (and not separately so)\nNot show any other details about the files, such as their size, owner, and so on.\n\nAll of this, and more, can be changed by providing ls with options and/or arguments.\n\n\n3.2 First, more on arguments\nLet’s start with an argument, since we’re familiar with those in the context of cd. Any argument to ls should be a path to operate on. For example, if we wanted to see what’s inside that mysterious data dir, we could type:\nls data\nfastq\nWell, that’s not much information, just another dir — so let’s look inside that:\nls data/fastq  # These will be shown in red in your output, since they are compressed\nASPC1_A178V_R1.fastq.gz  ASPC1_G31V_R2.fastq.gz      Miapaca2_G31V_R1.fastq.gz\nASPC1_A178V_R2.fastq.gz  Miapaca2_A178V_R1.fastq.gz  Miapaca2_G31V_R2.fastq.gz\nASPC1_G31V_R1.fastq.gz   Miapaca2_A178V_R2.fastq.gz\nAh, there are some gzipped FASTQ files! These contain our sequence data, and we’ll go and explore them in a bit.\nWe can also provide ls with multiple arguments — and it will nicely tell us which files are in each of the dirs we specified:\nls data metadata\ndata:\nfastq\n\nmetadata:\nmeta.tsv\nMany Unix commands will accept multiple arguments (files or dirs to operate on), which can be very useful.\n\n\n3.3 Options\nFinally, we’ll turn to options. Whereas, in general, arguments tell a command what to operate on, options (also called “flags”) will modify its behavior.\nFor example, we can call ls with the option -l (a dash followed by a lowercase L):\nls -l \ntotal 17\ndrwxr-xr-x 3 jelmer PAS0471 4096 Jul 27 11:53 data\ndrwxr-xr-x 2 jelmer PAS0471 4096 Jul 27 11:54 metadata\n-rw-r--r-- 1 jelmer PAS0471  963 Jul 27 16:48 README.md\nNotice that it lists the same three items as our first ls call above, but now, they’re printed in a different format: one item per line, with lots of additional information included. For example, the date and time is that when the file was last modified, and the numbers just to the left of that (e.g., 4096) show the file sizes in bytes2.\nLet’s add another option, -h — before reading on, can you pick out what it did to modify the output?\nls -l -h\ntotal 17K\ndrwxr-xr-x 3 jelmer PAS0471 4.0K Jul 27 11:53 data\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Jul 27 11:54 metadata\n-rw-r--r-- 1 jelmer PAS0471  964 Jul 27 17:48 README.md\nNote the difference in the format of the column reporting the sizes of the items listed — we now have “human-readable filesizes”, where sizes on the scale of kilobytes will be shown in Ks, of megabytes in Ms, and of gigabytes in Gs.\nMany options have a “long option” counterpart, i.e. a more verbose way of specifying the option. For example, -h can also be specified as --human-readable:\nls -l --human-readable      # Output not shown, same as above\n(And then there are also options that are only available in long format — even with case-sensitivity, one runs out of single-letter abbreviations at some point!)\nDespite that short options like the -l and -s we’ve seen (single-dash, single-letter) are very terse and may at times seem impossible to remember, they are still often preferred with common Unix commands, because they are shorter to type — and keep in mind that you might use, say, ls -lh dozens if not hundreds of time a day if you work in the Unix shell a lot.\nA very useful feature of “short options” is that they can be pasted together as follows:\nls -lh   # Output not shown, same as above\n\n\n\n\n\n\nMore on the long-format output of ls\n\n\n\nThe figure below shows what information is shown in each of the columns (but note that it shows a different listing of files, and uses the new-to-us -a option, short for “all”, to also show “hidden files”):\n\n\n\n\n\n\n\n\n3.4 Combining options and arguments\nFinally, we can combine options and arguments, and let’s do so take a closer look at our dir with FASTQ files — now the -h option is especially useful because it makes it easy to see that the files vary between 4.1 MB and 5.3 MB in size:\nls -lh data/fastq\ntotal 38M\n-rw-r--r-- 1 jelmer PAS0471 4.1M Jul 27 11:53 ASPC1_A178V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.2M Jul 27 11:53 ASPC1_A178V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.1M Jul 27 11:53 ASPC1_G31V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.3M Jul 27 11:53 ASPC1_G31V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.1M Jul 27 11:53 Miapaca2_A178V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.3M Jul 27 11:53 Miapaca2_A178V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.1M Jul 27 11:53 Miapaca2_G31V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.3M Jul 27 11:53 Miapaca2_G31V_R2.fastq.gz\n(Beginners are often inclined to move to a directory when they just want to ls its contents, but its often more convenient to stay put and use an argument to ls instead, like we did above.)\n\n\n\n3.5 Recap of ls, arguments, and options\nIn summary, in this section we have learned that:\n\nThe ls command lists files (by default without additional info and non-recursively)\nUsing arguments, we tell ls (and other commands) what to operate on. Arguments come at the end of the command line epxression, and are not preceded by a dash or any other “pointer”. They are typically names of files or dirs, but can be other things too.\nUsing options, we can make ls (and other commands) show us the results in different ways. They are preceded by at least one dash (-, like -l).\n\n\n\n\n\n\n\nOther types of options\n\n\n\nThe options we’ve seen so far act as “on/off switches”, and this is very common among Unix commands.\nBut some options are not on/off switches and accept values (confusingly, these values can also be called “arguments” to options). For example, the --color option to ls determines how it colorizes its output: there is ls --color=never — versus, among other possibilities, ls --color=always.\nWe’ll see a lot of options that take values when running bioinformatics programs, such as to set specific analysis parameters — for example: trim_galore --quality 30 --length 50 would set a minimum Phred quality score threshold of 30 and a minimum read length threshold of 50 bases for the program TrimGalore, which we will later use to quality-trim and adapter-trim FASTQ files. (This --&lt;option&gt; &lt;value&gt; syntax, i.e. without an = is more common than the --&lt;option&gt;=&lt;value&gt; syntax shown for ls above.)\nIn contrast to when you are using common Unix commands, I would recommend to mostly use long options whenever available when running bioinformatics programs like TrimGalore. That way, it’s easier for you to remember what you did with that option, and more likely to be immediately understood by anyone else reading the code (cf. trim_galore -q 30 -l 50 and trim_galore --quality 30 --length 50).\n\n\n\n\n\n\n\n\nThe tree command and recursive ls (Click to expand)\n\n\n\n\n\nThe tree command lists files recursively (i.e., it will also show us what’s contained in the directories in our working directory), and does so in a tree-like fashion — this can be great to quickly get an intuitive overview of files in a dir:\ntree -C     # The -C option will colorize the output\n\n\n\nAs an aside: if we want to make ls list files recursively, we can use the -R option:\nls -R\n.:\ndata  metadata  README.md\n\n./data:\nfastq\n\n./data/fastq:\nASPC1_A178V_R1.fastq.gz  ASPC1_G31V_R2.fastq.gz      Miapaca2_G31V_R1.fastq.gz\nASPC1_A178V_R2.fastq.gz  Miapaca2_A178V_R1.fastq.gz  Miapaca2_G31V_R2.fastq.gz\nASPC1_G31V_R1.fastq.gz   Miapaca2_A178V_R2.fastq.gz\n\n./metadata:\nmeta.tsv"
  },
  {
    "objectID": "modules/A03_shell1.html#paths",
    "href": "modules/A03_shell1.html#paths",
    "title": "The Unix Shell - Part I",
    "section": "4 Paths",
    "text": "4 Paths\nAs we’ve mentioned, “paths” are specifications of a location on a computer, either of a file or a directory.\nWe’ve talked about the commands cd and ls that operate on paths, and without going into much detail about it so far, we’ve already seen two distinct ways of specifying paths:\n\nAbsolute (full) paths always start from the root directory of the computer, which is represented by a leading /, such as in /fs/scratch/PAS0471/.\n(Absolute paths are like GPS coordinates to specify a geographic location on earth: they will provide location information regardless of where we are ourselves.)\nRelative paths start from your current location (working directory). When we typed ls data earlier, we indicated that we wanted to show the contents of the data directory located inside our current working directory — that probably seemed intuitive. But be aware that the shell would look absolutely nowhere else for that dir than in our current working directory.\n(Relative paths are more like directions to a location that say things like “turn left” — these instructions depend on our current location.)\n\nAbsolute paths may seem preferable because they will work regardless of where you are located, but:\n\nThey can be a lot more typing than we need (or want) to do.\nWhile context-specific, a much more important disadvantage of absolute paths is that they can only be expected to work on one specific computer, and are guaranteed not to work after you move files around.\n\n\nHow might relative dirs work on other computers or after moving files?\n\n\nSolution (click here)\n\nSay that Lucie has a directory for a research project, /fs/ess/PAS0471/lucie/rnaseq1, with lots of dirs and files contained in it. In all her code, she specify paths relative to that top-level project directory.\nThen, she share that entire directory with someone else, copying it off OSC. If her collaborator goes wherever they now have that directory stored, e.g. /home/philip/lucie_collab/rnaseq1, and then start using Lucie’s code with relative paths, they would still work.\nSimilarly, if Lucie moves her dir to /fs/scratch/PAS0805/lucie/rnaseq1, all her code with relative paths would still work as well.\nThis is something we’ll come back to later when talking about reproducibity.\n\n\n\n\n4.1 Moving “up” when using relative paths\nThere are a couple of “shortcuts” available for relative paths. First of all, . (a single period) is another way of representing the current working directory. Therefore, for instance, ls ./data is functionally the same as ls data, and just a more explicit way of saying that the data dir is located in your current working dir (this syntax is occasionally helpful).\nMore usefully for our purposes here, .. (two periods) means one level up in the directory hierarchy, with “up” meaning towards the root directory (I guess the directory tree is best visualized upside down!):\nls ..               # One level up, listing /fs/ess/PAS0471/demo\n202307_rnaseq\nThis pattern can be continued all the way to the root of the computer, so ../.. would list files two levels up:\nls ../..            # Two levels up, listing /fs/ess/PAS0471\naarevalo       conroy      frederico       Nisha     osu8947              ross\nacl            containers  hsiangyin       osu10028  osu9207              Saranga\nAlmond_Genome  danraywill  jelmer          osu10436  osu9207_Lubell_bkup  Shonna\namine1         data        jelmer_osu5685  osu5685   osu9390              SLocke\nap_soumya      demo        jlhartman       osu6702   osu9453              sochina\naudreyduff     dhanashree  linda           osu8107   osu9657\nbahodge11      edwin       Maggie          osu8468   pipeline\ncalconey       ferdinand   mcic-scripts    osu8548   poonam\ncamila         Fiama       Menuka          osu8618   raees\nCecilia        Flye        nghi            osu8900   rawalranjana44\nAlong these lines, there are two other shortcuts worth mentioning:\n\n~ represents your Home directory, so cd ~ would move there and ls ~ would list the files there\n- is a cd-specific shortcut that it is like the “back” button in your browser: it will go to your previous location. (But it only has a memory of 1, so subsequent cd -s would simply move you back and forth between two directories.)\n\n\n\n\n\n\n\nThese shortcuts work with all commands\n\n\n\nExcept for -, all of the above shortcuts are general shell shortcuts that work with any command that takes a path."
  },
  {
    "objectID": "modules/A03_shell1.html#recap",
    "href": "modules/A03_shell1.html#recap",
    "title": "The Unix Shell - Part I",
    "section": "5 Recap",
    "text": "5 Recap\nWe’ve learned about structure of command line expressions in the Unix shell, which include: the command itself, options, arguments, and output (including, in some cases, error messages).\nA few key general points to remember are that:\n\nCommands that take actions like changing directory (and the same will be true for commands that copy and move files, for example) will by default not print any output to the screen, only errors if those occur.3\n\n\nCommands whose main function is to provide information (think ls, date, pwd) will print their output to the screen. We’ll learn later how we can “redirect” output to a file or to another command!\nUsing options (ls -l), we can modify the behavior of a command, and using arguments (ls data), we can modify what it operates on in the first place.\n\n\n\n\n\n\n\nAlways start with a command\n\n\n\nOne additional, important thing to realize about the structure of command line expressions is this:\nEverything you type on the command line should start with the name of a command, or equivalently, a program or script (these are all just “programs”).\nTherefore, for example, just typing the name of a file, even if it exists in your current working directory, will return an error. (I.e., it won’t do anything with that file, such as printing its contents, like you had perhaps expected.) This is because the first word of a command line expressio should be a command, and the name of a file is (usually!) not a command:\nREADME.md\nREADME.md: command not found\n\n\n\n\n\n\n\n\nMany bioinformatics programs are basically specialized commands\n\n\n\nIn many ways, as mentioned in the box above, you can think of using a command-line bioinformatics program as using just another command.\nTherefore, our general skills with Unix commands will very much extend to using command-line bioinformatics tools!\n\n\nWe’ve learned to work with the following truly ubiquitous Unix commands:\n\npwd — print your current working directory\ncd — change your working directory\nls — list files and dirs\n\nAnd we have seen a few other simpler utility commands as well (date, whoami, and tree in a dropdown box).\nWe’ll continue with the basics of the Unix shell in part II."
  },
  {
    "objectID": "modules/A03_shell1.html#at-home-reading-getting-help",
    "href": "modules/A03_shell1.html#at-home-reading-getting-help",
    "title": "The Unix Shell - Part I",
    "section": "At-home reading: getting help",
    "text": "At-home reading: getting help\nWe saw several different options for the ls command, and that may have left you wondering how you are supposed to know about them.\n\n5.1 The --help option\nMany (but not all!) commands have a --help option which will primarily describe the command’s function and “syntax” including many of the available options.\nFor a very brief example, try:\nwhoami --help\nFor a much longer example, try:\nls --help\n\n\n5.2 The man command\nThe man command provides manual pages for Unix commands, which is more complete than the --help help, but sometimes overwhelming as well as terse and not always easy to fully understand — Google is your friend as well!\nFor a short example, try:\nman pwd\n\n\n\n\n\n\nThe man page opens in a “pager” – try to scroll around and type q to quit!\n\n\n\n\n\n\nFor a much longer example, try:\nman ls"
  },
  {
    "objectID": "modules/A03_shell1.html#footnotes",
    "href": "modules/A03_shell1.html#footnotes",
    "title": "The Unix Shell - Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut just for reference, there are other ways of accessing a Unix shell at OSC: for example, you can also get Shell access through the “Clusters” menu in OSC OnDemand, or you could connect your local Unix shell to OSC through SSH (again, see this reference page).↩︎\nThough these sizes are only directly useful for files, not dirs! You can also ignore the total 17 line at the top.↩︎\nWe’ll see later on how we can make commands more “verbose” than they are by default, which can certainly be useful.↩︎"
  },
  {
    "objectID": "modules/x_examples.html",
    "href": "modules/x_examples.html",
    "title": "Batch Jobs in Practice",
    "section": "",
    "text": "So far, we have covered all the building blocks to be able to run command-line programs at OSC:\nWith these skills, it’s relatively straightforward to create and submit scripts to run most command-line programs that can analyze our genomics data.\nOf course, how straightforward this exactly is depends on the ease of use of the programs you need to run, but that will be true in general whenever you learn a new approach and the associated software."
  },
  {
    "objectID": "modules/x_examples.html#setup",
    "href": "modules/x_examples.html#setup",
    "title": "Batch Jobs in Practice",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/&lt;your-folder&gt; in the box Working Directory (replace &lt;your-folder&gt; by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    =&gt; Terminal =&gt; New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =&gt;   File   =&gt;   Open Folder and enter /fs/ess/scratch/PAS2250/&lt;your-folder&gt;."
  },
  {
    "objectID": "modules/x_examples.html#worked-example-part-i-a-script-to-run-fastqc",
    "href": "modules/x_examples.html#worked-example-part-i-a-script-to-run-fastqc",
    "title": "Batch Jobs in Practice",
    "section": "2 Worked example, part I: A script to run FastQC",
    "text": "2 Worked example, part I: A script to run FastQC\n\n2.1 FastQC: A program for quality control of FASTQ files\nFastQC is perhaps the most ubiquitous genomics software. It produces visualizations and assessments of FASTQ files for statistics such as per-base quality (below) and adapter content. Running FastQC should, at least for Illumina data, almost always be the first analysis step after receiving your sequences.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser and which has about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\n\n\n\nA FastQC quality score graph for decent-quality reads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA FastQC quality score graph for poor-quality reads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 FastQC syntax\nTo analyze one optionally gzipped FASTQ file with FastQC, the syntax is simply:\n\nfastqc &lt;fastq-file&gt;\n\nOr if we wanted to specify the output directory (otherwise, output files end up in the current working directory):\n\nfastqc --outdir=&lt;output-dir&gt; &lt;fastq-file&gt;\n\nFor instance, if we wanted output files to go to the directory results/fastqc and wanted the program to analyze the file data/fastq/SRR7609467.fastq.gz, a functional command would like like this:\n\nfastqc --outdir=results/fastqc data/fastq/SRR7609467.fastq.gz\n\n\n\n\n\n\n\nFastQC’s output file names are automatically determined\n\n\n\nWe can specify the output directory, but not the actual file names, which will be automatically determined by FastQC based on the input file name.\nFor one FASTQ file, it will output one HTML file and one ZIP archive. The latter contains files with the summary statistics that were computed and on which the figures are based — we generally don’t need to look at that.\n\n\n\n\n\n2.3 A basic FastQC script\nHere is what a basic script to run FastQC could look like:\n\n#!/bin/bash\n\n## Bash strict settings\nset -euo pipefail\n\n## Copy the placeholder variables\ninput_file=\"$1\"\noutput_dir=\"$2\" \n\n## Run FastQC\nfastqc --outdir=\"$output_dir\" \"$input_file\"\n\nBut we’ll add a few things to to run this script smoothly as a batch job at OSC:\n\nWe load the relevant OSC module:\n\nmodule load fastqc/0.11.8\n\nWe add a few sbatch options:\n\n#SBATCH --account=PAS2250\n#SBATCH --output=slurm-fastqc-%j.out\n\n\nWe’ll also add a few echo statements to report what’s going on, and use a trick we haven’t yet seen — creating the output directory but only if it doesn’t yet exist:\n\nmkdir -p \"$output_dir\"\n\n\n\n\n\n\n\nThe -p option for mkdir\n\n\n\n\n\nUsing the -p option does two things at once for us, both of which are necessary for a foolproof inclusion of this command in a script:\n\nIt will enable mkdir to create multiple levels of directories at once: by default, mkdir errors out if the parent directory/directories of the specified directory don’t yet exist.\n\nmkdir newdir1/newdir2\n#&gt; mkdir: cannot create directory ‘newdir1/newdir2’: No such file or directory\n\n\nmkdir -p newdir1/newdir2    # This successfully creates both directories\n\nIf the directory already exists, it won’t do anything and won’t return an error (which would lead the script to abort at that point with our set settings).\n\nmkdir newdir1/newdir2\n#&gt; mkdir: cannot create directory ‘newdir1/newdir2’: File exists\n\n\nmkdir -p newdir1/newdir2   # This does nothing since the dirs already exist\n\n\n\n\n\nOur script now looks as follows:\n\n\n\n\n\n\nClick here to see the script\n\n\n\n\n\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n#SBATCH --output=slurm-fastqc-%j.out\n  \n## Bash strict settings\nset -euo pipefail\n\n## Load the software\nmodule load fastqc\n\n## Copy the placeholder variables\ninput_file=\"$1\"\noutput_dir=\"$2\" \n\n## Initial reporting\necho \"Starting FastQC script\"\ndate\necho \"Input FASTQ file:   $input_file\"\necho \"Output dir:         $output_dir\"\necho\n\n## Create the output dir if needed\nmkdir -p \"$output_dir\"\n\n## Run FastQC\nfastqc --outdir=\"$output_dir\" \"$input_file\"\n\n## Final reporting\necho\necho \"Listing output files:\"\nls -lh \"$output_dir\"\n\necho\necho \"Done with FastQC script\"\ndate\n\n\n\n\nNotice that this script is very similar to our toy scripts from yesterday and today: mostly standard (“boilerplate”) code with just a single command to run our program of interest.\nTherefore, you can adopt this script as a template for scripts that run other command-line programs, and will generally only need minor modifications!\n\n\n\n\n\n\nKeep your scripts simple – use one program in a script\n\n\n\n\n\nIn general, it is a good idea to keep your scripts simple and run one program per script.\nOnce you get the hang of it, it may seem appealing to string a number of programs together in a single script, so that it’s easier to rerun everything — but that will often end up leading to more difficulties than convenience.\nTo really tie your full set of analyses together in an actual workflow / pipeline, you will want to start using a workflow management system like Snakemake or NextFlow.\n\n\n\n\n\n\n2.4 Submitting our FastQC script as a batch job\nOpen a new file in VS Code (     =&gt;   File   =&gt;   New File) and save it as fastqc.sh within your scripts/ directory. Paste in the code above and save the file.\nThen, we submit the script:\n\nsbatch scripts/fastqc.sh data/fastq/SRR7609467.fastq.gz results/fastqc\n\n\nSubmitted batch job 12521308\n\n\n\n\n\n\n\nOnce again: Where does our output go?\n\n\n\n\n\n\nOutput that would have been printed to screen if we had run the script directly: in the Slurm log file slurm-fastqc-&lt;job-nr&gt;.out\nFastQC’s main output files (HTML ZIP): to the output directory we specified.\n\n\n\n\nLet’s take a look at the queue:\n\nsqueue -u $USER\n# Fri Aug 19 10:38:16 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n#           12521308 serial-40 fastqc.s   jelmer  PENDING       0:00   1:00:00      1 (None)\n\nOnce it’s no longer in the list produced by squeue, it’s done. Then, let’s check the Slurm log file1:\n\ncat slurm-fastqc-12521308.out    # You'll have a different number in the file name\n\n\n\n\n\n\n\nClick to see the contents of the Slurm log file\n\n\n\n\n\n\ncat misc/slurm-fastqc-12521308.out    # You'll have a different number in the file name\n\nStarting FastQC script\nFri Aug 19 10:39:52 EDT 2022\nInput FASTQ file:   data/fastq/SRR7609467.fastq.gz\nOutput dir:         results/fastqc\n\nStarted analysis of SRR7609467.fastq.gz\nApprox 5% complete for SRR7609467.fastq.gz\nApprox 10% complete for SRR7609467.fastq.gz\nApprox 15% complete for SRR7609467.fastq.gz\nApprox 20% complete for SRR7609467.fastq.gz\nApprox 25% complete for SRR7609467.fastq.gz\nApprox 30% complete for SRR7609467.fastq.gz\nApprox 35% complete for SRR7609467.fastq.gz\nApprox 40% complete for SRR7609467.fastq.gz\nApprox 45% complete for SRR7609467.fastq.gz\nApprox 50% complete for SRR7609467.fastq.gz\nApprox 55% complete for SRR7609467.fastq.gz\nApprox 60% complete for SRR7609467.fastq.gz\nApprox 65% complete for SRR7609467.fastq.gz\nApprox 70% complete for SRR7609467.fastq.gz\nApprox 75% complete for SRR7609467.fastq.gz\nApprox 80% complete for SRR7609467.fastq.gz\nApprox 85% complete for SRR7609467.fastq.gz\nApprox 90% complete for SRR7609467.fastq.gz\nApprox 95% complete for SRR7609467.fastq.gz\nAnalysis complete for SRR7609467.fastq.gz\n\nListing output files:\ntotal 16K\n-rw-r--r-- 1 jelmer PAS0471 224K Aug 19 10:39 SRR7609467_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 233K Aug 19 10:39 SRR7609467_fastqc.zip\n\nDone with FastQC script\nFri Aug 19 10:39:58 EDT 2022\n\n\n\n\n\nOur script already listed the output files, but let’s take a look at those too, and do so in the VS Code file browser in the side bar. To actually view FastQC’s HTML output file, we unfortunately need to download it with this older version of VS Code that’s installed at OSC — but the ability to download files from here is a nice one!"
  },
  {
    "objectID": "modules/x_examples.html#worked-example-part-ii-a-loop-in-a-workflow-script",
    "href": "modules/x_examples.html#worked-example-part-ii-a-loop-in-a-workflow-script",
    "title": "Batch Jobs in Practice",
    "section": "3 Worked example, part II: A loop in a workflow script",
    "text": "3 Worked example, part II: A loop in a workflow script\n\n3.1 A “workflow” file\nSo far, we’ve been typing our commands to run or submit scripts directly in the terminal. But it’s better to directly save these sorts of commands.\nTherefore, we will now create a new file for the purpose of documenting the steps that we are taking, and the scripts that we are submitting. You can think of this file as your analysis lab notebook2.\nIt’s easiest to also save this as a shell script (.sh) extension, even though it is not at all like the other scripts we’ve made, which are meant to be run/submitted in their entirety.\n\n\n\n\n\n\nNot like the other scripts\n\n\n\n\n\nOnce we’ve added multiple batch job steps, and the input of say step 2 depends on the output of step 1, we won’t be able to just run the script as is. This is because all the jobs would then be submitted at the same time, and step 2 would likely start running before step 1 is finished.\nThere are some possibilities with sbatch to make batch jobs wait on each other (e.g. the --dependency option), but this gets tricky quickly. As also mentioned above, if you want a fully automatically rerunnable workflow / pipeline, you should consider using a workflow management system like Snakemake or NextFlow.\n\n\n\nSo let’s go ahead and open a new text file, and save it as workflow.sh.\n\n\n\n3.2 Looping over all our files\nThe script that we wrote above will run FastQC for a single FASTQ file. Now, we will write a loop that iterates over all of our FASTQ files (only 8 in this case, but could be 100s just the same), and submits a batch job for each of them.\nLet’s type the following into our workflow.sh script, and then copy-and-paste it into the terminal to run the loop:\n\nfor fastq_file in data/fastq/*fastq.gz; do\n    sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n\n\nSubmitted batch job 2451089\nSubmitted batch job 2451090\nSubmitted batch job 2451091\nSubmitted batch job 2451092\nSubmitted batch job 2451093\nSubmitted batch job 2451094\nSubmitted batch job 2451095\nSubmitted batch job 2451096\n\n\n\nOn Your Own: Check if everything went well\n\nUse squeue to monitor your jobs.\nTake a look at the Slurm log files while the jobs are running and/or after the jobs are finished. A nice trick when you have many log files to check, is to use tail with a wildcard:\n\ntail slurm-fastqc*\n\nTake a look at the FastQC output files: are you seeing 12 HTML files?"
  },
  {
    "objectID": "modules/x_examples.html#adapting-our-scripting-workflow-to-run-other-command-line-programs",
    "href": "modules/x_examples.html#adapting-our-scripting-workflow-to-run-other-command-line-programs",
    "title": "Batch Jobs in Practice",
    "section": "4 Adapting our scripting workflow to run other command-line programs",
    "text": "4 Adapting our scripting workflow to run other command-line programs\n\nOn Your Own: Run another program\nUsing the techniques you’ve learned in this workshop, and especially, using our FastQC script as a template, try to run another command-line genomics program.\nWe below, we provide basically complete command-lines for three programs: MultiQC (summarizing FastQC output into one file), Trimmomatic (quality trimming and removing adapaters), and STAR (mapping files to a reference genome).\nYou can also try another program that you’ve been wanting to use.\n\n\n Commands to load/install and run other software:\n\n\n4.1 MultiQC\nMultiQC is a very useful program that can summarize QC and logging output from many other programs, such as FastQC, trimming software and read mapping software.\nThat means if you have sequenced 50 samples with paired-end reads, you don’t need to wade through 100 FASTQ HTML files to see if each is of decent quality — MultiQC will summarize all that output in nice, interactive figures in a single HTML file!\nHere, we’ll assume you want to run it on the FastQC output, which simply means using your FastQC output directory as the input directory for MultiQC.\n\nInstall\nMultiQC needs to be installed using an unusual 2-3 step procedure (one of the very few programs that can’t be installed in one go with conda):\n\nconda create -n multiqc python=3.7\nsource activate multiqc\nconda install -c bioconda -c conda-forge multiqc\n\n\n\n\n\n\n\nFailed to install? Using other people’s conda environments\n\n\n\n\n\nIf your MultiQC installation fails (this is a tricky one, with very many dependencies!), you can also use mine, by putting these line in your script:\n\nmodule load miniconda3\nsource activate /fs/project/PAS0471/jelmer/conda/multiqc-1.12\n\n\n\n\n\n\nRun\nYou would run MultiQC once for all files (no loop!) and with FastQC output as your input, as follows:\n\n# Copy the placeholder variables\ninput_dir=$1     # Directory where your FastQC output is stored\noutput_dir=$2    # Output dir of your choosing, e.g. result/multiqc\n\n# Activate the conda environment\nmodule load miniconda3\nsource activate multiqc\n\n# Run MultiQC\nmultiqc \"$input_dir\" -o \"$output_dir\"\n\n\n\n\n\n4.2 Trimmomatic\nTrimmomatic is a commonly-used program to both quality-trim FASTQ data and to remove adapters from the sequences.\n\nLoad the OSC module\n\nmodule load trimmomatic/0.38\n\n\n\nRun\nTo run Trimmomatic for one FASTQ file (=&gt; loop needed like with FastQC):\n\n# Load the module\nmodule load trimmomatic/0.38\n\n# Copy the placeholder variables\ninput_fastq=$1    # One of our \"raw\" FASTQ files in data/fastq\noutput_fastq=$2   # Output file directory and name to your choosing\n\n# We provide you with a file that has all common Illumina adapters:\nadapter_file=/fs/ess/scratch/PAS2250/jelmer/mcic-scripts/trim/adapters.fa\n\n# Run Trimmomatic\n# (note that the OSC module makes the environment variable $TRIMMOMATIC available)\njava -jar \"$TRIMMOMATIC\" SE \\\n  \"$input_fastq\" \"$output_fastq\" \\\n  ILLUMINACLIP:\"$adapter_file\":2:30:10 \\\n  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\n\n\n\n\n\n\nAvoid long lines with \\\n\n\n\nThe \\ in the Trimmomatic command above simply allow us to continue a single command one a new line, so we don’t get extremely long lines!\n\n\n\n\n\n\n4.3 STAR\n\n4.3.1 Load the OSC module\n\nmodule load gnu/10.3.0\nmodule load star/2.7.9a\n\n\n\n4.3.2 Index the genome\nFirst, we need to unzip the FASTA reference genome file:\n\ngunzip reference/Pvul.fa.gz\n\n\n#SBATCH --cpus-per-task=8\n\n# Load the module\nmodule load gnu/10.3.0\nmodule load star/2.7.9a\n\n# Copy the placeholder variables\nreference_fasta=$1       # Pvul.fa reference genome FASTA file\nindex_dir=$2             # Output dir with the genome index\n\n# Run STAR to index the reference genome\nSTAR --runMode genomeGenerate \\\n     --genomeDir \"$index_dir\" \\\n     --genomeFastaFiles \"$reference_fasta\" \\\n     --runThreadN \"$SLURM_CPUS_PER_TASK\"\n\n\n\n4.3.3 Map\n\n#SBATCH --cpus-per-task=8\n\n# Load the module\nmodule load star/2.7.9a\n\n# Copy the placeholder variables\nfastq_file=$1         # A FASTQ file to map to the reference genome\nindex_dir=$2          # Dir with the genome index (created in indexing script)\n\n# Extract a sample ID from the filename!\nsample_id=$(basename \"$fastq_file\" .fastq.gz)\n\n# Run STAR to map the FASTQ file\nSTAR \\\n  --runThreadN \"$SLURM_CPUS_PER_TASK\" \\\n  --genomeDir \"$index_dir\" \\\n  --readFilesIn $fastq_file \\\n  --readFilesCommand zcat \\\n  --outFileNamePrefix \"$outdir\"/\"$sample_id\" \\\n  --outSAMtype BAM Unsorted SortedByCoordinate\n\n\n\n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "modules/x_examples.html#footnotes",
    "href": "modules/x_examples.html#footnotes",
    "title": "Batch Jobs in Practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor longer running jobs, you may also want to keep an eye on the log file while it’s running.↩︎\nThough possibly a highly sanitized one – you may want to store daily notes and dead ends in a separate file.↩︎"
  },
  {
    "objectID": "modules/A09_software.html#overview-setting-up",
    "href": "modules/A09_software.html#overview-setting-up",
    "title": "Using Software at OSC",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nTo analyze RNAseq data and other genomics data sets, a typical workflow includes using a sequence of specialized bioinformatics software.\nAt OSC, there are system-wide installations of a number of bioinformatics programs. As we briefly saw earlier for FastQC, and will talk about more here, we do need to “load” such programs before we can use them. However, OSC’s collection of bioinformatics programs is unfortunately not comprehensive, and some of the available programs only come in relatively old versions.\nWe therefore also need another way to make bioinformatics programs available to ourselves. Two common methods are the Conda software management program and containers. We will talk about loading MCIC’s Conda environments, while the at-home reading covers installing software yourself with Conda, and using containers downloaded from the internet.\n\nStart VS Code and open your folder\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\nMake sure to open your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir, either by using the Open Folder menu item, or by clicking on this dir when it appears in the Welcome tab.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nIn the Welcome tab under Recent, you should see your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir listed: click on that to open it. Alternatively, use      =&gt;   File   =&gt;   Open Folder to open that dir in VS Code.\n\n\n\n\n\n\n\n\n\n\nDon’t have your own dir with the data? (Click to expand)\n\n\n\n\n\nIf you missed the last session, or deleted your rnaseq_intro dir entirely, run these commands to get a (fresh) copy of all files you should have so far:\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\nAnd if you do have an rnaseq_intro dir, but you want to start over because you moved or removed some of the files while practicing, then delete the dir before your run the commands above:\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\nYou should have at least the following files in this dir:\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── md5sums.txt\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md\n│   └── ref\n│       ├── GCF_000001405.40.fna\n│       ├── GCF_000001405.40.gtf"
  },
  {
    "objectID": "modules/A09_software.html#loading-software-at-osc-with-lmod-modules",
    "href": "modules/A09_software.html#loading-software-at-osc-with-lmod-modules",
    "title": "Using Software at OSC",
    "section": "1 Loading software at OSC with Lmod modules",
    "text": "1 Loading software at OSC with Lmod modules\nOSC administrators manage software with the “Lmod” system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it. That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.\nWe can load, unload, and search for available software modules using the module command and its various subcommands.\n\n1.1 Checking whether a program is available\nThe OSC website has a list of installed software. You can also search for available software in the shell using two subtly different module subcommands1:\n\nmodule spider lists all modules that are installed.\nmodule avail lists modules that can be directly loaded given the current environment (i.e., taking into account which other software has been loaded).\n\nSimply running module spider or module avail would spit out the full lists of installed/available programs — it is more useful to add a search term as an argument to these commands — below, we’ll search for the Conda distribution “miniconda”, with each of these two subcommands:\nmodule spider miniconda\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  miniconda3:\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        miniconda3/4.10.3-py37\n        miniconda3/4.12.0-py38\n        miniconda3/4.12.0-py39\n        miniconda3/23.3.1-py310\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"miniconda3\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider miniconda3/4.12.0-py39\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nmodule avail miniconda\n------------------------------------------------------------------------------------------------------ /apps/lmodfiles/Core -------------------------------------------------------------------------------------------------------\n   miniconda3/4.10.3-py37 (D)    miniconda3/4.12.0-py38    miniconda3/4.12.0-py39    miniconda3/23.3.1-py310\n\n  Where:\n   D:  Default Module\nAs stated at the bottom of the output below, the (D) in the module avail output above marks the default version of the program: this is the version of the program that will be loaded if we don’t specify a version ourselves (see examples below). The module spider command does not provide this information.\n\n\n\n1.2 Loading software\nAll other Lmod software functionality is also accessed using module subcommands. For instance, to make a program available to us we use the load subcommand:\n# Load a module:\nmodule load miniconda3               # Load the default version\nmodule load miniconda3/23.3.1-py310  # Load a specific version\n\n\n\n\n\n\nModules do not remain loaded across separate shell sessions\n\n\n\nModule loading does not persist across shell sessions. Whenever you get a fresh shell session (including but not limited to after logging into OSC again), you will have to (re)load any modules you want to use!\n\n\nTo check which modules have been loaded, use module list. Its output will also include automatically loaded modules, so for example, if you loaded miniconda3/23.3.1-py310, you should see the following list where the miniconda3 module is listed as the 6th entry:\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest   2) gcc-compatibility/8.4.0   3) intel/19.0.5   4) mvapich2/2.3.3   5) modules/sp2020   6) miniconda3/23.3.1-py310\nOccasionally, when you run into conflicting (mutually incompatible) modules, it can be useful to unload modules, which you can do as follows:\nmodule unload miniconda3    # Unload a specific module\nmodule purge                # Unload all modules\n\n\n\n1.3 A practical example: FastQC again\nHere, we’ll load the module for FastQC again. First, let’s confirm that we indeed cannot currently use FastQC by running the fastqc command with the --help option:\nfastqc --help\nbash: fastqc: command not found\n\n\n\n\n\n\nHelp!\n\n\n\nMany command-line programs can be run with with a --help (and/or -h) flag, and this is often a good thing to try first, since it will tell use whether we can use the program — and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether FastQC is available at OSC, and if so, in which versions:\nmodule avail fastqc\nfastqc/0.11.8\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be a reason to specify the version when we load the FastQC module?\n\n\n\n\n\nWhen we use the module load command inside a script, specifying a version would:\n\nEnsure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nMake it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\nmodule load fastqc/0.11.8\nNow, we can retry our --help attempt:\nfastqc --help\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n        fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN  \n# [...truncated...]\n\nOn your own: load miniconda3\nThe miniconda3 module will allow us to use Conda software environments, which we’ll talk about more below.\n\nLet’s start with a clean sheet by running module purge.\nLoad the default version of miniconda3, and then check which version was loaded.\n\n\n\n\n\n\n\nSolution (Click here)\n\n\n\n\n\nmodule load miniconda3\n\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest   2) gcc-compatibility/8.4.0   3) intel/19.0.5   4) mvapich2/2.3.3   5) modules/sp2020   6) miniconda3/4.10.3-py37\nThe version 4.10.3-py37 was loaded.\n\n\n\n\nNow load the latest version of miniconda3 without unloading the earlier version first. What output do you get?\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\nLmod detected that you tried to load a different version of a software that was already loaded, so it changes the version and tells you about it:\nmodule load miniconda3/23.3.1-py310\nThe following have been reloaded with a version change:\n  1) miniconda3/4.10.3-py37 =&gt; miniconda3/23.3.1-py310"
  },
  {
    "objectID": "modules/A09_software.html#when-software-isnt-installed-at-osc",
    "href": "modules/A09_software.html#when-software-isnt-installed-at-osc",
    "title": "Using Software at OSC",
    "section": "2 When software isn’t installed at OSC",
    "text": "2 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than what is available. In that case, the following two are generally your best options:\n\nConda, which creates software environments that you can activate much like the Lmod modules.\nContainers, which are self-contained software environments that include operating systems, akin to mini virtual machines. While Docker containers are most well-known, OSC uses Apptainer (formerly known as Singularity) containers.\n\n\n\n\n\n\n\nOther options to install software / get it installed\n\n\n\n\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through Lmod / module commands).\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”2 and will often have difficulties with “dependencies”3.\n\n\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nNext, we’ll talk about Conda and using the MCIC’s Conda environments. The at-home reading includes installing software yourself with Conda, and using containers downloaded from the internet."
  },
  {
    "objectID": "modules/A09_software.html#intro-to-conda-using-mcics-conda-environments",
    "href": "modules/A09_software.html#intro-to-conda-using-mcics-conda-environments",
    "title": "Using Software at OSC",
    "section": "3 Intro to Conda & using MCIC’s Conda environments",
    "text": "3 Intro to Conda & using MCIC’s Conda environments\nThe Conda software can create so-called environments in which one can install one or more software packages.\nAs you can see in the at-home reading below, as long as a program is available in one of the online Conda repositories (which is nearly always for bioinformatics programs), then installing it is quite straightforward, doesn’t require admin privileges, and is done with a procedure that is nearly identical regardless of the program you are installing.\nHowever, at OSC, you will probably not even have to install anything yourself, at least not if you are following “standard” workflows with common data like RNAseq. To this end, I maintain an “MCIC collection” of Conda environments that anyone can use.\nA Conda environment is just a directory, and since all the environments in this collection are in the same place at OSC, you can list the MCIC Conda environments as follows:\nls /fs/ess/PAS0471/jelmer/conda\nabricate-1.0.1  bedops-2.4.39  checkm-1.2.0   entrez-direct    htseq-2.0.2          longstitch-1.0.3  nanopolish-0.13.2    prokka            repeatmasker-4.1.2.p1         samtools                star\nagat-0.9.1      bedtools       clinker        evigene          inspector-1.2.0      mafft             ncbi-datasets        pseudofinder      repeatmodeler-2.0.3           scoary                  subread-2.0.1\nalv             bioawk         clonalframeml  fastp            interproscan-5.55    maskrc-svg        nextdenovo-env       purge_dups-1.2.6  resfinder                     seqkit                  tgsgapcloser\namrfinderplus   biopython      codan-1.2      fastqc           iqtree               medaka-1.7.2      nextflow             pycoqc-2.5.2      resistomeanalyzer-2018.09.06  seqtk                   tracy-0.7.1\nantismash       bit            cogclassifier  fastq-dl         justorthologs-0.0.2  metaxa-2.2.3      orna-2.0             qiime2-2022.8     rgi-5.2.1                     signalp-6.0             transabyss-2.0.1\nariba-2.14.6    blast          cutadapt       fasttree-2.1.11  kallisto-0.48.0      minibusco         orthofinder          qualimap-env      r-metabar                     sistr-1.1.1             transdecoder-5.5.0\nastral-5.7.8    bowtie2-2.5.0  deeploc        filtlong-env     kat-2.4.2            minimap2-2.24     orthofisher          quast-5.0.2       rnaquast-2.2.1                smartdenovo-env         treetime\naswcli          bracken-2.6.1  deeptmhmm      flye-2.9.1       knsp-3.1             mlst              panaroo              quickmerge-env    roary-3.13                    snippy-4.6.0            trimgalore\nbactopia        braker2-env    deeptmhmm2     fmlrc2-0.1.7     kofamscan            mlst_check        phylofisher          racon-1.5.0       r-rnaseq                      snp-sites-2.5.1         trimmomatic-0.39\nbactopia-dev    busco          diamond        gcta             kraken2-2.1.2        mobsuite          pilon-1.24           ragtag-2.1.0      rsem-1.3.3                    soapdenovo-trans-1.0.4  trinity-2.13.2\nbakta           bwa-0.7.17     dwgsim         gffread-0.12.7   krakentools-1.2      multiqc           pkgs                 rascaf            rseqc-env                     sortmerna-env           unicycler\nbase            bwa-mem-2.2.1  eggnogmapper   gubbins          krona                mummer4           plasmidfinder-2.1.6  rcorrector-1.0.5  r_tree                        sourmash                virulencefinder\nbbmap           cactus         emboss         hisat2           liftoff-1.6.3        nanolyse-1.2.1    plink2               r-deseq           sabre-1.0                     spades-3.15.5           wtdbg-2.5\nbcftools        cgmlst         entap-0.10.8   hmmer            links-2.0.1          nanoplot          porechop             recognizer-1.8.3  salmon                        sra-tools\nThis is organized similarly to the Lmod modules in that there’s generally one separate environment for one program (and all its dependencies), and the environment is named after that program.\nThe naming of the environments is unfortunately not entirely consistent: many environments include the version number of the program, but many others do not. (Generally speaking, for environments without version numbers, you should expect the version of the program to be very recent, as I try to keep these up-to-date4).\nThis collection includes Conda environments for several programs we need during RNAseq analysis that are not installed at OSC, such as MultiQC, TrimGalore, and SortMeRNA.\n\n\n3.1 Activating Conda environments\nConda itself is already installed at OSC through Miniconda, but we always need to load its module before we can use it:\nmodule load miniconda3\nAs mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system. But whereas we use the term “load” for Lmod modules, we use “activate” for Conda environments — it means the same thing.\nAlso like Lmod, there is a main command (conda) and several subcommands (deactivate, create, install, update) for different functionality. However, for historical reasons, the most foolproof way to activate a Conda environment is to use source activate rather than the expected conda activate — for instance:\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n(multiqc) [jelmer@p0085 rnaseq-intro]$\n\n\n\n\n\n\nConda environment indicator\n\n\n\nWhen we have an active Conda environment, its name is displayed in front of our prompt, as depicted above with (multiqc).\n\n\nAfter we have activated the MultiQC environment, we should be able to actually use the program. To test this, we’ll simply run the multiqc command with the --help option like we did for FastQC:\n\nmultiqc --help\n\n /// MultiQC 🔍 | v1.15                                                                                                                                                                                                            \n                                                                                                                                                                                                                                   \n Usage: multiqc [OPTIONS] [ANALYSIS DIRECTORY]                                                                                                                                                                                     \n                                                                                                                                                                                                                                   \n MultiQC aggregates results from bioinformatics analyses across many samples into a single report.                                                                                                                                 \n It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.                                                       \n To run, supply with one or more directory to scan for analysis results. For example, to run in the current working directory, use 'multiqc .'                                                                                     \n                                                                                                                                                                                                                                   \n╭─ Main options ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ --force            -f  Overwrite any existing reports                                                                                                                                                                           │\n│ --config           -c  Specific config file to load, after those in MultiQC dir / home dir / working dir. (PATH)                                                                                                                │\n│ --cl-config            Specify MultiQC config YAML on the command line (TEXT)                                                                                                                                                   │\n│ --filename         -n  Report filename. Use 'stdout' to print to standard out. (TEXT)                                                                                                                                           │\n│ --outdir           -o  Create report in the specified output directory. (TEXT)                                                                                                                                                  │\n│ --ignore           -x  Ignore analysis files (GLOB EXPRESSION)                                                                                                                                                                  │\n│ --ignore-samples       Ignore sample names (GLOB EXPRESSION)                                                                                                                                                                    │\n│ --ignore-symlinks      Ignore symlinked directories and files                                                                                                                                                                   │\n│ --file-list        -l  Supply a file containing a list of file paths to be searched, one per row                                                                                                                                │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n[...truncated...]\n\nUnlike Lmod / module load, Conda will by default only keep a single environment active. Therefore, when you have one environment activate and then activate another, you will switch environments:\n# After running this command, the multiqc env will be active\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# After running his command, the trimgalore env will be active...\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# ...but the multiqc env will no longer be:\nmultiqc --help\nbash: multiqc: command not found...\nHowever, the conda activate --stack option enables you to have multiple Conda environments active at once:\n# Assuming you had trimgalore activated, now add the multiqc env:\nconda activate --stack /fs/ess/PAS0471/jelmer/conda/multiqc\n\nmultiqc --help\n# (Output not shown, but this should print help info)\n\ntrim_galore --help\n# (Output not shown, but this should print help info)\nNote that the command is conda activate --stack and not source activate --stack!\n\n\n\n3.2 Lines to add to your shell script\nAs mentioned above for Lmod modules, you need to load them in every shell session you want to use them — and the same is true for Conda environments. While Conda enviroments that are loaded in your interactive shell environment will “carry over” to the environment in which your script runs (even when you submit them to the Slurm queue with sbatch; topic of the next session), it is good practice to always include the necessary code to load/activate programs in your shell scripts.\nWhen the program you will run in a script is in an Lmod module, this only involves a module load call — e.g., for FastQC:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load fastqc\nWhen the program you will run in a script is in a Conda environment, this entails a module load command to load Conda itself, followed by a source activate command to load the relevant Conda environment — e.g. for MultiQC:\n#!/bin/bash\n\n# Load software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# Strict/safe Bash settings \nset -euo pipefail\n\n\n\n\n\n\nPerils of Conda environments inside scripts\n\n\n\n\nIn the example above, the set -euo pipefail line was moved below the source activate command, because the Conda activation procedure can otherwise result in “unbound variable” errors.\nAnother unfortunate aspect of Conda environments at OSC is the following. Problems can occur when you have a Conda environment active in your interactive shell while you submit a script as a batch job that activates a different environment.\nTherefore, it is generally a good idea to not have any Conda environments active in your interactive shell when submitting batch jobs5. To deactivate the currently active Conda environment, simply type conda deactivate without any arguments:\nconda deactivate"
  },
  {
    "objectID": "modules/A09_software.html#at-home-reading-creating-your-own-conda-environments",
    "href": "modules/A09_software.html#at-home-reading-creating-your-own-conda-environments",
    "title": "Using Software at OSC",
    "section": "At-home reading: Creating your own Conda environments",
    "text": "At-home reading: Creating your own Conda environments\nWhen you want to create your own Conda environments and install programs, make sure to load the most recent miniconda3 module, which is currently not the default one. This is because installation has become much quicker and less likely to fail than in earlier versions. (Note that when we are just loading environments, like above, the version doesn’t matter).\nAs of August 2023, the most recent miniconda version is 23.3.1-py310 (recall that you can list available versions with module spider):\nmodule load miniconda3/23.3.1-py310\n\nOne-time Conda configuration\nBefore we can create our own environments, we first have to do some one-time configuration6. This will set the Conda “channels” (basically, software repositories) that we want to use when we install programs, including the relative priorities among channels (since one program may be available from multiple channels).\nWe can do this configuration with the config subcommand — run the following commands in your shell:\nconda config --add channels defaults     # Added first =&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last =&gt; highest priority\nLet’s check whether the configuration was successfully saved:\nconda config --get channels\n--add channels 'defaults'   # lowest priority\n--add channels 'bioconda'\n--add channels 'conda-forge'   # highest priority\n\n\n\n3.3 Example: Creating an environment for Trim Galore!\nTo practice using Conda, we will now create a Conda environment with the program Trim Galore! installed. Trim Galore! is a commonly used tool for quality trimming and adapter trimming of FASTQ files — we’ll learn more about it in a later session, since we will use it on our RNAseq data. It does not have a system-wide installation at OSC, unfortunately.\nHere is the command to all at once create a new Conda environment and install Trim Galore! into that environment:\n\n# (Don't run this)\nconda create -y -n trim-galore -c bioconda trim-galore\n\nLet’s break that command down:\n\ncreate is the Conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation once Conda has determined what needs to be installed.\nFollowing the -n option, we can specify the name of the environment, so -n trim-galore means that we want our environment to be called trim-galore. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a “channel” (repository) from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe trim-galore at the end of the line simply tells Conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\nSpecifying a version\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name, and may also want to include that version number in the Conda environment’s name — try running the command below:\nconda create -y -n trim-galore-0.6.10 -c bioconda trim-galore=0.6.10\nCollecting package metadata (current_repodata.json): done  \nSolving environment: done\n# [...truncated...]\n\n\n\n\n\n\nSee the full output when I ran this command (Click to expand)\n\n\n\n\n\n\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 23.3.1\n  latest version: 23.7.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.7.2\n\n\n\n## Package Plan ##\n\n  environment location: /fs/project/PAS0471/jelmer/conda/trimgalore-0.6.10\n\n  added / updated specs:\n    - trim-galore=0.6.10\n\n\nThe following packages will be downloaded:\n\n    | package            | build                                            |\n    | ------------------ | ------------------------------------------------ |\n    | bz2file-0.98       | py_0           9 KB  conda-forge                 |\n    | cutadapt-1.18      | py37h14c3975_1         206 KB  bioconda          |\n    | fastqc-0.12.1      | hdfd78af_0        11.1 MB  bioconda              |\n    | pigz-2.6           | h27826a3_0          87 KB  conda-forge           |\n    | python-3.7.12      | hf930737_100_cpython        57.3 MB  conda-forge |\n    | trim-galore-0.6.10 | hdfd78af_0          45 KB  bioconda              |\n    | xopen-0.7.3        | py_0          11 KB  bioconda                    |\n    ------------------------------------------------------------\n                                           Total:        68.8 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.9-hd590300_0 \n  bz2file            conda-forge/noarch::bz2file-0.98-py_0 \n  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4 \n  ca-certificates    conda-forge/linux-64::ca-certificates-2023.7.22-hbcca054_0 \n  cairo              conda-forge/linux-64::cairo-1.16.0-hbbf8b49_1016 \n  cutadapt           bioconda/linux-64::cutadapt-1.18-py37h14c3975_1 \n  expat              conda-forge/linux-64::expat-2.5.0-hcb278e6_1 \n  fastqc             bioconda/noarch::fastqc-0.12.1-hdfd78af_0 \n  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-hab24e00_0 \n  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n  freetype           conda-forge/linux-64::freetype-2.12.1-hca18f0e_1 \n  gettext            conda-forge/linux-64::gettext-0.21.1-h27087fc_0 \n  giflib             conda-forge/linux-64::giflib-5.2.1-h0b41bf4_3 \n  graphite2          conda-forge/linux-64::graphite2-1.3.13-h58526e2_1001 \n  harfbuzz           conda-forge/linux-64::harfbuzz-7.3.0-hdb3a94d_0 \n  icu                conda-forge/linux-64::icu-72.1-hcb278e6_0 \n  keyutils           conda-forge/linux-64::keyutils-1.6.1-h166bdaf_0 \n  krb5               conda-forge/linux-64::krb5-1.21.2-h659d440_0 \n  lcms2              conda-forge/linux-64::lcms2-2.15-haa2dc70_1 \n  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.40-h41732ed_0 \n  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n  libcups            conda-forge/linux-64::libcups-2.3.3-h4637d8d_4 \n  libdeflate         conda-forge/linux-64::libdeflate-1.18-h0b41bf4_0 \n  libedit            conda-forge/linux-64::libedit-3.1.20191231-he28a2e2_2 \n  libexpat           conda-forge/linux-64::libexpat-2.5.0-hcb278e6_1 \n  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n  libgcc-ng          conda-forge/linux-64::libgcc-ng-13.1.0-he5830b7_0 \n  libglib            conda-forge/linux-64::libglib-2.76.4-hebfc3b9_0 \n  libgomp            conda-forge/linux-64::libgomp-13.1.0-he5830b7_0 \n  libiconv           conda-forge/linux-64::libiconv-1.17-h166bdaf_0 \n  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-2.1.5.1-h0b41bf4_0 \n  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0 \n  libpng             conda-forge/linux-64::libpng-1.6.39-h753d276_0 \n  libsqlite          conda-forge/linux-64::libsqlite-3.42.0-h2797004_0 \n  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.1.0-hfd8a6a1_0 \n  libtiff            conda-forge/linux-64::libtiff-4.5.1-h8b53f26_0 \n  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n  libwebp-base       conda-forge/linux-64::libwebp-base-1.3.1-hd590300_0 \n  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n  libzlib            conda-forge/linux-64::libzlib-1.2.13-hd590300_5 \n  ncurses            conda-forge/linux-64::ncurses-6.4-hcb278e6_0 \n  openjdk            conda-forge/linux-64::openjdk-20.0.0-h8e330f5_0 \n  openssl            conda-forge/linux-64::openssl-3.1.2-hd590300_0 \n  pcre2              conda-forge/linux-64::pcre2-10.40-hc3806b6_0 \n  perl               conda-forge/linux-64::perl-5.32.1-4_hd590300_perl5 \n  pigz               conda-forge/linux-64::pigz-2.6-h27826a3_0 \n  pip                conda-forge/noarch::pip-23.2.1-pyhd8ed1ab_0 \n  pixman             conda-forge/linux-64::pixman-0.40.0-h36c2ea0_0 \n  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n  python             conda-forge/linux-64::python-3.7.12-hf930737_100_cpython \n  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n  setuptools         conda-forge/noarch::setuptools-68.0.0-pyhd8ed1ab_0 \n  sqlite             conda-forge/linux-64::sqlite-3.42.0-h2c6b66d_0 \n  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 \n  trim-galore        bioconda/noarch::trim-galore-0.6.10-hdfd78af_0 \n  wheel              conda-forge/noarch::wheel-0.41.1-pyhd8ed1ab_0 \n  xopen              bioconda/noarch::xopen-0.7.3-py_0 \n  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-h7f98852_1002 \n  xorg-inputproto    conda-forge/linux-64::xorg-inputproto-2.3.2-h7f98852_1002 \n  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.6-h8ee46fc_0 \n  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n  xorg-libxi         conda-forge/linux-64::xorg-libxi-1.7.10-h7f98852_0 \n  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n  xorg-libxt         conda-forge/linux-64::xorg-libxt-1.3.0-hd590300_1 \n  xorg-libxtst       conda-forge/linux-64::xorg-libxtst-1.2.3-h7f98852_1002 \n  xorg-recordproto   conda-forge/linux-64::xorg-recordproto-1.14.2-h7f98852_1002 \n  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 \n  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5 \n  zstd               conda-forge/linux-64::zstd-1.5.2-hfc55251_7 \n\n\n\nDownloading and Extracting Packages\n                                                                                                                                                                                                                                   \nPreparing transaction: done                                                                                                                                                                                                        \nVerifying transaction: done                                                                                                                                                                                                        \nExecuting transaction: done                                                                                                                                                                                                        \n#                                                                                                                                                                                                                                  \n# To activate this environment, use                                                                                                                                                                                                \n#                                                                                                                                                                                                                                  \n#     $ conda activate trimgalore-0.6.10\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\n\n\n\nNow, you should be able to activate the enviroment (using just it’s name – see the box below):\n# Activate the environment:\nsource activate trim-galore\n\n# Test if TrimGalore can be run - note, the command is 'trim_galore': \ntrim_galore --help\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nSpecifying the full path to the environment dir\n\n\n\nYou may have noticed above that we merely gave the enviroment a name (trim-galore or trim-galore-0.6.10), and did not tell it where to put this environment. Similarly, we were able to activate the environment with just its name. Conda assigns a personal default directory for its environments, somewhere in your Home directory.\nYou can install environments in a different location with the -p (instead of -n) option, for example:\nmkdir -p /fs/scratch/PAS0471/$USER/conda\nconda create -y -p /fs/scratch/PAS0471/$USER/conda/trim-galore -c bioconda trim-galore\nAnd when you want to load someone else’s Conda environments, you’ll always have to specify the full path to environment’s dir, like you did when loading an MCIC Conda environment above.\n\n\n\n\n\n\n3.4 Finding the Conda installation info online\nMinor variations on the conda create command above can be used to install almost any program for which a Conda package is available, which is the vast majority of open-source bioinformatics programs!\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its Conda package’s name is\nWhich Conda channel we should use\nWhich versions are available\n\nMy strategy to finding this out is to simply Google the program name together with “conda”, e.g. “cutadapt conda” if I wanted to install the CutAdapt program. Let’s see that in action:\n\n\n\nClick on that first link (it should always be the first Google hit):\n\n\n\n\nBuild the installation command\nI always take the top of the two example installation commands as a template, which is here: conda install -c bioconda cutadapt.\nYou may notice the install subcommand, which we haven’t yet seen. This would install Cutadapt into the currently activated Conda environment. Since our strategy here –and my general strategy– is to create a new environment each time you’re installing a program, just installing a program into whatever environment is currently active is not a great idea. To use the install command with a new environment, the strategy would be to first create an “empty” environment, and then run the install command.\nHowever, we saw above that we can do all of this in a single command. To build this create-plus-install command, all we need to do is replace install in the example command on the Conda website by create -y -n &lt;env-name&gt;. Then, our full command (without version specification) will be:\nconda create -y -n cutadapt -c bioconda cutadapt\nTo see which version will be installed by default, and to see which older versions are available:\n\n\n\nFor almost any other program, you can use the exact same procedure to find the Conda package and install it!\n\n\n\n\n\n\nA few more Conda commands to manage your environments\n\n\n\n\nExport a plain-text “YAML” file that contains the instructions to recreate your currently-active environment (useful for reproducibility!)\nconda env export &gt; my_env.yml\nAnd you can use the following to create a Conda environment from such a YAML file:\nconda env create -n my_env --force --file my_env.yml\nRemove an environment entirely:\nconda env remove -n cutadapt\nList all your conda environments:\nconda env list\nList all packages (programs) installed in an environment — due to dependencies, this can be a long list, even if you only actively installed one program:\nconda list -p /fs/ess/PAS0471/jelmer/conda/multiqc\n\n\n\n\n\n\n\n\n\nUse one environment per program (as here) or one per research project\n\n\n\nBelow are two reasonable ways to organize your Conda environments, and their respective advantages:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you’re working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nEven though it might seem easier, a third alternative, to simply install all programs across all projects in one single environment, is not recommended. This doesn’t benefit reproducibility, and your environment is likely to stop functioning properly sooner or later.\n(A side note: even when you want to install a single program, multiple programs are in fact nearly always installed: the programs that your target program depends on, i.e. “dependencies”.)"
  },
  {
    "objectID": "modules/A09_software.html#at-home-reading-using-apptainer-containers",
    "href": "modules/A09_software.html#at-home-reading-using-apptainer-containers",
    "title": "Using Software at OSC",
    "section": "At-home reading: Using Apptainer containers",
    "text": "At-home reading: Using Apptainer containers\nBesides Conda, containers are another way to use bioinformatics programs at OSC that don’t have system-wide installations.\nContainers are similar to Virtual Machines and different from Conda environments in that they come with an entire operating system. This makes creating your own container “image” (see box below on terminology) much more involved than creating a Conda environment, and we will not cover that here.\nHowever, there are pre-existing container images available for most bioinformatics programs, and they can be easily found, downloaded, and used.\n\n\n\n\n\n\nContainer terminology\n\n\n\n\nContainer image: File (Apptainer) or files (Docker) that contain the container application.\nContainer (sensu stricto): A running container image.\nDefinition file (Apptainer) / Dockerfile (Docker): A plain text file that contains the recipe to build a container image.\n\n\n\nAmong container platforms, Apptainer (formerly known as Singularity) and especially Docker are the most widely used ones. At supercomputers like OSC, however, only Apptainer containers can be used. Luckily, the Apptainer program can work with Docker container images: it will convert them on the fly.\n\nFinding container images online\nThere are several online repositories with publicly available container images, but I would recommend BioContainers https://biocontainers.pro/registry or Quay.io https://quay.io/biocontainers.\nFor example, let’s look on the BioContainers website for a TrimGalore container image:\n\n\n\n\nThe search result on the BioContainers website after entering “trim galore” in the search box.\n\n\n\nClick on the only entry that is shown, trim-galore, which will get you to a page like this:\n\n\n\n\n\nAs you can see, this website also includes Conda installation instructions — to see the container results, scroll down and you should see this:\n\n\n\n\nAfter scrolling down on the results page, you should see a recent available container image. Note that the command shown is singularity run, but we will use the more up-to-date apptainer run later.\n\n\n\nThe version tag that is shown (0.6.9--hdfd78af_0 above) pertains to the version of TrimGalore, but the result that is shown here is not will always the container image(s) with the most recent version. To see a list of all available images, click on the Packages and Containers tab towards the top, and then sort by Last Update:\n\n\n\n\nThe logo with the large S depicts Singularity/Apptainer containers.\n\n\n\nWhenever both a Singularity/Apptainer and a Docker image for the desired version of the program is available, use the Singularity/Apptainer image. This is because those don’t have to be converted, while Docker images do. But when the version you want is only available as a Docker image, that will work too: as mentioned above, it will be automatically converted to the proper format.\n\n\n\nRunning a container image\nWhen you’ve found a container image that you want to use, copy its URL from the BioContainers website. For example, for the most recent TrimGalore version as of September 2023: https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0.\nYou could also copy the full command — however, we will modify that in two ways:\n\nWe will use the more up-to-date apptainer command7\nWe’ll use the exec subcommand instead of run, which allows us to enter a custom command to run in the container (the run subcommand would only run some preset default action, which is rarely useful for our purposes).\n\nAs such, our base command to run TrimGalore in the container will be as follows:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0\n# (Don't run this, we'll need to add a TrimGalore command)\nAfter the code above, we would finish our command by simply entering a TrimGalore command in the exact same way as we would when running TrimGalore outside of the context of a container. For example, to just print the help info like we’ve been doing before, the TrimGalore command is:\ntrim_galore --help\nAnd to run that inside the container, our full command will be:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0 \\\n    trim_galore --help\nINFO:    Downloading network image\n321.4MiB / 321.4MiB [===================================================================================================================================] 100 % 3.0 MiB/s 0s\nWARNING: Environment variable LD_PRELOAD already has value [], will not forward new value [/apps/xalt/xalt/lib64/libxalt_init.so] from parent process environment\n\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nNote\n\n\n\n\nThe Apptainer/Singularity software does not need to be loaded at OSC, it is always automatically loaded.\nThe \\ in the code above allows us to continue a command on another line.\n\n\n\nSo, all that is different from running a program inside a container versus a locally installed program, is that you prefix apptainer exec &lt;URL&gt; when using a container.\nThe first time you run this command, the container will be downloaded, which can take a few minutes (by default it will be downloaded to ~/.apptainer/cache, but you can change this by setting the $APPTAINER_CACHEDIR environment variable). After that, the downloaded image will be used and the command should be executed about as instantaneously as when running TrimGalore outside of a container.\nYou will keep seeing the warning WARNING: Environment variable LD_PRELOAD [...] whenever you run a container, but this is nothing to worry about.\nFinally, the --help option above can also simply be replaced by a host of other TrimGalore options and arguments so as to actually trim a pair of FASTQ files, i.e. with input and output files. You can just specify the paths to those files in the same way as without a container, this will work out of the box!\n\n\n\n\n\n\nWhen to use a Container versus Conda\n\n\n\n\nCurrently, my default is to first try installation with Conda. But I will try a container when installing a program through Conda fails, or my Conda environment misbehaves (e.g., memory errors with dumped cores).\nWhen you need multiple programs in quick succession or in a single command (e.g., you’re piping the output of one program into a second program), it can be more convenient to have those programs installed in a single environmnent or container. Pre-built multi-program containers are not as easy to find. And since building your own Conda environment is easier than building your own container, this is a situation where you might prefer Conda."
  },
  {
    "objectID": "modules/A09_software.html#footnotes",
    "href": "modules/A09_software.html#footnotes",
    "title": "Using Software at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, we call module the command and e.g. spider the subcommand. But sometimes the subcommands are also simply called commands.↩︎\nWhen your personal computer asks you to “authenticate” while you are installing something, you are authenticating yourself as a user with administrator privileges. At OSC, you don’t have such privileges.↩︎\nOther software upon which the software that you are trying to install depends.↩︎\nIt isn’t feasible to keep separate environments around for many different versions of a program, mostly because Conda environments contain a very large number of files, and OSC has file number quotas. This is why I have in many cases chosen the strategy of just updating the version within the same environment.↩︎\nUnless you first deactivate any active environments in your script.↩︎\nThat is, these settings will be saved somewhere in your OSC home directory, and you never have to set them again unless you need to make changes.↩︎\nThough note that as of September 2023, the singularity command does still work, and it will probably continue to work for a while.↩︎"
  },
  {
    "objectID": "modules/removed.html",
    "href": "modules/removed.html",
    "title": "RNAseq analysis",
    "section": "",
    "text": "Checking the checksums for the downloaded FASTA and GTF (Click to expand)\n\n\n\n\n\nThe NCBI FTP directory for the human genome also contains a file with checksums, md5checksums.txt.\nLet’s download it — we’ll use the -P option to tell wget to put it directly in the data/ref dir:\nwget -P data/ref https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/md5checksums.txt\nIt’s a little harder to check the file integrity in these cases because we only have two out of the several dozen files listed in this md5checksums.txt file (namely, all those in the FTP dir), and to make matters worse, we renamed them.\ncd data/ref\n\ngrep \"GCF_000001405.40_GRCh38.p14_genomic.gtf.gz\" md5checksums.txt\nf573144e507a9fd85150cf6a3c8f8471  ./GCF_000001405.40_GRCh38.p14_genomic.gtf.gz\ngrep \"GCF_000001405.40_GRCh38.p14_genomic.fna.gz\" md5checksums.txt\nc30471567037b2b2389d43c908c653e1  ./GCF_000001405.40_GRCh38.p14_genomic.fna.gz\nmd5sum GCF*\n689762f267eafe361b6ee4b21638eb51  GCF_000001405.40_GRCh38.p14_genomic.fna\na5274984906df2cc65319dfc1b307a01  GCF_000001405.40_GRCh38.p14_genomic.gtf\n\n\n\n\n\n\n\n\n\nAlternatives to looping with a glob (Click to expand)\n\n\n\n\n\nWith genomics data, the routine of looping over an entire directory of files, or selections made with simple globbing patterns, should serve you very well.\nBut in some cases, you may want to iterate only over a specific list of filenames (or partial filenames such as sample IDs) that represent a complex selection.\n\nIf this is a short list, you could directly specify it in the loop:\nfor sample in A1 B6 D3; do\n    R1=data/fastq/\"$sample\"_R1.fastq.gz\n    R2=data/fastq/\"$sample\"_R2.fastq.gz\n    # Some file processing...\ndone\nIf it is a longer list, you could create a simple text file with one line per sample ID / filename, and use command substitution as follows:\nfor fastq_file in $(cat file_of_filenames.txt); do\n    # Some file processing...\ndone\n\nIn cases like this, Bash arrays (basically, variables that consist of multiple values, like a vector in R) or while loops may provide more elegant solutions, but those are outside the scope of this introduction.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "modules/A01_osc.html#introduction",
    "href": "modules/A01_osc.html#introduction",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis session will provide an introduction to supercomputers in general and to the Ohio Supercomputer Center (OSC) specifically.\n\n1.1 Supercomputers\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run.\nYou analyses need large numbers of CPUs or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs.\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working RNAseq data or other kinds of genomic data, many of these reasons apply. This can make it hard or simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\n1.2 The Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio (not The Ohio State University). It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources. At least for folks at OSU, using OSC is currently usually free in practice. Having such a good HPC resource available at no cost is not something we should take for granted — at many institutions, academics not only have have to pay for these kinds of resources, but those are often more limited and not as easy to access.\nIn upcoming sessions, we’ll continue to work at OSC, so you will get a fair bit of experience with using it. We’ll also have specific sessions dedicated to using VS Code at OSC, loading and installing software at OSC and using the Slurm job scheduler.\nOSC has three main websites:\n\nhttps://osc.edu: OSC’s general website, with lots of information about the supercomputers, the software that’s installed, and how to use OSC.\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: A site to manage your account and OSC Projects you are an admin for (login needed)."
  },
  {
    "objectID": "modules/A01_osc.html#the-structure-of-a-supercomputer-center",
    "href": "modules/A01_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 The Structure of a Supercomputer Center",
    "text": "2 The Structure of a Supercomputer Center\nLet’s start with some terminology, going from smaller things to bigger things:\n\nCore / Processor / CPU / Thread — Components of a computer that can each (semi-)indendepently be asked to perform a computing task like running a bioinformatics program. While these terms are not technically all synonyms, we can treat them as such for our purposes.\nNode — A single computer that is a part of a supercomputer and has dozens of cores (i.e., they tend to be more powerful than a personal laptop or desktop).\nSupercomputer / Cluster — Many computers connected by a high-speed network. (“Pitzer” and “Owens” are the two currently active ones at OSC.)\nSupercomputer Center — A facility like OSC that has one or more supercomputers.\n\n\n\n\nThis is what the Owens supercomputer at OSC physically looks like:\n\n\n\n\n\n\n\n\n\nLinux and Unix\n\n\n\nLike the vast majority of supercomputers, those of OSC run on the Linux operating system (as opposed to on MacOS or Windows). In turn, Linux is a Unix-based operating system like MacOS (but unlike Windows).\n\n\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two clusters!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\nLet’s take those in order.\n\n\n2.1 File Systems\nThere are 4 main file systems where you can store files at OSC: Home Directories, Project Directories, Scratch Directories, and Compute storage.\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/ 1\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/ 2\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\nCompute\n$TMPDIR\n1 TB\nNo\nAfter job completes\nCompute job\n\n\n\nYou’ll interact most with the Project directories: this is because for most files, you’ll want a permanent and backed-up location (i.e., not Scratch or Compute storage), and the Home directory offers relatively limited storage as well as challenges with file sharing.\n\n\n\n\n\n\nUnix terminology and environment variables\n\n\n\nWe’ll talk about all of this more in upcoming sessions, but to clarify some of the terms and concepts mentioned here:\n\n“Directory” (or “dir” for short) is a commonly used term in Unix that just means “folder”.\nIn the “Located within” column in the table above, the leading forward slash / signifies the system’s “root” (top-level) directory, and forward slashes are also used to separate directories (unlike in Windows, which uses backslashes).\nFile and directory locations on a computer are often referred to as “paths”.\n$TMPDIR is a so-called “environment variable” that contains the path to the Compute storage directory (in the Unix shell, all variables are referenced by putting a $ before their name, and environment variables are in all-caps). A variable is useful in this case, because the location of this storage space will vary depending on the compute node at which it’s located. Along similar lines, your Home directory’s path is stored in $HOME.\n\n\n\n\n2.1.1 Home Directory\nWhen you initially get an account with OSC, a Home directory is created for you, named with your OSC username. This directory will always be within /users/, and then, somewhat strangely, in a directory containing the name of the OSC Project you were first added to (and this will not change even if you’re no longer a member of that project, or if that project ceases to exist). For example, my Home directory is /users/PAS0471/jelmer.\nYou will only ever have one Home directory. You also cannot expand the standard 500 GB of storage — if you need more space, you should turn to your Project directories.\nIf possible, I recommend to use your Home directory only for some general files (like some software, tutorials and practice, general scripts and databases), and to use Project directories for all your research project data and results.\n\n\n2.1.2 Project Directories\nProject directories are linked to OSC projects, which are typically set up by PIs. They offer flexibility in terms of the amount of storage available, and also in terms of who can access files in the directory.\nBy default, all members of an OSC Project have “read access” (the ability to see and copy files) for all files in a project directory, which makes it suitable for collaborating on a research project. But rest assured: except for OSC staff, other people can never move, modify, or delete your files (i.e., they don’t have “write access”) — not even the admins (PIs) for the OSC Project in question.\nLike Home directories, Project directories are backed up daily. You don’t have direct access to the backups, but if you’ve accidentally deleted some important files (Linux has no thrash bin!), you can request them to be restored to the way they were on a specific date.\n\n\n\n\n\n\nFile Systems are shared among the clusters\n\n\n\nWhile OSC’s current two clusters, Owens and Pitzer, are largely separate, they do share the same File System. This means that you can access your files in the exact same way regardless of which supercomputer you have connected to.\nFor example, your Home directory can be accessed using the same path (in my case, /users/PAS0471/jelmer) on Pitzer and Owens.\n\n\n\n\n2.1.3 Temporary storage: Scratch and Compute\nEvery OSC Project also has a Scratch directory. The two main advantages of Scratch space are that it is effectively unlimited and that it has faster data read and write (“I/O”) speed than Home and Project space. However, it’s not backed up, and files that are unmodified for 90 days are automatically deleted. As such, Scratch storage is mostly useful for intermediate results that are likely not needed again and can be reproduced easily.3\nCompute storage space is even more fleeting: as soon as the compute “job” in question has stopped (e.g. your script has finished), these files will be deleted. We’ll talk a bit more about this type of storage later, as using them can save time for I/O-intensive analyses.\n\n\n\n\n2.2 Login Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, and they are shared among everyone and cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing.\nAttempting large computing efforts on these nodes risks taxing the limited resources on these nodes, and bogging things down for everyone. There are checks built in that limit what you are able to do on the login nodes (i.e. jobs running for longer than 20 minutes will be killed), but it’s best to just not push it at all. Any serious computing should be done on the compute nodes.\n\n\n\n2.3 Compute Nodes\nCompute nodes are really the powerhouse of the supercomputer, and this is where you run your data processing and analysis.\nYou can use compute nodes by putting in requests for resources, such as the number of nodes, cores, and for how long you will need them. Because many different users are sending such requests –i.e., for “compute jobs” or just “jobs”– all the time, there is software called a job scheduler (specifically, Slurm in case of OSC) that considers each request and assigns the necessary resources to the job as they become available.\n\n\n\n\n\n\nInteractive and batch use of compute nodes\n\n\n\nRequests for compute node jobs can be made either through the OnDemand website or with commands like sinteractive and sbatch.\nFor instance, when we start an RStudio session at OSC, we first have to fill out a little form with such a request, and then RStudio will run on a compute node. This is an example of using a compute node interactively — “you” are located on a compute node, and any R command you type will be executed there. More commonly for genomics work, you’ll be using compute nodes non-interactively, that is, through “batch jobs”. When doing so, you will write a script in advance and send it to the job scheduler, which will run the script on a compute node that “you” don’t go to at all.\nThe session Compute Jobs with Slurm is dedicated to this topic.\n\n\nCompute nodes come in different shapes and sizes. “Standard nodes” are by far the most numerous (e.g., Owens has 648 and Pitzer has 564 of them) and even those vary in size, from 28 cores per node (Owens) to 48 cores per node (the “expansion” part of Pitzer). Some examples of other types of nodes are ones with extra memory (largemem and hugemem) and ones that provide access to GPUs (Graphical Processing Units) rather than CPUs.\nFortunately, you don’t tend to have to think much about node types as you start using OSC, since Standard nodes are automatically picked by default, and those will serve you well for the vast of majority genomics analysis.4\n\n\n\n\n\n\nMemory versus storage\n\n\n\nWhen we talk about “memory”, this refers to RAM: the data that your computer has actively “loaded” or in use. For example, if you play a computer game or have many browser tabs open, your computer’s memory will be heavily used. Genomics programs sometimes load all of the input data from disk to memory for fast access, or hold a huge assembly graph in memory, and as such may need a lot of memory as well.\nDon’t confuse memory with file storage, the data that is on disk, some of which may have been unused for years.\n\n\n\n\n\n2.4 Putting it together\nAll these parts are connected together to create a supercomputer — for example, let’s take a look at the specs for Owens now that we understand the components a bit better:"
  },
  {
    "objectID": "modules/A01_osc.html#connecting-to-osc-with-ondemand",
    "href": "modules/A01_osc.html#connecting-to-osc-with-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 Connecting to OSC with OnDemand",
    "text": "3 Connecting to OSC with OnDemand\nThe classic way of connecting to supercomputers is using SSH, like with the ssh command in a Unix shell on your computer (see this reference page). However, OSC has pioneered the use of a web portal called OnDemand, which has since become more widely used among supercomputer centers.\nThe OSC OnDemand website, https://ondemand.osc.edu, then, allows you to access OSC resources through a web browser. When you go there, you first need to log in with your OSC (not OSU!) credentials. After that, you should see a landing page similar to the one below:\n\n\n\nThe main part of the page (below the logo) only contains some general OSC messages and updates — what we will focus on instead are some of the options in the blue bar along the top.\n\n\n3.1 File System Access\nLet’s start with Files. Hovering over this dropdown menu gives a list of directories you have access to. If you’re account is new, you might only have three: a Home directory, and a Project and Scratch directory for one OSC Project.\nFor every project you’re associated with, directories are added — I’m associated with quite a few different projects, so I have a long list under Files. I’ll select the Project directory for the MCIC’s main OSC Project, PAS0471, which is /fs/ess/PAS0471:\n\n\n\nOnce there, I can see a list of directories and files inside this Project directory, and I can click on the directories to explore the contents further.\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders: see the buttons across the top.\nAdditionally, there are Upload and Download buttons for uploading files from your computer to OSC, and downloading them from OSC to your computer. These are only suitable for relatively small transfers, roughly below 1 GB. Other options to transfer files to and from OSC are remote transfer commands like scp (also for smaller transfers), and SFTP or Globus for larger transfers. To learn more about these options, see the reference page on OSC file transfer.\n\n\n\n\n\n\nNote\n\n\n\nWe will skip the “Jobs” dropdown menu in the blue top bar, because in later sessions, we will learn to create, submit, and monitor compute jobs at the command line instead, which quickly becomes more efficient as you get the hang of it.\n\n\n\n\n\n3.2 System Status (in Clusters)\nMoving on to “Clusters”, we’ll start with the item at the bottom of that dropdown menu, “System Status”:\n\n\n\nThis page shows an overview of the current usage of the two clusters, which might help to decide which cluster you want to use and set some expectations for compute job waiting times:\n\n\n\n\n\n\n3.3 Unix Shell Access (in Clusters)\nInteracting with a supercomputer in a point-and-click manner only goes so far. Using a supercomputer effectively requires interacting with the system using a command-line (CLI) rather than a graphical user (GUI) interface.\nAgain under the Clusters option in the blue top bar, you can access a Unix shell either on Owens or Pitzer:\n\n\n\nI’m selecting a shell on the Pitzer supercomputer, which will open a new browser tab looking like this:\n\n\n\nWe most commonly interact with a supercomputer using a Unix shell, and we’ll learn about the basics of doing so in an upcoming session. However, we’ll mostly be accessing a Unix shell in a different manner, namely inside the VS Code text editor, which also gives us some additional functionality in a user-friendly way.\n\n\n3.4 Interactive Apps\nWe can get access to VS Code, as well as many other programs with GUIs such as RStudio, via the Interactive Apps dropdown menu (and the menu item next to that, My Interactive Sessions, will list the sessions that are currently active as well as finished ones).\n\n\n\n\n\n\n\n\n\n‘VS Code’ versus ‘Code Server’\n\n\n\nIn the list, the “VS Code” program is called “Code Server”, much like “RStudio” is called “RStudio Server”. They are the same programs but with minor edits to allow them to run remotely in a browser rather than as locally installed on your own computer.\n\n\n“Interactive Apps” like VS Code and RStudio run on compute nodes — therefore, we need to fill out a form and specify some details for our interactive compute job request:\n\nThe OSC Project that should be billed for the compute resource usage — a dropdown menu will list all Projects you are a member of.\nThe amount of time we want to make a reservation for — we’ll be kicked off as soon as that amount of time has passed!\nThe “working directory” (starting location in the file system) for the program, which we can type in the box or select with the “Select Path” button (the default is your Home directory, here referred to as $HOME).\nThe software version — the most recent available one should be automatically selected, and that is almost always what you’ll want.\n\n\n\n\nClick on Launch at the bottom and this will send your request to the compute job scheduler. First, your job will be “Queued” — that is, waiting for the job scheduler to allocate resources on the compute nodes to it:\n\n\n\nIn general, it should be granted resources within a few seconds (the card will then say “Starting”), and be ready for usage (“Running”) in another couple of seconds:\n\n\n\nThen, you can click on the blue Connect to VS Code button to open a new browser tab that runs VS Code. We’ll explore VS Code in the next session."
  },
  {
    "objectID": "modules/A01_osc.html#upcoming-sessions-on-osc",
    "href": "modules/A01_osc.html#upcoming-sessions-on-osc",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 Upcoming sessions on OSC",
    "text": "4 Upcoming sessions on OSC\nToday, we have learned some of the basics of supercomputers and of accessing OSC. In separate sessions in this series, we will look at:\n\nUsing specific “Interactive Apps” (GUI-based programs):\n\nVS Code\nRStudio (TBA)\n\nLoading and installing command-line software at OSC\nSubmitting batch jobs using the Slurm scheduler\n\nAdditionally, there are pages with reference material (see the right side of the top menu bar of this side) on:\n\nFile transfer to and from OSC\nUsing OSC with SSH (rather than through OnDemand)"
  },
  {
    "objectID": "modules/A01_osc.html#at-home-reading-admin-further-resources",
    "href": "modules/A01_osc.html#at-home-reading-admin-further-resources",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "At-home reading: admin & further resources",
    "text": "At-home reading: admin & further resources\n\nAdministrative miscellaneae\n\nRequesting & managing OSC Projects, and user accounts\nGenerally, only PIs request OSC projects, and they typically manage them as well. OSC has this page with more information on how to do so. Whoever manages an OSC Project can add both existing OSC users and new users to the Project. Anyone added to an OSC Project will have access to the project’s directories, and will be able specify this Project when issuing compute node resource requests.\nWhen you get added to an OSC Project and don’t yet have an OSC account, you will automatically receive an email with a link that allows you to create an account. It is not possible to create an account before having been added to an OSC Project.\nBilling\nOSC will bill OSC Projects (not individual users), and only for the following two things:\n\nFile storage in the Project Storage file system\nCompute node usage by “core hour” (e.g. using 2 cores for 2 hours = 4 core hours)\n\nThe prices for academic usage are quite low (see this page for specifics), and importantly, at OSU, they are often covered at the department level such that individual PIs do not have to directly pay for this at all.\nWhen you use OSC, it’s good practice to acknowledge and cite OSC in your papers, see their citation page.\nFor many questions such as if you have problems with your account, have problems installing or using specific software, or don’t understand why your jobs keep failing, you can email OSC at oschelp@osc.edu. They are usually very quick to respond!\n\n\n\n\nOSC’s learning resources\nTo learn more about OSC, I would start with these short courses:\n\nOSC’s online asynchronous courses\n\nThis includes a number of short videos\nWhen I tried to access these last, it wasn’t always clear where to go after enrolling, and one of the two courses had even diseappeared from the list. But the website https://scarlet.instructure.com then listed the courses and provided access.\n\n\nThis series of pages is also useful:\n\nNew User Resource Guide\nSupercomputing FAQ\nHOWTOs (tutorials on specific topics)\nInfo on batch (non-interactive) compute jobs (rather technical)\nOSC “events” such as Office Hours\n\n\n\n\nAcknowledgements\nThis page uses material from an OSC Introduction written by Mike Sovic and from OSC’s Kate Cahill Software Carpentry introduction to OSC."
  },
  {
    "objectID": "modules/A01_osc.html#footnotes",
    "href": "modules/A01_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr /fs/project/↩︎\nOr /fs/ess/scratch↩︎\nFor example, many genome and transcriptome assemblers output a lot of data, but you will only need a few files (like the assembly) for your next steps.↩︎\nSome examples where you might need a different type of node are genome or transcriptome assembly where you might need nodes with a lot of memory, or Oxford Nanopore sequence data basecalling where you might need GPUs.↩︎"
  }
]