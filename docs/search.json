[
  {
    "objectID": "modules/04_shell.html#introduction",
    "href": "modules/04_shell.html#introduction",
    "title": "The Unix Shell - Part I",
    "section": "1 Introduction",
    "text": "1 Introduction\nMany of the things you typically do by pointing and clicking can alternatively be done by typing commands. The Unix shell allows you to interact with computers via commands. It is natively available through a Terminal app in computers with Unix-like operating systems like Linux (on which OSC runs) or MacOS, and can also be installed on Windows computers with relatively little trouble these days (see the SSH reference page on this website).\nWorking effectively on a remote supercomputer tends to simply require using a command line interface. But there are more advantages to doing command line computing than just allowing you to work on a supercomputer, such as:\n\nWorking efficiently with large files\nAchieving better reproducibility in research\nPerforming general computing tasks more efficiently, once you get the hang of it\nMaking it much easier to repeat similar tasks across files, samples, and projects, with the possibility of true automation\nFor bioinformatics, being able to use (the latest) command-line programs directly without having to depend on “GUI wrappers” written by third parties, that often cost money and also lag behind in functionality\n\nIn these sessions, we’ll use a Unix shell at OSC inside VS Code. For this session, specifically, I will assume you still have an active VS Code session as setup in the previous one, have VS Code located at /fs/ess/PAS0471, and with an open Terminal — if not, see the instructions in the dropdown box right below.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nType pwd to check where you are. If you are not in /fs/ess/PAS0471, click Open folder... in the Welcome tab, or      =&gt;   File   =&gt;   Open Folder, then type/select /fs/ess/PAS0471 and press Ok.\n\n\n\n\n\n\n\n\n\n\nSome Terminology\n\n\n\nWe’re going to focus on the practice of doing command line computing here, and not get too bogged down in terminology, but let’s highlight a few interrelated terms you’re likely to run across:\n\nCommand Line — the most general term, an interface where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\nBash Shell — a Unix shell that uses the Bash language\n\nWhile it might not fly for a computer science class, for day-to-day computing/bioinformatics, you’ll probably hear all these terms used somewhat interchangably. Basically, we’re talking about the process of interacting with your computer by giving it commands as opposed to the point-and-click way you’re likely more familiar with."
  },
  {
    "objectID": "modules/04_shell.html#first-steps",
    "href": "modules/04_shell.html#first-steps",
    "title": "The Unix Shell - Part I",
    "section": "2 First steps",
    "text": "2 First steps\n\n2.1 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies a bit across shells and can also be customized, but our prompts at OSC should show the following:\n[&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example:\n[jelmer@p0080 PAS0471]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n\n\n\nHow shell code is presented on this website\n\n\n\nThe pale gray boxes like the ones shown above will be used to represent your command prompt, or rather, to show the command line expressions that you will type.\nIn upcoming boxes, the prompt itself ([...]$) will not be shown, but only the command line expressions that you type. This is to save space and also because if we omit the prompt, you will be able to directly copy and paste commands from the website to your shell.\nAlso, in a notation like &lt;username&gt;, the &lt; &gt; are there to indicate this is not an actual, functional example, but a descriptive generalization, and should not be part of the final code. In this case, then, it should be replaced merely by a username (e.g. jelmer), and not by &lt;jelmer&gt;, as you can see in the example with the prompt above.\n\n\n\n\n\n2.2 A few simple commands: date, whoami, pwd\nThe Unix shell comes with hundreds of commands. Let’s start with a few simple ones.\nThe date command prints the current date and time:\n\ndate\n\nFri Jul 28 13:04:38 EDT 2023\n\n\n\n\n\n\n\n\nCopying code from the website, and code output\n\n\n\nWhen you hover your mouse above the top box with the command (sometime you have to click in it first), you should see a copy icon appear on the far right, which will copy the command to your clipboard: for longer expressions, this can be handy so you can paste this right into your shell and don’t have to type. Generally speaking, though, learning works better when you type the commands yourself!\nAlso, the darker gray box below, with italic text, is intended to show the output of commands as they are printed to the screen in the shell.\n\n\nThe whoami (who-am-i) command prints your username:\n\nwhoami\n\njelmer\nThe pwd (Print Working Directory) commands prints the path to the directory you are currently located in:\n\npwd\n\n/fs/ess/PAS0471\nAll 3 commands that we just used provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n\n\n\nWorking directories, and paths part I\n\n\n\nOn Unix systems, all the files on a computer exist within a single hierarchical system of directories (folders). When working in the Unix shell, you are always “in” one of these directories. The directory you’re “in” at any given time is referred to as your working directory.\nIn a path (specification of a file or directory location) such as that output by pwd, directories are separated by forward slashes /.\nA leading forward slash in a path indicates the root directory of the computer, and as such, the path provided by pwd is an absolute path (or: full path), and not a relative path — more on that later.\nWhile not shown in the cd output, if you happen to see a trailing forward slash in a path (eg. /fs/ess/PAS0471/), you can be sure that the path points to a directory and not a file."
  },
  {
    "objectID": "modules/04_shell.html#cd-and-command-actions-defaults-and-arguments",
    "href": "modules/04_shell.html#cd-and-command-actions-defaults-and-arguments",
    "title": "The Unix Shell - Part I",
    "section": "3 cd and command actions, defaults, and arguments",
    "text": "3 cd and command actions, defaults, and arguments\nIn the above three command line expressions:\n\nWe merely typed the name of a command and nothing else\nThe main function of the command was to provide some information, which was the output printed to screen\n\nBut many commands perform an action other than printing output. For example, the very commonly used command cd (Change Directory) will, you guessed it, change your working directory. And as it happens, it normally has no output at all.\nWe start by simply typing cd:\n[jelmer@p0080 PAS0471]$ cd\n[jelmer@p0080 ~]$\nDid anything happen? You might expect a command like cd to report what it did, but it does not. As a general rule for Unix commands that perform actions, and one that also applies to cd: if the command does not print any output, this means it was successful.\nSo where did we change our working directory to, given that we did not tell cd where to move? Our prompt (as shown in the code box below) actually did give us a clue: PAS0471 was changed to ~ But what does ~ mean?\n\nYour Turn: Check what directory we moved to\n\n\nSolution (click here)\n\npwd\n/users/PAS0471/jelmer\nIt appears that we moved to our Home directory! (Remember, we were in the Project directory /fs/ess/PAS0471.)\nAnd as it turns out, ~ is a shell shortcut to indicate your Home directory — more on that later.\n\n\nFrom this, we can infer that the default behavior of cd, i.e. when it is not given any additional information, is to move to a user’s home directory. This is actually a nice trick to remember!\nNow, let’s move to another directory, one that contains some files we can explore to learn our next few commands. We can do so by specifying the path to that directory after the cd command (make sure to leave a space after cd!):\ncd /fs/ess/PAS0471/demo/202307_rnaseq/\npwd\n/fs/ess/PAS0471/demo/202307_rnaseq\nIn more abstract terms, what we did above was to provide cd with an argument (namely, the path to the dir to move to). Arguments generally tell commands what file or directory to operate on, and come at the end of a command line expression. There should always be a space between the command and its argument(s)!\n\n\n\n\n\n\nTab completion!! (Click to expand)\n\n\n\n\n\n\nAfter typing /fs/e, press the Tab key!\n/fs/ess/\nAfter typing /fs/ess/P, press the Tab key. Nothing will happen, so now press it quickly twice in succession.\nDisplay all 709 possibilities? (y or n)\nType n. Why does this happen?\nAfter typing /fs/ess/PAS04, press the Tab key twice quickly in succession (“Tab-tab”).\nPAS0400/ PAS0409/ PAS0418/ PAS0439/ PAS0453/ PAS0456/ PAS0457/ PAS0460/ PAS0471/ PAS0472/ PAS0498/ \nAfter typing /fs/ess/PAS0471/demo/2, press the Tab key!\n/fs/ess/PAS0471/demo/202307_rnaseq/\n\nThe tab completion feature will check for files/dirs present in the location you’re at, and based on the characters you’ve typed so for, complete the path as far as it can.\nSometimes it can’t move forward at all because there are multiple files or dirs that have the same next character. Pressing “Tab-tab” will then show your options, though in unusual circumstances like one above, there are so many that it asks for confirmation. In such cases, it’s usually better to just keep typing assuming that you know where you want to go.\nIn general, though, Tab completion is an incredibly useful feature that you should try to get accustomed to using right away!\n\n\n\nAs we’ve seen, then, cd gives no output when it succesfully changed the working directory (“silence is golden”!). But let’s also see what happens when it does not succeed — it gives the following error:\ncd /fs/Ess/PAS0471\nbash: cd: /fs/Ess/PAS0471: No such file or directory\n\nYour Turn: What was the problem with the path we specified?\n\n\nSolutions (click here)\n\nWe used a capital E in /Ess/ — this should have been /ess/.\nIn other words, paths (dir and file specifications) are case-sensitive on Unix systems!\n\n\nIn summary, in this section we’ve learned that:\n\nThe cd command can be used to change your working directory\nUnix commands like cd that perform actions will by default only print output to screen when something goes wrong (i.e., errors)\nCommands can have default behaviours when they are not given specific directions\nWe can give commands like cd arguments to tell them what to do / operate on.\n\nNext, we’ll learn about options to commands in the context of the ls command."
  },
  {
    "objectID": "modules/04_shell.html#ls-and-command-options",
    "href": "modules/04_shell.html#ls-and-command-options",
    "title": "The Unix Shell - Part I",
    "section": "4 ls and command options",
    "text": "4 ls and command options\n\n4.1 The default behavior of ls\nThe ls command, short for “list”, is a quite flexible command to list files and directories:\nls\ndata  metadata  README.md\n(You should still be in /fs/ess/PAS0471/demo/202307_rnaseq. If not, cd there first.)\n\n\n\n\n\n\nls output colors\n\n\n\nUnfortunately, the ls output shown above does not show the different colors you should see in your shell — here are some of the most common ones:\n\nEntries in blue are directories (like data and metadata above)\nEntries in black are regular files (like README.md above)\nEntries in red are compressed files (we’ll see an example soon).\n\n\n\nThe default behavior of ls includes that it will:\n\nList files and dirs inside our current working directory, and do so non-recursively: it won’t list files inside those directories, and so on.\nShow as many files and dirs as it can on one line, each separated by a few spaces\nSort files and dirs alphabetically (and not separately so)\nNot show any other details about the files, such as their size, owner, and so on.\n\nAll of this, and more, can be changed by providing ls with options and/or arguments.\n\n\n4.2 First, more on arguments\nLet’s start with an argument, since we’re familiar with those in the context of cd. Any argument to ls should be a path to operate on. For example, if we wanted to see what’s inside that mysterious data dir, we could type:\nls data\nfastq\nWell, that’s not much information, just another dir — so let’s look inside that:\nls data/fastq  # These will be shown in red in your output, since they are compressed\nASPC1_A178V_R1.fastq.gz  ASPC1_G31V_R2.fastq.gz      Miapaca2_G31V_R1.fastq.gz\nASPC1_A178V_R2.fastq.gz  Miapaca2_A178V_R1.fastq.gz  Miapaca2_G31V_R2.fastq.gz\nASPC1_G31V_R1.fastq.gz   Miapaca2_A178V_R2.fastq.gz\nAh, there are some gzipped FASTQ files! These contain our sequence data, and we’ll go and explore them in a bit.\nWe can also provide ls with multiple arguments — and it will nicely tell us which files are in each of the dirs we specified:\nls data metadata\ndata:\nfastq\n\nmetadata:\nmeta.tsv\nMany Unix commands will accept multiple arguments (files or dirs to operate on), which can be very useful.\n\n\n4.3 Options\nFinally, we’ll turn to options. Whereas, in general, arguments tell a command what to operate on, options (also called “flags”) will modify its behavior.\nFor example, we can call ls with the option -l (a dash followed by a lowercase L):\nls -l \ntotal 17\ndrwxr-xr-x 3 jelmer PAS0471 4096 Jul 27 11:53 data\ndrwxr-xr-x 2 jelmer PAS0471 4096 Jul 27 11:54 metadata\n-rw-r--r-- 1 jelmer PAS0471  963 Jul 27 16:48 README.md\nNotice that it lists the same three items as our first ls call above, but now, they’re printed in a different format: one item per line, with lots of additional information included. For example, the date and time is that when the file was last modified, and the numbers just to the left of that (e.g., 4096) show the file sizes in bytes1.\nLet’s add another option, -h — before reading on, can you pick out what it did to modify the output?\nls -l -h\ntotal 17K\ndrwxr-xr-x 3 jelmer PAS0471 4.0K Jul 27 11:53 data\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Jul 27 11:54 metadata\n-rw-r--r-- 1 jelmer PAS0471  964 Jul 27 17:48 README.md\nNote the difference in the format of the column reporting the sizes of the items listed — we now have “human-readable filesizes”, where sizes on the scale of kilobytes will be shown in Ks, of megabytes in Ms, and of gigabytes in Gs.\nMany options have a “long option” counterpart, i.e. a more verbose way of specifying the option. For example, -h can also be specified as --human-readable:\nls -l --human-readable      # Output not shown, same as above\n(And then there are also options that are only available in long format — even with case-sensitivity, one runs out of single-letter abbreviations at some point!)\nDespite that short options like the -l and -s we’ve seen (single-dash, single-letter) are very terse and may at times seem impossible to remember, they are still often preferred with common Unix commands, because they are shorter to type — and keep in mind that you might use, say, ls -lh dozens if not hundreds of time a day if you work in the Unix shell a lot.\nA very useful feature of “short options” is that they can be pasted together as follows:\nls -lh   # Output not shown, same as above\n\n\n\n\n\n\nMore on the long-format output of ls\n\n\n\nThe figure below shows what information is shown in each of the columns (but note that it shows a different listing of files, and uses the new-to-us -a option, short for “all”, to also show “hidden files”):\n\n\n\n\n\n\n\n\n4.4 Combining options and arguments\nFinally, we can combine options and arguments, and let’s do so take a closer look at our dir with FASTQ files — now the -h option is especially useful because it makes it easy to see that the files vary between 4.1 MB and 5.3 MB in size:\nls -lh data/fastq\ntotal 38M\n-rw-r--r-- 1 jelmer PAS0471 4.1M Jul 27 11:53 ASPC1_A178V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.2M Jul 27 11:53 ASPC1_A178V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.1M Jul 27 11:53 ASPC1_G31V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 4.3M Jul 27 11:53 ASPC1_G31V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.1M Jul 27 11:53 Miapaca2_A178V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.3M Jul 27 11:53 Miapaca2_A178V_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.1M Jul 27 11:53 Miapaca2_G31V_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.3M Jul 27 11:53 Miapaca2_G31V_R2.fastq.gz\n(Beginners are often inclined to move to a directory when they just want to ls its contents, but its often more convenient to stay put and use an argument to ls instead, like we did above.)\n\n\n\n4.5 Recap of ls, arguments, and options\nIn summary, in this section we have learned that:\n\nThe ls command lists files (by default without additional info and non-recursively)\nUsing arguments, we tell ls (and other commands) what to operate on. Arguments come at the end of the command line epxression, and are not preceded by a dash or any other “pointer”. They are typically names of files or dirs, but can be other things too.\nUsing options, we can make ls (and other commands) show us the results in different ways. They are preceded by at least one dash (-, like -l).\n\n\n\n\n\n\n\nOther types of options\n\n\n\nThe options we’ve seen so far act as “on/off switches”, and this is very common among Unix commands.\nBut some options are not on/off switches and accept values (confusingly, these values can also be called “arguments” to options). For example, the --color option to ls determines how it colorizes its output: there is ls --color=never — versus, among other possibilities, ls --color=always.\nWe’ll see a lot of options that take values when running bioinformatics programs, such as to set specific analysis parameters — for example: trim_galore --quality 30 --length 50 would set a minimum Phred quality score threshold of 30 and a minimum read length threshold of 50 bases for the program TrimGalore, which we will later use to quality-trim and adapter-trim FASTQ files. (This --&lt;option&gt; &lt;value&gt; syntax, i.e. without an = is more common than the --&lt;option&gt;=&lt;value&gt; syntax shown for ls above.)\nIn contrast to when you are using common Unix commands, I would recommend to mostly use long options whenever available when running bioinformatics programs like TrimGalore. That way, it’s easier for you to remember what you did with that option, and more likely to be immediately understood by anyone else reading the code (cf. trim_galore -q 30 -l 50 and trim_galore --quality 30 --length 50).\n\n\n\n\n\n\n\n\nThe tree command and recursive ls (Click to expand)\n\n\n\n\n\nThe tree command lists files recursively (i.e., it will also show us what’s contained in the directories in our working directory), and does so in a tree-like fashion — this can be great to quickly get an intuitive overview of files in a dir:\ntree -C     # The -C option will colorize the output\n\n\n\nAs an aside: if we want to make ls list files recursively, we can use the -R option:\nls -R\n.:\ndata  metadata  README.md\n\n./data:\nfastq\n\n./data/fastq:\nASPC1_A178V_R1.fastq.gz  ASPC1_G31V_R2.fastq.gz      Miapaca2_G31V_R1.fastq.gz\nASPC1_A178V_R2.fastq.gz  Miapaca2_A178V_R1.fastq.gz  Miapaca2_G31V_R2.fastq.gz\nASPC1_G31V_R1.fastq.gz   Miapaca2_A178V_R2.fastq.gz\n\n./metadata:\nmeta.tsv"
  },
  {
    "objectID": "modules/04_shell.html#paths",
    "href": "modules/04_shell.html#paths",
    "title": "The Unix Shell - Part I",
    "section": "5 Paths",
    "text": "5 Paths\nAs we’ve mentioned, “paths” are specifications of a location on a computer, either of a file or a directory.\nWe’ve talked about the commands cd and ls that operate on paths, and without going into much detail about it so far, we’ve already seen two distinct ways of specifying paths:\n\nAbsolute (full) paths always start from the root directory of the computer, which is represented by a leading /, such as in /fs/scratch/PAS0471/.\n(Absolute paths are like GPS coordinates to specify a geographic location on earth: they will provide location information regardless of where we are ourselves.)\nRelative paths start from your current location (working directory). When we typed ls data earlier, we indicated that we wanted to show the contents of the data directory located inside our current working directory — that probably seemed intuitive. But be aware that the shell would look absolutely nowhere else for that dir than in our current working directory.\n(Relative paths are more like directions to a location that say things like “turn left” — these instructions depend on our current location.)\n\nAbsolute paths may seem preferable because they will work regardless of where you are located, but:\n\nThey can be a lot more typing than we need (or want) to do.\nWhile context-specific, a much more important disadvantage of absolute paths is that they can only be expected to work on one specific computer, and are guaranteed not to work after you move files around.\n\n\nHow might relative dirs work on other computers or after moving files?\n\n\nSolution (click here)\n\nSay that Lucie has a directory for a research project, /fs/ess/PAS0471/lucie/rnaseq1, with lots of dirs and files contained in it. In all her code, she specify paths relative to that top-level project directory.\nThen, she share that entire directory with someone else, copying it off OSC. If her collaborator goes wherever they now have that directory stored, e.g. /home/philip/lucie_collab/rnaseq1, and then start using Lucie’s code with relative paths, they would still work.\nSimilarly, if Lucie moves her dir to /fs/scratch/PAS0805/lucie/rnaseq1, all her code with relative paths would still work as well.\nThis is something we’ll come back to later when talking about reproducibity.\n\n\n\n\n5.1 Moving “up” when using relative paths\nThere are a couple of “shortcuts” available for relative paths. First of all, . (a single period) is another way of representing the current working directory. Therefore, for instance, ls ./data is functionally the same as ls data, and just a more explicit way of saying that the data dir is located in your current working dir (this syntax is occasionally helpful).\nMore usefully for our purposes here, .. (two periods) means one level up in the directory hierarchy, with “up” meaning towards the root directory (I guess the directory tree is best visualized upside down!):\nls ..               # One level up, listing /fs/ess/PAS0471/demo\n202307_rnaseq\nThis pattern can be continued all the way to the root of the computer, so ../.. would list files two levels up:\nls ../..            # Two levels up, listing /fs/ess/PAS0471\naarevalo       conroy      frederico       Nisha     osu8947              ross\nacl            containers  hsiangyin       osu10028  osu9207              Saranga\nAlmond_Genome  danraywill  jelmer          osu10436  osu9207_Lubell_bkup  Shonna\namine1         data        jelmer_osu5685  osu5685   osu9390              SLocke\nap_soumya      demo        jlhartman       osu6702   osu9453              sochina\naudreyduff     dhanashree  linda           osu8107   osu9657\nbahodge11      edwin       Maggie          osu8468   pipeline\ncalconey       ferdinand   mcic-scripts    osu8548   poonam\ncamila         Fiama       Menuka          osu8618   raees\nCecilia        Flye        nghi            osu8900   rawalranjana44\nAlong these lines, there are two other shortcuts worth mentioning:\n\n~ represents your Home directory, so cd ~ would move there and ls ~ would list the files there\n- is a cd-specific shortcut that it is like the “back” button in your browser: it will go to your previous location. (But it only has a memory of 1, so subsequent cd -s would simply move you back and forth between two directories.)\n\n\n\n\n\n\n\nThese shortcuts work with all commands\n\n\n\nExcept for -, all of the above shortcuts are general shell shortcuts that work with any command that takes a path."
  },
  {
    "objectID": "modules/04_shell.html#recap",
    "href": "modules/04_shell.html#recap",
    "title": "The Unix Shell - Part I",
    "section": "6 Recap",
    "text": "6 Recap\nWe’ve learned about structure of command line expressions in the Unix shell, which include: the command itself, options, arguments, and output (including, in some cases, error messages).\nA few key general points to remember are that:\n\nCommands that take actions like changing directory (and the same will be true for commands that copy and move files, for example) will by default not print any output to the screen, only errors if those occur.2\n\n\nCommands whose main function is to provide information (think ls, date, pwd) will print their output to the screen. We’ll learn later how we can “redirect” output to a file or to another command!\nUsing options (ls -l), we can modify the behavior of a command, and using arguments (ls data), we can modify what it operates on in the first place.\n\n\n\n\n\n\n\nAlways start with a command\n\n\n\nOne additional, important thing to realize about the structure of command line expressions is this:\nEverything you type on the command line should start with the name of a command, or equivalently, a program or script (these are all just “programs”).\nTherefore, for example, just typing the name of a file, even if it exists in your current working directory, will return an error. (I.e., it won’t do anything with that file, such as printing its contents, like you had perhaps expected.) This is because the first word of a command line expressio should be a command, and the name of a file is (usually!) not a command:\nREADME.md\nREADME.md: command not found\n\n\n\n\n\n\n\n\nMany bioinformatics programs are basically specialized commands\n\n\n\nIn many ways, as mentioned in the box above, you can think of using a command-line bioinformatics program as using just another command.\nTherefore, our general skills with Unix commands will very much extend to using command-line bioinformatics tools!\n\n\nWe’ve learned to work with the following truly ubiquitous Unix commands:\n\npwd — print your current working directory\ncd — change your working directory\nls — list files and dirs\n\nAnd we have seen a few other simpler utility commands as well (date, whoami, and tree in a dropdown box).\nWe’ll continue with the basics of the Unix shell in part II (TBA)."
  },
  {
    "objectID": "modules/04_shell.html#at-home-reading-getting-help",
    "href": "modules/04_shell.html#at-home-reading-getting-help",
    "title": "The Unix Shell - Part I",
    "section": "7 At-home reading: getting help",
    "text": "7 At-home reading: getting help\nWe saw several different options for the ls command, and that may have left you wondering how you are supposed to know about them.\n\n7.1 The --help option\nMany (but not all!) commands have a --help option which will primarily describe the command’s function and “syntax” including many of the available options.\nFor a very brief example, try:\nwhoami --help\nFor a much longer example, try:\nls --help\n\n\n7.2 The man command\nThe man command provides manual pages for Unix commands, which is more complete than the --help help, but sometimes overwhelming as well as terse and not always easy to fully understand — Google is your friend as well!\nFor a short example, try:\nman pwd\n\n\n\n\n\n\nThe man page opens in a “pager” – try to scroll around and type q to quit!\n\n\n\n\n\n\nFor a much longer example, try:\nman ls"
  },
  {
    "objectID": "modules/04_shell.html#footnotes",
    "href": "modules/04_shell.html#footnotes",
    "title": "The Unix Shell - Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough these sizes are only directly useful for files, not dirs! You can also ignore the total 17 line at the top.↩︎\nWe’ll see later on how we can make commands more “verbose” than they are by default, which can certainly be useful.↩︎"
  },
  {
    "objectID": "modules/03_vscode.html",
    "href": "modules/03_vscode.html",
    "title": "The VS Code Text Editor",
    "section": "",
    "text": "In this module, we will learn the basics of a fancy text editor called VS Code (in full, Visual Studio Code). Conveniently, we can use a version of this editor (sometimes referred to as Code Server) in our browser via the OSC OnDemand website.\nWe will use VS Code throughout these sessions as practically a one-stop solution for our computing activities at OSC. This is also how I use this editor in my daily work.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. If you’ve ever worked with R, the RStudio program is another good example of an IDE. For our purposes, RStudio will be our IDE for R, and VS code will be our IDE for shell scripts and code."
  },
  {
    "objectID": "modules/03_vscode.html#starting-a-vs-code-session-in-osc-ondemand",
    "href": "modules/03_vscode.html#starting-a-vs-code-session-in-osc-ondemand",
    "title": "The VS Code Text Editor",
    "section": "1 Starting a VS Code session in OSC OnDemand",
    "text": "1 Starting a VS Code session in OSC OnDemand\nIn the previous session, I showed you how to start a VS Code session in OnDemand, but for the sake of completeness, instructions to do so are also shown below.\n\n\n\n\n\n\nStarting VS Code at OSC\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nThe starting directory doesn’t matter\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code."
  },
  {
    "objectID": "modules/03_vscode.html#getting-started-with-vs-code",
    "href": "modules/03_vscode.html#getting-started-with-vs-code",
    "title": "The VS Code Text Editor",
    "section": "2 Getting started with VS Code",
    "text": "2 Getting started with VS Code\n\n\n\n\n2.1 Side bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA      (“hamburger menu” icon) in the top, which has most of the standard menu items that you often find in a top bar, like File.\nA      (cog wheel icon) in the bottom, through which you can mainly access settings.\nA bunch of icons in the middle that serve to switch between different options for the (wide) Side Bar, which can show one of the following:\n\nExplorer: File browser (and, e.g., an outline for the active file)\nSearch: To search recursively across all files in the active folder\nSource Control: To work with version control systems like Git (not used in this workshop)\nRun and Debug: For debugging your code (not used in this workshop)\nExtensions: To install extensions (we’ll install one later)\n\n\n\n\n2.2 Editor pane and Welcome document\nThe main part of the VS Code is the editor pane. Whenever you open VS Code, a tab with a Welcome document is automatically opened. This provides some help for beginners, but also, for example, a handy overview of recently opened folders.\nWe can also use the Welcome document to open a new text file by clicking New file below Start (alternatively, click      =&gt;   File   =&gt;   New File), which open as a second “tab” in the editor pane. We’ll work with our own text files (scripts) starting tomorrow.\n\n\n\n\n\n\nRe-open the Welcome document\n\n\n\nIf you’ve closed the Welcome document but want it back, click      =&gt;   Help   =&gt;   Welcome.\n\n\n\n\n2.3 Terminal (with a Unix shell)\n By default, no terminal is open in VS Code – open one by clicking      =&gt; Terminal =&gt; New Terminal.\nThis opens up a terminal with a Unix shell. In the next session, we’ll start talking about actually using the Unix shell."
  },
  {
    "objectID": "modules/03_vscode.html#a-folder-as-a-starting-point",
    "href": "modules/03_vscode.html#a-folder-as-a-starting-point",
    "title": "The VS Code Text Editor",
    "section": "3 A folder as a starting point",
    "text": "3 A folder as a starting point\nConveniently, VS Code takes a specific folder (directory) as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nBy default, VS Code via OnDemand will open your Home directory.\nHere, we’ll change to the project dir for OSC project PAS0471, which is /fs/ess/PAS0471.\n Let’s open that folder. Click Open folder... in the Welcome tab (or      =&gt;   File   =&gt;   Open Folder).\nYou’ll notice that the program completely reloads. And You might also see a pop-up like this – you can check the box and click Yes:\n\n\n\n\n\n\n\n\n\n\n\nTaking off where you were\n\n\n\nWhen you reopen a folder you’ve had open before, VS Code will resume where you were before in terms of:\n\nReopening any files you had open\nIf you had an active terminal, it will re-open a terminal.\n\nThis is quite convenient, especially when you start working on multiple projects (different folders) in VS Code and frequently switch between those."
  },
  {
    "objectID": "modules/03_vscode.html#some-vs-code-tips-and-tricks",
    "href": "modules/03_vscode.html#some-vs-code-tips-and-tricks",
    "title": "The VS Code Text Editor",
    "section": "4 Some VS Code tips and tricks",
    "text": "4 Some VS Code tips and tricks\n\n4.1 Making use of your screen’s real estate\nSince we are using VS Code inside a browser window, we are unfortunately losing some screen space. Make sure to maximize the browser window and if you have a bookmarks bar, you should consider hiding it (for Chrome: Ctrl/⌘+Shift+B).\nYou may also opt to hide the side bars using the    =&gt;   View   =&gt;   Appearance menu (or Ctrl/⌘+B for the (wide) Side Bar).\n\n\n4.2 Resizing panes\nYou can resize panes (the terminal, editor, and side bar) by hovering your cursor over the borders and then dragging it.\n\n\n4.3 The Command Palette / Color themes\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for.\nTo access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P).\n\nOn Your Own: Try a few color themes\nOpen the Command Palette and start typing “color theme”, and you’ll see the relevant option pop up.\nThen, try out a few themes and see what you like!\n(You can also access the Color Themes option via      =&gt; Color Theme.)"
  },
  {
    "objectID": "modules/03_vscode.html#at-home-reading",
    "href": "modules/03_vscode.html#at-home-reading",
    "title": "The VS Code Text Editor",
    "section": "5 At-home reading",
    "text": "5 At-home reading\n\nkeyboard shortcuts\nWorking with keyboard shortcuts (also called “keybindings”) for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, replace Ctrl with ⌘).\n\n\n\n\n\n\nKeyboard shortcut cheatsheet\n\n\n\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\n\n\n\nOpen a terminal: Ctrl+` (backtick) or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nToggle the (wide) Side Bar: Ctrl+B\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+⬆/⬇ will move lines up or down.\n\nMultiple cursors: Press & hold Ctrl+Shift, then ⬆/⬇ arrows to add cursors upwards or downwards.\nToggle line comment (“comment out” code, and removing those comment signs): Ctrl+/\nSplit the editor window vertically: Ctrl+\\ (See also the options in      View =&gt; Editor Layout)\n\n\n\n\n\n\n\nBrowser interference\n\n\n\nUnfortunately, some VS Code and terminal keyboard shortcuts don’t work in this setting where we are using VS Code inside a browser, because existing browser keyboard shortcuts take precedence.\nIf you end up using VS Code a lot in your work, it is therefore worth switching to your own installation of the program — see the section below.\n\n\n\n\n\nLocal VS Code installation\nAnother nice feature of VS Code is that it is freely available for all operating systems (and even though it is made by Microsoft, it is also open source).\nTherefore, if you like the program, you can also install it on your own computer and do your local text editing / script writing in the same environment at OSC (it is also easy to install on OSU-managed computers, because it is available in the OSU “Self Service” software installer).\nEven better, the program can be “tunneled into” OSC, so that your working directory for the entire program can be at OSC rather than on your local computer. This gives the same experience as using VS Code through OSC OnDemand, except that you’re not working witin a browser window, which has some advantages (also: no need to fill out a form, and you’ll never run out of time).\nTo install VS Code on your own machine, follow these instructions from the VS Code website: Windows / Mac / Linux.\nTo SSH-tunnel VS Code into OSC, see these instructions on the SSH reference page on this website (they are a bit rudimentary, ask me if you get stuck)."
  },
  {
    "objectID": "modules/05_vars-loops.html",
    "href": "modules/05_vars-loops.html",
    "title": "Variables, Globbing, and Loops",
    "section": "",
    "text": "In this module, we will cover a few topics that are good to know about before you start writing and running shell scripts:\nThese are valuable skills in general — globbing is an essential technique in the Unix shell, and variables and for loops ubiquitous programming concepts."
  },
  {
    "objectID": "modules/05_vars-loops.html#setup",
    "href": "modules/05_vars-loops.html#setup",
    "title": "Variables, Globbing, and Loops",
    "section": "1 Setup",
    "text": "1 Setup\nStarting a VS Code session with an active terminal:\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/&lt;your-folder&gt; in the box Working Directory (replace &lt;your-folder&gt; by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    =&gt; Terminal =&gt; New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =&gt;   File   =&gt;   Open Folder and enter /fs/ess/scratch/PAS2250/&lt;your-folder&gt;."
  },
  {
    "objectID": "modules/05_vars-loops.html#variables",
    "href": "modules/05_vars-loops.html#variables",
    "title": "Variables, Globbing, and Loops",
    "section": "2 Variables",
    "text": "2 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs.\nUsing variables makes it easier to change such settings. We also need to understand variables to work with loops and with scripts.\n\n2.1 Assigning and referencing variables\nTo assign a value to a variable in Bash (in short: to assign a variable), use the syntax variable=value:\n\n# Assign the value \"beach\" to the variable \"location\":\nlocation=beach\n\n# Assign the value \"200\" to the variable \"nlines\":\nnlines=200\n\n\n\n\n\n\n\nBe aware: don’t put spaces around the equals sign (=)!\n\n\n\n\n\n\nTo reference a variable (i.e., to access its value), you need to put a dollar sign $ in front of its name. We’ll use the echo command to review the values that our variables contain:\n\n\n\n\n\n\necho simply prints back (“echoes”) whatever you tell it to\n\n\n\n\necho Hello!\n\nHello!\n\n\n\n\n\necho $location\n\n\n\nbeach\n\n\n\necho $nlines\n\n\n\n200\n\n\nConveniently, we can directly use variables in lots of contexts, as if we had instead typed their values:\n\ninput_file=data/fastq/SRR7609467.fastq.gz\n\nls -lh $input_file \n\n-rw-r--r--@ 1 poelstra.1  staff   8.3M Jul 16 16:12 data/fastq/SRR7609467.fastq.gz\n\n\n\nls_options=\"-lh\"            # (We'll talk about the quotes that are used here later)\n\nls $ls_options data/meta\n\ntotal 8\n-rw-r--r--@ 1 poelstra.1  staff   583B Jul 16 16:12 meta.tsv\n\n\n\n\n\n2.2 Rules and tips for naming variables\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file and $ls_options above, as opposed to say $x and $bla.\nThere are multiple ways of distinguishing words in the absence of spaces, such as $inputFile and $input_file: I prefer the latter, which is called “snake case”, and I always use lowercase.\n\n\n\n2.3 Quoting variables\nAbove, we learned that a variable name cannot contain spaces. But what happens if our variable’s value contains spaces? First off, when we try to assign the variable without using quotes, we get an error:\n\ntoday=Thu, Aug 18\n\n\nAug: command not found\n\n\n\n\n\n\n\nWhy do you think we got this error?\n\n\n\n\n\nBash tried assign everything up to the first space (i.e., Thu,) to today. After that, since we used a space, it assumed the next word (Aug) was something else: specifically, another command.\n\n\n\nBut it works when we quote (with double quotes, \"...\") the entire string that makes up the value:\n\ntoday=\"Thu, Aug 18\"\necho $today\n\nThu, Aug 18\n\n\n\nNow, let’s try to reference this variable in another context. Note that the touch command can create new files, e.g. touch a.txt creates the file a.txt. So let’s try make a new file with today’s date:\n\ntouch README_$today.txt\nls\n\n\n18.txt\nAug\nREADME_Thu,\n\n\n\n\n\n\n\nWhat went wrong here?\n\n\n\n\n\nThe shell performed so-called field splitting using spaces as a separator, splitting the value into three separate units – as a result, three files were created.\n\n\n\nLike with assignment, our problems can be avoided by quoting a variable when we reference it:\n\ntouch README_\"$today\".txt\n\n# This will list the most recently modified file (ls -t sorts by last modified date):\nls -t | head -n 1\n\n\n\nREADME_Thu, Aug 18.txt\n\n\nIt is good practice to quote variables when you reference them: it never hurts, and avoids unexpected surprises.\n\n\n\n\n\n\nAt-home reading: Where does a variable name end?\n\n\n\n\n\nAnother issue we can run into when we don’t quote variables is that we can’t explicitly define where a variable name ends within a longer string of text:\n\necho README_$today_final.txt\n\n\n\nREADME_.txt\n\n\n\n\n\n\n\n\nWhat went wrong here? (Hint: check the coloring highlighting above)\n\n\n\n\n\n\nFollowing a $, the shell will stop interpreting characters as being part of the variable name only when it encounters a character that cannot be part of a variable name, such as a space or a period.\nSince variable names can contain underscores, it will look for the variable $today_final, which does not exist.\nImportantly, the shell does not error out when you reference a non-existing variable – it basically ignores it, such that README_$today_final.txt becomes README_.txt, as if we hadn’t referenced any variable.\n\n\n\n\nQuoting solves this issue, too:\n\necho README_\"$today\"_final.txt\n\n\n\nREADME_Thu, Aug 18_final.txt\n\n\n\n\n\n\n\n\n\n\n\nAt-home reading: Quoting as “escaping” special meaning – and double vs. single quotes\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, we are escaping other “special characters”, such as globbing wildcards, with double quotes. Compare:\n\necho *     # This will echo/list all files in the current working dir (!)\n\n18.txt Aug README_Thu, README_Thu, Aug 18.txt data sandbox scripts\n\n\n\necho \"*\"   # This will simply print the \"*\" character \n\n*\n\n\nHowever, as we saw above, double quotes do not turn off the special meaning of $ (denoting a string as a variable):\n\necho \"$today\"\n\n\n\nThu, Aug 18\n\n\n…but single quotes will:\n\necho '$today'\n\n$today\n\n\n\n\n\n\n\n\n2.4 Command substitution\nIf you want to store the result of a command in a variable, you can use a construct called “command substitution” by wrapping the command inside $().\nLet’s see an example. The date command will print the current date and time:\n\ndate\n\nWed Jul 19 12:15:53 EDT 2023\n\n\nIf we try to store the date in a variable directly, it doesn’t work: the literal string “date” is stored, not the output of the command:\n\ntoday=date\necho \"$today\"\n\ndate\n\n\nThat’s why we need command substitution with $():\n\ntoday=$(date)\necho \"$today\"\n\nWed Jul 19 12:15:53 EDT 2023\n\n\n\nIn practice, you might use command substitution with date to include the current date in files. To do so, first, note that we can use date +%F to print the date in YYYY-MM-DD format, and omit the time:\n\ndate +%F\n\n2023-07-19\n\n\nLet’s use that in a command substitution — but a bit differently than before: we use the command substitution $(date +%F) directly in our touch command, rather than first assigning it to a variable:\n\n# Create a file with our $today variable:\ntouch README_\"$(date +%F)\".txt\n\n# Check the name of our newly created file:\nls -t | head -n 1\n\n\n\nREADME_2023-07-19.txt\n\n\nAmong many other uses, command substitution is handy when you want your script to report some results, or when a next step in the script depends on a previous result.\n\n\nOn Your Own: Command substitution\nSay we wanted to store and report the number of lines in a file, which can be a good QC measure for FASTQ and other genomic data files.\nwc -l gets you the number of lines, and you can use a trick to omit the filename:\n\nwc -l data/fastq/SRR7609472.fastq.gz\n\n   30387 data/fastq/SRR7609472.fastq.gz\n\n\n\n# Use `&lt;` (input redirection) to omit the filename:\nwc -l &lt; data/fastq/SRR7609472.fastq.gz\n\n   30387\n\n\nUse command substitution to store the output of the last command in a variable, and then use an echo command to print:\nThe file has 30387 lines\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnlines=$(wc -l &lt; data/fastq/SRR7609472.fastq.gz)\n\necho \"The file $nlines lines\"\n\nThe file    30387 lines\n\n\nNote: You don’t have to quote variables inside a quoted echo call, since it’s, well, already quoted. If you also quote the variables, you will in fact unquote it, although that shouldn’t pose a problem inside echo statements.\n\n\n\n\n\n\n2.5 At-home reading: Environment variables\n\n\n\n\n\n\nEnvironment variable basics\n\n\n\n\n\nThere are also predefined variables in the Unix shell: that is, variables that exist in your environment by default. These so-called “environment variables” are always spelled in all-caps:\n\n# Environment variable $USER contains your user name \necho $USER\n\npoelstra.1\n\n\n\n# Environment variable $HOME contains the path to your home directory\necho $HOME\n\n\n/users/PAS0471/jelmer\n\nEnvironment variables can provide useful information. They can especially come in handy in in scripts submitted to the Slurm compute job scheduler."
  },
  {
    "objectID": "modules/05_vars-loops.html#globbing-with-shell-wildcard-expansion",
    "href": "modules/05_vars-loops.html#globbing-with-shell-wildcard-expansion",
    "title": "Variables, Globbing, and Loops",
    "section": "3 Globbing with Shell wildcard expansion",
    "text": "3 Globbing with Shell wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing.\n\n3.1 Shell wildcards\nIn the term “wildcard expansion”, wildcard refers to a few symbols that have a special meaning: specifically, they match certain characters in file names. We’ll see below what expansion refers to.\nHere, we’ll only talk about the most-used wildcard, *, in detail. But for the sake of completeness, I list them all below:\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne [] or everything except one ([^]) of the “character set” within brackets\n\n\n\n\n\n\n3.2 The * wildcard and wildcard expansion\nA a first example of using *, to match all files in a directory:\n\nls data/fastq/*\n\ndata/fastq/SRR7609467.fastq.gz\ndata/fastq/SRR7609468.fastq.gz\ndata/fastq/SRR7609469.fastq.gz\ndata/fastq/SRR7609470.fastq.gz\ndata/fastq/SRR7609471.fastq.gz\ndata/fastq/SRR7609472.fastq.gz\ndata/fastq/SRR7609473.fastq.gz\ndata/fastq/SRR7609474.fastq.gz\ndata/fastq/SRR7609475.fastq.gz\ndata/fastq/SRR7609476.fastq.gz\ndata/fastq/SRR7609477.fastq.gz\ndata/fastq/SRR7609478.fastq.gz\n\n\nOf course ls data/fastq would have shown the same files, but what happens under the hood is different:\n\nls data/fastq — The ls command detects and lists all files in the directory\nls data/fastq/* — The wildcard * is expanded to all matching files, (in this case, all the files in this directory), and then that list of files is passed to ls. This command is therefore equivalent to running:\n\nls data/fastq/SRR7609467.fastq.gz data/fastq/SRR7609468.fastq.gz data/fastq/SRR7609469.fastq.gz data/fastq/SRR7609470.fastq.gz data/fastq/SRR7609471.fastq.gz data/fastq/SRR7609472.fastq.gz data/fastq/SRR7609473.fastq.gz data/fastq/SRR7609474.fastq.gz data/fastq/SRR7609475.fastq.gz data/fastq/SRR7609476.fastq.gz data/fastq/SRR7609477.fastq.gz data/fastq/SRR7609478.fastq.gz\n\n\nTo see this, note that we don’t need to use ls at all to get a listing of these files!\n\necho data/fastq/*\n\ndata/fastq/SRR7609467.fastq.gz data/fastq/SRR7609468.fastq.gz data/fastq/SRR7609469.fastq.gz data/fastq/SRR7609470.fastq.gz data/fastq/SRR7609471.fastq.gz data/fastq/SRR7609472.fastq.gz data/fastq/SRR7609473.fastq.gz data/fastq/SRR7609474.fastq.gz data/fastq/SRR7609475.fastq.gz data/fastq/SRR7609476.fastq.gz data/fastq/SRR7609477.fastq.gz data/fastq/SRR7609478.fastq.gz\n\n\n\nA few more examples:\n\n# This will still list all 12 FASTQ files --\n# can be a good pattern to use to make sure you're not selecting other types of files \nls data/fastq/*fastq.gz\n\ndata/fastq/SRR7609467.fastq.gz\ndata/fastq/SRR7609468.fastq.gz\ndata/fastq/SRR7609469.fastq.gz\ndata/fastq/SRR7609470.fastq.gz\ndata/fastq/SRR7609471.fastq.gz\ndata/fastq/SRR7609472.fastq.gz\ndata/fastq/SRR7609473.fastq.gz\ndata/fastq/SRR7609474.fastq.gz\ndata/fastq/SRR7609475.fastq.gz\ndata/fastq/SRR7609476.fastq.gz\ndata/fastq/SRR7609477.fastq.gz\ndata/fastq/SRR7609478.fastq.gz\n\n\n\n# Only select the ...67.fastq.gz, ...68.fastq.gz, and ...69.fastq.gz files \nls data/fastq/SRR760946*fastq.gz\n\ndata/fastq/SRR7609467.fastq.gz\ndata/fastq/SRR7609468.fastq.gz\ndata/fastq/SRR7609469.fastq.gz\n\n\n\n\n\n\n\n\nWhat pattern would you use if you wanted to select all gzipped (.fastq.gz) and plain FASTQ files (.fastq) at the same time?\n\n\n\n\n\n\nls data/fastq/SRR760946*.fastq*\n\nThe second * will match filenames with nothing after .fastq as well as file names with characters after .fastq, such as .gz.\n\n\n\n\n\n\n3.3 Common uses of globbing\nWhat can we use this for, other than listing matching files? Below, we’ll use globbing to select files to loop over. Even more commonly, we can use this to move (mv), copy (cp), or remove (rm) multiple files at once. For example:\n\ncp data/fastq/SRR760946* .     # Copy 3 FASTQ files to your working dir \nls *fastq.gz                   # Check if they're here\n\nSRR7609467.fastq.gz\nSRR7609468.fastq.gz\nSRR7609469.fastq.gz\n\n\n\nrm *fastq.gz                  # Remove all FASTQ files in your working dir\nls *fastq.gz                  # Check if they're here\n\n\nls: cannot access ’*fastq.gz’: No such file or directory\n\nFinally, let’s use globbing to remove the mess of files we made when learning about variables:\n\nrm README_*\nrm Aug 18.txt\n\n\n\n\n\n\n\nDon’t confuse shell wildcards with regular expressions!\n\n\n\n\n\nFor those of you who know some regular expressions: these are conceptually similar to wildcards, but the * and ? symbols don’t have the same meaning, and there are way fewer shell wildcards than regular expression symbols.\nIn particular, note that . is not a shell wildcard and thus represents a literal period."
  },
  {
    "objectID": "modules/05_vars-loops.html#for-loops",
    "href": "modules/05_vars-loops.html#for-loops",
    "title": "Variables, Globbing, and Loops",
    "section": "4 For loops",
    "text": "4 For loops\nLoops are a universal element of programming languages, and are used to repeat operations, such as when you want to run the same script or command for multiple files.\nHere, we’ll only cover what is by far the most common type of loop: the for loop.\nfor loops iterate over a collection, such as a list of files: that is, they allow you to perform one or more actions for each element in the collection, one element at a time.\n\n4.1 for loop syntax and mechanics\nLet’s see a first example, where our “collection” is just a very short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nfor loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name\n\n\nin\nAfter in, we specify the collection we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\n\nA semicolon ; (as used before do) separates two commands on a single line\n\n\n\n\n\nA semicolon separates two commands written on a single line – for instance, instead of:\n\nmkdir results\ncd results\n\n…you could equivalently type:\n\nmkdir results; cd results\n\nThe ; in the for loop syntax has the same function, and as such, an alternative way to format a for loop is:\n\nfor a_number in 1 2 3\ndo\n    echo \"In this iteration of the loop, the number is $a_number\"\ndone\n\nBut that’s one line longer and a bit awkwardly asymmetric.\n\n\n\nThe aspect that is perhaps most difficult to understand is that in each iteration of the loop, one element in the collection (in the example above, either 1, 2, or 3) is being assigned to the variable specified after for (in the example above, a_number).\n\nIt is also important to realize that the loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\nThe following example, where we let the computer sleep for 1 second before printing the date and time with the date command, demonstrates that the loop is being executed sequentially:\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    sleep 1s          # Let the computer sleep for 1 second\n    date              # Print the date and time\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\nusage: sleep seconds\nWed Jul 19 12:15:54 EDT 2023\n--------\nIn this iteration of the loop, the number is 2\nusage: sleep seconds\nWed Jul 19 12:15:54 EDT 2023\n--------\nIn this iteration of the loop, the number is 3\nusage: sleep seconds\nWed Jul 19 12:15:54 EDT 2023\n--------\n\n\n\n\nOn Your Own: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nJust like we looped over 3 numbers above (1, 2, and 3), you want to loop over the three mushroom names, morel, destroying_angel, and eyelash_cup.\nNotice that when we specify the collection “manually”, like we did above with numbers, the elements are simply separated by a space.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\n\nmorel is an Ohio mushroom\ndestroying_angel is an Ohio mushroom\neyelash_cup is an Ohio mushroom\n\n\n\n\n\n\n\n\n4.2 Looping over files with globbing\nIn practice, we rarely manually list the collection of items we want to loop over. Instead, we commonly loop over files directly using globbing:\n\n# We make sure we only select gzipped FASTQ files using the `*fastq.gz` glob\nfor fastq_file in data/raw/*fastq.gz; do\n    echo \"File $fastq_file has $(wc -l &lt; $fastq_file) lines.\"\n    # More processing...\ndone\n\nThis technique is extremely useful, and I use it all the time. Take a moment to realize that we’re not doing a separate ls and storing the results: as mentioned, we can directly use a globbing pattern to select our files.\nIf needed, you can use your globbing / wild card skills to narrow down the file selection:\n\n# Perhaps we only want to select R1 files (forward reads): \nfor fastq_file in data/raw/*R1*fastq.gz; do\n    # Some file processing...\ndone\n\n# Or only filenames starting with A or B:\nfor fastq_file in data/raw/[AB]*fastq.gz; do\n    # Some file processing...\ndone\n\n\n\n\n\n\n\nAt-home reading: Alternatives to looping with a glob\n\n\n\n\n\nWith genomics data, the routine of looping over an entire directory of files, or selections made with simple globbing patterns, should serve you very well.\nBut in some cases, you may want to iterate only over a specific list of filenames (or partial filenames such as sample IDs) that represent a complex selection.\n\nIf this is a short list, you could directly specify it in the loop:\n\nfor sample in A1 B6 D3; do\n    R1=data/fastq/\"$sample\"_R1.fastq.gz\n    R2=data/fastq/\"$sample\"_R2.fastq.gz\n    # Some file processing...\ndone\n\nIf it is a longer list, you could create a simple text file with one line per sample ID / filename, and use command substitution as follows:\n\nfor fastq_file in $(cat file_of_filenames.txt); do\n    # Some file processing...\ndone\n\n\nIn cases like this, Bash arrays (basically, variables that consist of multiple values, like a vector in R) or while loops may provide more elegant solutions, but those are outside the scope of this introduction."
  },
  {
    "objectID": "modules/09_examples.html",
    "href": "modules/09_examples.html",
    "title": "Batch Jobs in Practice",
    "section": "",
    "text": "So far, we have covered all the building blocks to be able to run command-line programs at OSC:\nWith these skills, it’s relatively straightforward to create and submit scripts to run most command-line programs that can analyze our genomics data.\nOf course, how straightforward this exactly is depends on the ease of use of the programs you need to run, but that will be true in general whenever you learn a new approach and the associated software."
  },
  {
    "objectID": "modules/09_examples.html#setup",
    "href": "modules/09_examples.html#setup",
    "title": "Batch Jobs in Practice",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/&lt;your-folder&gt; in the box Working Directory (replace &lt;your-folder&gt; by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    =&gt; Terminal =&gt; New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =&gt;   File   =&gt;   Open Folder and enter /fs/ess/scratch/PAS2250/&lt;your-folder&gt;."
  },
  {
    "objectID": "modules/09_examples.html#worked-example-part-i-a-script-to-run-fastqc",
    "href": "modules/09_examples.html#worked-example-part-i-a-script-to-run-fastqc",
    "title": "Batch Jobs in Practice",
    "section": "2 Worked example, part I: A script to run FastQC",
    "text": "2 Worked example, part I: A script to run FastQC\n\n2.1 FastQC: A program for quality control of FASTQ files\nFastQC is perhaps the most ubiquitous genomics software. It produces visualizations and assessments of FASTQ files for statistics such as per-base quality (below) and adapter content. Running FastQC should, at least for Illumina data, almost always be the first analysis step after receiving your sequences.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser and which has about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\n\n\n\nA FastQC quality score graph for decent-quality reads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA FastQC quality score graph for poor-quality reads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 FastQC syntax\nTo analyze one optionally gzipped FASTQ file with FastQC, the syntax is simply:\n\nfastqc &lt;fastq-file&gt;\n\nOr if we wanted to specify the output directory (otherwise, output files end up in the current working directory):\n\nfastqc --outdir=&lt;output-dir&gt; &lt;fastq-file&gt;\n\nFor instance, if we wanted output files to go to the directory results/fastqc and wanted the program to analyze the file data/fastq/SRR7609467.fastq.gz, a functional command would like like this:\n\nfastqc --outdir=results/fastqc data/fastq/SRR7609467.fastq.gz\n\n\n\n\n\n\n\nFastQC’s output file names are automatically determined\n\n\n\nWe can specify the output directory, but not the actual file names, which will be automatically determined by FastQC based on the input file name.\nFor one FASTQ file, it will output one HTML file and one ZIP archive. The latter contains files with the summary statistics that were computed and on which the figures are based — we generally don’t need to look at that.\n\n\n\n\n\n2.3 A basic FastQC script\nHere is what a basic script to run FastQC could look like:\n\n#!/bin/bash\n\n## Bash strict settings\nset -euo pipefail\n\n## Copy the placeholder variables\ninput_file=\"$1\"\noutput_dir=\"$2\" \n\n## Run FastQC\nfastqc --outdir=\"$output_dir\" \"$input_file\"\n\nBut we’ll add a few things to to run this script smoothly as a batch job at OSC:\n\nWe load the relevant OSC module:\n\nmodule load fastqc/0.11.8\n\nWe add a few sbatch options:\n\n#SBATCH --account=PAS2250\n#SBATCH --output=slurm-fastqc-%j.out\n\n\nWe’ll also add a few echo statements to report what’s going on, and use a trick we haven’t yet seen — creating the output directory but only if it doesn’t yet exist:\n\nmkdir -p \"$output_dir\"\n\n\n\n\n\n\n\nThe -p option for mkdir\n\n\n\n\n\nUsing the -p option does two things at once for us, both of which are necessary for a foolproof inclusion of this command in a script:\n\nIt will enable mkdir to create multiple levels of directories at once: by default, mkdir errors out if the parent directory/directories of the specified directory don’t yet exist.\n\nmkdir newdir1/newdir2\n#&gt; mkdir: cannot create directory ‘newdir1/newdir2’: No such file or directory\n\n\nmkdir -p newdir1/newdir2    # This successfully creates both directories\n\nIf the directory already exists, it won’t do anything and won’t return an error (which would lead the script to abort at that point with our set settings).\n\nmkdir newdir1/newdir2\n#&gt; mkdir: cannot create directory ‘newdir1/newdir2’: File exists\n\n\nmkdir -p newdir1/newdir2   # This does nothing since the dirs already exist\n\n\n\n\n\nOur script now looks as follows:\n\n\n\n\n\n\nClick here to see the script\n\n\n\n\n\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n#SBATCH --output=slurm-fastqc-%j.out\n  \n## Bash strict settings\nset -euo pipefail\n\n## Load the software\nmodule load fastqc\n\n## Copy the placeholder variables\ninput_file=\"$1\"\noutput_dir=\"$2\" \n\n## Initial reporting\necho \"Starting FastQC script\"\ndate\necho \"Input FASTQ file:   $input_file\"\necho \"Output dir:         $output_dir\"\necho\n\n## Create the output dir if needed\nmkdir -p \"$output_dir\"\n\n## Run FastQC\nfastqc --outdir=\"$output_dir\" \"$input_file\"\n\n## Final reporting\necho\necho \"Listing output files:\"\nls -lh \"$output_dir\"\n\necho\necho \"Done with FastQC script\"\ndate\n\n\n\n\nNotice that this script is very similar to our toy scripts from yesterday and today: mostly standard (“boilerplate”) code with just a single command to run our program of interest.\nTherefore, you can adopt this script as a template for scripts that run other command-line programs, and will generally only need minor modifications!\n\n\n\n\n\n\nKeep your scripts simple – use one program in a script\n\n\n\n\n\nIn general, it is a good idea to keep your scripts simple and run one program per script.\nOnce you get the hang of it, it may seem appealing to string a number of programs together in a single script, so that it’s easier to rerun everything — but that will often end up leading to more difficulties than convenience.\nTo really tie your full set of analyses together in an actual workflow / pipeline, you will want to start using a workflow management system like Snakemake or NextFlow.\n\n\n\n\n\n\n2.4 Submitting our FastQC script as a batch job\nOpen a new file in VS Code (     =&gt;   File   =&gt;   New File) and save it as fastqc.sh within your scripts/ directory. Paste in the code above and save the file.\nThen, we submit the script:\n\nsbatch scripts/fastqc.sh data/fastq/SRR7609467.fastq.gz results/fastqc\n\n\nSubmitted batch job 12521308\n\n\n\n\n\n\n\nOnce again: Where does our output go?\n\n\n\n\n\n\nOutput that would have been printed to screen if we had run the script directly: in the Slurm log file slurm-fastqc-&lt;job-nr&gt;.out\nFastQC’s main output files (HTML ZIP): to the output directory we specified.\n\n\n\n\nLet’s take a look at the queue:\n\nsqueue -u $USER\n# Fri Aug 19 10:38:16 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n#           12521308 serial-40 fastqc.s   jelmer  PENDING       0:00   1:00:00      1 (None)\n\nOnce it’s no longer in the list produced by squeue, it’s done. Then, let’s check the Slurm log file1:\n\ncat slurm-fastqc-12521308.out    # You'll have a different number in the file name\n\n\n\n\n\n\n\nClick to see the contents of the Slurm log file\n\n\n\n\n\n\ncat misc/slurm-fastqc-12521308.out    # You'll have a different number in the file name\n\nStarting FastQC script\nFri Aug 19 10:39:52 EDT 2022\nInput FASTQ file:   data/fastq/SRR7609467.fastq.gz\nOutput dir:         results/fastqc\n\nStarted analysis of SRR7609467.fastq.gz\nApprox 5% complete for SRR7609467.fastq.gz\nApprox 10% complete for SRR7609467.fastq.gz\nApprox 15% complete for SRR7609467.fastq.gz\nApprox 20% complete for SRR7609467.fastq.gz\nApprox 25% complete for SRR7609467.fastq.gz\nApprox 30% complete for SRR7609467.fastq.gz\nApprox 35% complete for SRR7609467.fastq.gz\nApprox 40% complete for SRR7609467.fastq.gz\nApprox 45% complete for SRR7609467.fastq.gz\nApprox 50% complete for SRR7609467.fastq.gz\nApprox 55% complete for SRR7609467.fastq.gz\nApprox 60% complete for SRR7609467.fastq.gz\nApprox 65% complete for SRR7609467.fastq.gz\nApprox 70% complete for SRR7609467.fastq.gz\nApprox 75% complete for SRR7609467.fastq.gz\nApprox 80% complete for SRR7609467.fastq.gz\nApprox 85% complete for SRR7609467.fastq.gz\nApprox 90% complete for SRR7609467.fastq.gz\nApprox 95% complete for SRR7609467.fastq.gz\nAnalysis complete for SRR7609467.fastq.gz\n\nListing output files:\ntotal 16K\n-rw-r--r-- 1 jelmer PAS0471 224K Aug 19 10:39 SRR7609467_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 233K Aug 19 10:39 SRR7609467_fastqc.zip\n\nDone with FastQC script\nFri Aug 19 10:39:58 EDT 2022\n\n\n\n\n\nOur script already listed the output files, but let’s take a look at those too, and do so in the VS Code file browser in the side bar. To actually view FastQC’s HTML output file, we unfortunately need to download it with this older version of VS Code that’s installed at OSC — but the ability to download files from here is a nice one!"
  },
  {
    "objectID": "modules/09_examples.html#worked-example-part-ii-a-loop-in-a-workflow-script",
    "href": "modules/09_examples.html#worked-example-part-ii-a-loop-in-a-workflow-script",
    "title": "Batch Jobs in Practice",
    "section": "3 Worked example, part II: A loop in a workflow script",
    "text": "3 Worked example, part II: A loop in a workflow script\n\n3.1 A “workflow” file\nSo far, we’ve been typing our commands to run or submit scripts directly in the terminal. But it’s better to directly save these sorts of commands.\nTherefore, we will now create a new file for the purpose of documenting the steps that we are taking, and the scripts that we are submitting. You can think of this file as your analysis lab notebook2.\nIt’s easiest to also save this as a shell script (.sh) extension, even though it is not at all like the other scripts we’ve made, which are meant to be run/submitted in their entirety.\n\n\n\n\n\n\nNot like the other scripts\n\n\n\n\n\nOnce we’ve added multiple batch job steps, and the input of say step 2 depends on the output of step 1, we won’t be able to just run the script as is. This is because all the jobs would then be submitted at the same time, and step 2 would likely start running before step 1 is finished.\nThere are some possibilities with sbatch to make batch jobs wait on each other (e.g. the --dependency option), but this gets tricky quickly. As also mentioned above, if you want a fully automatically rerunnable workflow / pipeline, you should consider using a workflow management system like Snakemake or NextFlow.\n\n\n\nSo let’s go ahead and open a new text file, and save it as workflow.sh.\n\n\n\n3.2 Looping over all our files\nThe script that we wrote above will run FastQC for a single FASTQ file. Now, we will write a loop that iterates over all of our FASTQ files (only 8 in this case, but could be 100s just the same), and submits a batch job for each of them.\nLet’s type the following into our workflow.sh script, and then copy-and-paste it into the terminal to run the loop:\n\nfor fastq_file in data/fastq/*fastq.gz; do\n    sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n\n\nSubmitted batch job 2451089\nSubmitted batch job 2451090\nSubmitted batch job 2451091\nSubmitted batch job 2451092\nSubmitted batch job 2451093\nSubmitted batch job 2451094\nSubmitted batch job 2451095\nSubmitted batch job 2451096\n\n\n\nOn Your Own: Check if everything went well\n\nUse squeue to monitor your jobs.\nTake a look at the Slurm log files while the jobs are running and/or after the jobs are finished. A nice trick when you have many log files to check, is to use tail with a wildcard:\n\ntail slurm-fastqc*\n\nTake a look at the FastQC output files: are you seeing 12 HTML files?"
  },
  {
    "objectID": "modules/09_examples.html#adapting-our-scripting-workflow-to-run-other-command-line-programs",
    "href": "modules/09_examples.html#adapting-our-scripting-workflow-to-run-other-command-line-programs",
    "title": "Batch Jobs in Practice",
    "section": "4 Adapting our scripting workflow to run other command-line programs",
    "text": "4 Adapting our scripting workflow to run other command-line programs\n\nOn Your Own: Run another program\nUsing the techniques you’ve learned in this workshop, and especially, using our FastQC script as a template, try to run another command-line genomics program.\nWe below, we provide basically complete command-lines for three programs: MultiQC (summarizing FastQC output into one file), Trimmomatic (quality trimming and removing adapaters), and STAR (mapping files to a reference genome).\nYou can also try another program that you’ve been wanting to use.\n\n\n Commands to load/install and run other software:\n\n\n4.1 MultiQC\nMultiQC is a very useful program that can summarize QC and logging output from many other programs, such as FastQC, trimming software and read mapping software.\nThat means if you have sequenced 50 samples with paired-end reads, you don’t need to wade through 100 FASTQ HTML files to see if each is of decent quality — MultiQC will summarize all that output in nice, interactive figures in a single HTML file!\nHere, we’ll assume you want to run it on the FastQC output, which simply means using your FastQC output directory as the input directory for MultiQC.\n\nInstall\nMultiQC needs to be installed using an unusual 2-3 step procedure (one of the very few programs that can’t be installed in one go with conda):\n\nconda create -n multiqc python=3.7\nsource activate multiqc\nconda install -c bioconda -c conda-forge multiqc\n\n\n\n\n\n\n\nFailed to install? Using other people’s conda environments\n\n\n\n\n\nIf your MultiQC installation fails (this is a tricky one, with very many dependencies!), you can also use mine, by putting these line in your script:\n\nmodule load miniconda3\nsource activate /fs/project/PAS0471/jelmer/conda/multiqc-1.12\n\n\n\n\n\n\nRun\nYou would run MultiQC once for all files (no loop!) and with FastQC output as your input, as follows:\n\n# Copy the placeholder variables\ninput_dir=$1     # Directory where your FastQC output is stored\noutput_dir=$2    # Output dir of your choosing, e.g. result/multiqc\n\n# Activate the conda environment\nmodule load miniconda3\nsource activate multiqc\n\n# Run MultiQC\nmultiqc \"$input_dir\" -o \"$output_dir\"\n\n\n\n\n\n4.2 Trimmomatic\nTrimmomatic is a commonly-used program to both quality-trim FASTQ data and to remove adapters from the sequences.\n\nLoad the OSC module\n\nmodule load trimmomatic/0.38\n\n\n\nRun\nTo run Trimmomatic for one FASTQ file (=&gt; loop needed like with FastQC):\n\n# Load the module\nmodule load trimmomatic/0.38\n\n# Copy the placeholder variables\ninput_fastq=$1    # One of our \"raw\" FASTQ files in data/fastq\noutput_fastq=$2   # Output file directory and name to your choosing\n\n# We provide you with a file that has all common Illumina adapters:\nadapter_file=/fs/ess/scratch/PAS2250/jelmer/mcic-scripts/trim/adapters.fa\n\n# Run Trimmomatic\n# (note that the OSC module makes the environment variable $TRIMMOMATIC available)\njava -jar \"$TRIMMOMATIC\" SE \\\n  \"$input_fastq\" \"$output_fastq\" \\\n  ILLUMINACLIP:\"$adapter_file\":2:30:10 \\\n  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\n\n\n\n\n\n\nAvoid long lines with \\\n\n\n\nThe \\ in the Trimmomatic command above simply allow us to continue a single command one a new line, so we don’t get extremely long lines!\n\n\n\n\n\n\n4.3 STAR\n\n4.3.1 Load the OSC module\n\nmodule load gnu/10.3.0\nmodule load star/2.7.9a\n\n\n\n4.3.2 Index the genome\nFirst, we need to unzip the FASTA reference genome file:\n\ngunzip reference/Pvul.fa.gz\n\n\n#SBATCH --cpus-per-task=8\n\n# Load the module\nmodule load gnu/10.3.0\nmodule load star/2.7.9a\n\n# Copy the placeholder variables\nreference_fasta=$1       # Pvul.fa reference genome FASTA file\nindex_dir=$2             # Output dir with the genome index\n\n# Run STAR to index the reference genome\nSTAR --runMode genomeGenerate \\\n     --genomeDir \"$index_dir\" \\\n     --genomeFastaFiles \"$reference_fasta\" \\\n     --runThreadN \"$SLURM_CPUS_PER_TASK\"\n\n\n\n4.3.3 Map\n\n#SBATCH --cpus-per-task=8\n\n# Load the module\nmodule load star/2.7.9a\n\n# Copy the placeholder variables\nfastq_file=$1         # A FASTQ file to map to the reference genome\nindex_dir=$2          # Dir with the genome index (created in indexing script)\n\n# Extract a sample ID from the filename!\nsample_id=$(basename \"$fastq_file\" .fastq.gz)\n\n# Run STAR to map the FASTQ file\nSTAR \\\n  --runThreadN \"$SLURM_CPUS_PER_TASK\" \\\n  --genomeDir \"$index_dir\" \\\n  --readFilesIn $fastq_file \\\n  --readFilesCommand zcat \\\n  --outFileNamePrefix \"$outdir\"/\"$sample_id\" \\\n  --outSAMtype BAM Unsorted SortedByCoordinate\n\n\n\n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "modules/09_examples.html#footnotes",
    "href": "modules/09_examples.html#footnotes",
    "title": "Batch Jobs in Practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor longer running jobs, you may also want to keep an eye on the log file while it’s running.↩︎\nThough possibly a highly sanitized one – you may want to store daily notes and dead ends in a separate file.↩︎"
  },
  {
    "objectID": "modules/01_intro.html#workflow-overview",
    "href": "modules/01_intro.html#workflow-overview",
    "title": "RNAseq data analysis: introduction",
    "section": "Workflow overview",
    "text": "Workflow overview"
  },
  {
    "objectID": "modules/01_intro.html#rnaseq-data-analysis",
    "href": "modules/01_intro.html#rnaseq-data-analysis",
    "title": "RNAseq data analysis: introduction",
    "section": "RNAseq data analysis",
    "text": "RNAseq data analysis\nRNAseq data analysis can be divided into two main parts:\n\nA bioinformatics-heavy part in which you generate gene counts from the raw reads.\nA more statistical part in which you analyze the count table to create lists of differentially expressed genes and enriched functional categories.\n\n\n\nPart I: From reads to count table\nThis part starts with the raw reads from the (typically Illumina) sequencing machine to eventually generate a table with expression counts for each gene by each sample. This part:\n\nIs usually done by sequentially running a series of programs with a command-line interface (CLI). Therefore, you typically use the Unix shell (command line) and shell scripts to do this.\nProcesses large amounts of data, and is generally not suitable to be run on a laptop or a desktop computer: you should use a high-performance computing (HPC) center or cloud computing. (We will use the Ohio Supercomputer Center, OSC.)\nIs quite standardized and therefore, a “pipeline” written for one dataset can be run for another one with minor changes, even if the datasets are from completely different experiments or different species.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause of the required technical skills and computing infrastructure, in combination with the standardized execution, there are some alternatives available to doing this by yourself step-by-step1:\n\nCompanies and university bioinformatics core facilities may be able to simply run this part for you.\nServices with graphical user interfaces (GUIs) are available, such as Galaxy.\nThese run the same command-line programs, but wrap their execution in a more user-friendly way.\n\nSuch options are especially worth considering when you have no plans or ambitions to do much other genomics work in the future – in other words, it may not be worth learning all the required technical skills just for one project.\nWhen you plan to do multiple genomics projects and/or are generally interested in gaining computing skills, it’s better to go ahead and learn to run these command-line programs yourself.\n\n\n\n\n\nPart II: Analyzing the count table\nIn this part, you will analyze the table with gene counts for each sample, for example to test for differential expression among groups (e.g., different treatments) and to test whether certain functional (GO, KEGG) gene categories have distinct expression patterns as a whole.\nThis part:\n\nIs typically run entirely in R, using a number of specialized R “packages”.\nIs not particularly “compute-intensive”: your count table is a text file of typically only a few Mb, and the analyses you’re running do not need much time or computer memory. As such, you can run this on your laptop or desktop computer. (Though we will do it at OSC, mainly for the sake of continuity.)\nIs much less standardized across projects: the details of the analysis depend a lot on your experimental design and what you’re interested in; in addition, initial results may influence your next steps, and so on."
  },
  {
    "objectID": "modules/01_intro.html#what-well-cover",
    "href": "modules/01_intro.html#what-well-cover",
    "title": "RNAseq data analysis: introduction",
    "section": "What we’ll cover",
    "text": "What we’ll cover\n\nComputing skills\nMany of these computing skills are needed only for part I below.\n\nIntroduction to the Ohio Supercomputer Center (OSC)\nThe VS Code (Code Server) text editor / IDE\nIntroduction to the Unix shell (“command line” / “Bash”)\nShell scripts and loops\nThe SLURM compute job scheduler\nUsing and installing software at OSC\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlong the way, we’ll also learn about project organization and ensuring reproducibility.\nI won’t include a full-blown introduction to R, but will provide some learning resources for those of you with little R experience before we get to the relevant part of the RNAseq analysis.\n\n\n\n\n\nAnalysis part I: From sequence reads to gene counts\n\nGenomic file formats relevant to RNAseq: FASTA, FASTQ, BAM/SAM, GFF\n\nRaw read QC with FastQC and MultiQC\nRead pre-processing with TrimGalore and SortMeRNA\nRead alignment to a reference genome with STAR\nAlignment QC with (at least) MultiQC\nGene expression counting with Salmon\n\n\n\nAnalysis part II: Analyzing gene counts in R\n\nGetting an overview of sample/group distinctiveness with a PCA\nDifferential expression analysis with {DESeq2}\nKEGG and GO enrichment analysis with {ClusterProfiler}"
  },
  {
    "objectID": "modules/01_intro.html#data-type-and-workflow-variations",
    "href": "modules/01_intro.html#data-type-and-workflow-variations",
    "title": "RNAseq data analysis: introduction",
    "section": "Data type and workflow variations",
    "text": "Data type and workflow variations\n\nReference-based versus de novo workflows\nWe will cover a “reference-based” RNAseq workflow: one where your focal organism has a reference genome assembly and annotation. “De novo” RNAseq workflows are necessary when you don’t have a reference genome. They are overall similar, but more time-consuming and bioinformatics-heavy, since you will first have to assemble a transcriptome from the RNAseq data itself.\n\n\nGene-level versus transcript-level counts, and short versus long reads\nWe will focus on generating and analyzing gene-level counts rather than transcript-level counts: that is, for each sample, we will obtain a single count for each gene even if that gene has multiple transcripts (isoforms). However, the program which we’ll use for counting (Salmon) can also generate transcript-level counts, and downstream transcript-level analysis is fairly similar too, though this certainly adds a level of complexity.\nAdditionally, we will use short-read (Illumina) sequencing data, for which transcript-level counts have much greater levels of uncertainty, since most reads cannot directly be assigned to a specific transcript. Consider using long reads, such as PacBio IsoSeq, if you’re interested in transcript-level inferences.\n\n\n“Bulk” versus single-cell RNAseq\nWe will focus on “bulk” RNAseq, where RNA was extracted from a large mixture of cells and possibly cell types. Single-cell RNAseq analysis is similar for the first part (generating counts), but differs more in the second part (count analysis)."
  },
  {
    "objectID": "modules/01_intro.html#footnotes",
    "href": "modules/01_intro.html#footnotes",
    "title": "RNAseq data analysis: introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdditionally, you can run standardized pipelines yourself, which wrap many individual steps into a single executable workflow. This especially becomes a time-efficient option once you know the computing basics, and also aids with reproducibility and following best-practices. For example, for RNAseq there is a Nextflow nf-core RNAseq pipeline. The steps we will run fill follow this pipeline closely – but in my opinion, for initial learning, it is better to go step-by-step without a formalized pipeline.↩︎"
  },
  {
    "objectID": "info/resources.html#general-applied-bioinformatics-resources",
    "href": "info/resources.html#general-applied-bioinformatics-resources",
    "title": "Further Resources",
    "section": "General applied bioinformatics resources",
    "text": "General applied bioinformatics resources\n\nBuffalo Book\nPractical Computing for Biologists books"
  },
  {
    "objectID": "info/resources.html#rnaseq-analysis",
    "href": "info/resources.html#rnaseq-analysis",
    "title": "Further Resources",
    "section": "RNAseq analysis",
    "text": "RNAseq analysis\n\nTranscript-level analysis\nTBA"
  },
  {
    "objectID": "info/glossary.html",
    "href": "info/glossary.html",
    "title": "Glossary / Abbreviations",
    "section": "",
    "text": "Under construction\n\n\n\nThis page is still under construction."
  },
  {
    "objectID": "info/glossary.html#computing",
    "href": "info/glossary.html#computing",
    "title": "Glossary / Abbreviations",
    "section": "Computing",
    "text": "Computing\n\nOSC\nHPC\nCPU\nGPU\nCLI\nGUI\nShell\nUnix\nLinux"
  },
  {
    "objectID": "info/glossary.html#genomics",
    "href": "info/glossary.html#genomics",
    "title": "Glossary / Abbreviations",
    "section": "Genomics",
    "text": "Genomics\n\nFASTA\nFASTQ\nBAM/SAM\nGFF"
  },
  {
    "objectID": "info/osc_transfer.html#introduction",
    "href": "info/osc_transfer.html#introduction",
    "title": "Transferring files to and from OSC",
    "section": "1 Introduction",
    "text": "1 Introduction\nThere are several ways of transferring files between your computer and OSC:\n\n\n\n\n\n\n\n\n\n\nMethod\nTransfer size\nCLI or GUI\nEase of use\nFlexibility/options\n\n\n\n\nOnDemand Files menu\nsmaller (&lt;1GB)\nGUI\nEasy\nLimited\n\n\nRemote transfer commands\nsmaller (&lt;1GB)\nCLI\nModerate\nExtensive\n\n\nSFTP\nlarger (&gt;1GB)\nEither\nModerate\nLimited\n\n\nGlobus\nlarger (&gt;1GB)\nGUI\nModerate 1\nExtensive\n\n\n\nThis page will cover each of those in more detail below.\n\n\n\n\n\n\nDownload directly from the web using commands at OSC\n\n\n\nIf you need files that are at a publicly accessible location on the internet (for example, NCBI reference genome data), you don’t need to download these to your computer and then upload them to OSC.\nInstead, you can use commands for downloading files directly to OSC, like wget or curl. This will be covered in one of the main sessions."
  },
  {
    "objectID": "info/osc_transfer.html#ondemand-files-menu",
    "href": "info/osc_transfer.html#ondemand-files-menu",
    "title": "Transferring files to and from OSC",
    "section": "2 OnDemand Files menu",
    "text": "2 OnDemand Files menu\nFor small transfers (below roughly 1 GB), you might find it easiest to use the Upload and Download buttons in the OSC OnDemand “Files” menu — their usage should be pretty intuitive."
  },
  {
    "objectID": "info/osc_transfer.html#remote-transfer-commands",
    "href": "info/osc_transfer.html#remote-transfer-commands",
    "title": "Transferring files to and from OSC",
    "section": "3 Remote transfer commands",
    "text": "3 Remote transfer commands\nFor small transfers, you can also use a remote transfer command like scp, or a more advanced one like rsync or rclone. Such commands can provide a more convenient transfer method than OnDemand if you want to keep certain directories synced between OSC and your computer.\nThe reason you shouldn’t use this for very large transfers is that the transfer will happen using a login node.\n\n3.1 scp\nOne option is scp (secure copy), which works much like the regular cp command, including that you’ll need -r for recursive transfers.\nThe key difference is that we have to somehow refer to a path on a remote computer, and we do so by starting with the remote computer’s address, followed by :, and then the path:\n# Copy from remote (OSC) to local (your computer):\nscp &lt;user&gt;@pitzer.osc.edu:&lt;remote-path&gt; &lt;local-path&gt;\n\n# Copy from local (your computer) to remote (OSC)\nscp &lt;local-path&gt; &lt;user&gt;@pitzer.osc.edu:&lt;remote-path&gt;\nHere are two examples of copying from OSC to your local computer:\n# Copy a file from OSC to a local computer - namely, to your current working dir ('.'):\nscp jelmer@pitzer.osc.edu:/fs/ess/PAS0471/jelmer/mcic-scripts/misc/fastqc.sh .\n\n# Copy a directory from OSC to a local computer - namely, to your home dir ('~'):\nscp -r jelmer@pitzer.osc.edu:/fs/ess/PAS0471/jelmer/mcic-scripts ~\nAnd two examples of copying from your local computer to OSC:\n# Copy a file from your computer to OSC --\n# namely, a file in from your current working dir to your home dir at OSC:\nscp fastqc.sh jelmer@pitzer.osc.edu:~\n\n# Copy a file from my local computer's Desktop to the Scratch dir for PAS0471:\nscp /Users/poelstra.1/Desktop/fastqc.sh jelmer@pitzer.osc.edu:/fs/scratch/PAS0471\nSome nuances for remote copying:\n\nAs the above code implies, in both cases (remote-to-local and local-to-remote), you will issue the copying commands from your local computer.\nFor the remote computer (OSC), the path should always be absolute, whereas that for your local computer can be either relative or absolute.\nSince all files can be accessed at the same paths at Pitzer and at Owens, it doesn’t matter whether you use @pitzer.osc.edu or @owens.osc.edu in the scp command.\n\n\n\n\n\n\n\nTransferring directly to and from OneDrive\n\n\n\nIf your OneDrive is mounted on or synced to your local computer (i.e., if you can see it in your computer’s file brower), you can also transfer directly between OSC and OneDrive.\nFor example, the path to my OneDrive files on my laptop is:\n/Users/poelstra.1/Library/CloudStorage/OneDrive-TheOhioStateUniversity.\nSo if I had a file called fastqc.sh in my top-level OneDrive dir, I could transfer it to my Home dir at OSC as follows:\nscp /Users/poelstra.1/Library/CloudStorage/OneDrive-TheOhioStateUniversity jelmer@pitzer.osc.edu:~\n\n\n\n\n\n3.2 rsync\nAnother option, which I can recommend, is the rsync command, especially when you have directories that you repeatedly want to sync: rsync won’t copy any files that are identical in source and destination.\nA useful combination of options is -avz --progress:\n\n-a enables archival mode (among other things, this makes it work recursively).\n-v increases verbosity — tells you what is being copied.\n-z enables compressed file transfer (=&gt; generally faster).\n--progress to show transfer progress for individual files.\n\nThe way to refer to remote paths is the same as with scp. For example, I could copy a dir_with_results in my local Home dir to my OSC Home dir as follows:\nrsync -avz --progress ~/dir_with_results jelmer@owens.osc.edu:~\n\n\n\n\n\n\nTrailing slashes in rsync\n\n\n\nOne tricky aspect of using rsync is that the presence/absence of a trailing slash for source directories makes a difference for its behavior. The following commands work as intended — to create a backup copy of a scripts dir inside a dir called backup2:\n# With trailing slash: copy the *contents* of source \"scripts\" into target \"scripts\":\nrsync -avz scripts/ backup/scripts\n\n# Without trailing slash: copy the source dir \"scripts\" into target dir \"backup\"\nrsync -avz scripts backup\nBut these commands don’t:\n# This would result in a dir 'backup/scripts/scripts':\nrsync -avz scripts backup/scripts\n\n# This would copy the files in \"scripts\" straight into \"backup\":\nrsync -avz scripts/ backup"
  },
  {
    "objectID": "info/osc_transfer.html#sftp",
    "href": "info/osc_transfer.html#sftp",
    "title": "Transferring files to and from OSC",
    "section": "4 SFTP",
    "text": "4 SFTP\nThe first of two options for larger transfers is SFTP. You can use the sftp command when you have access to a Unix shell on your computer, and this what I’ll cover below.\n\n\n\n\n\n\nSFTP with a GUI\n\n\n\nIf you have Windows without e.g. WSL or Git Bash (see the top of the SSH page on this site for more details), you can use a GUI-based SFTP client instead like WinSCP, Cyberduck, or FileZilla. CyberDuck also works on Mac, and FileZilla works on all operating systems, if you prefer to do SFTP transfers with a GUI, but I won’t cover their usage here.\n\n\n\n4.1 Logging in\nTo log in to OSC’s SFTP server, issue the following command in your local computer’s terminal, substituting &lt;user&gt; by your OSC username:\nsftp &lt;user&gt;@sftp.osc.edu   # E.g., 'jelmer@sftp.osc.edu'\nThe authenticity of host 'sftp.osc.edu (192.148.247.136)' can't be established.\nED25519 key fingerprint is SHA256:kMeb1PVZ1XVDEe2QiSumbM33w0SkvBJ4xeD18a/L0eQ.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nIf this is your first time connecting to OSC SFTP server, you’ll get a message like the one shown above: you should type yes to confirm.\nThen, you may be asked for your OSC password, and after that, you should see a “welcome” message like this:\n******************************************************************************\n\nThis system is for the use of authorized users only.  Individuals using\nthis computer system without authority, or in excess of their authority,\nare subject to having all of their activities on this system monitored\nand recorded by system personnel.  In the course of monitoring individuals\nimproperly using this system, or in the course of system maintenance,\nthe activities of authorized users may also be monitored.  Anyone using\nthis system expressly consents to such monitoring and is advised that if\nsuch monitoring reveals possible evidence of criminal activity, system\npersonnel may provide the evidence of such monitoring to law enforcement\nofficials.\n\n******************************************************************************\nConnected to sftp.osc.edu.\nNow, you will have an sftp prompt (sftp&gt;) instead of a regular shell prompt.\nFamiliar commands like ls, cd, and pwd will operate on the remote computer (OSC, in this case), and there are local counterparts for them: lls, lcd, lpwd — for example:\n# NOTE: I am prefacing sftp commands with the 'sftp&gt;' prompt to make it explicit\n#       these should be issued in an sftp session; but don't type that part.\nsftp&gt; pwd\nRemote working directory: /users/PAS0471/jelmer\nsftp&gt; lpwd\nLocal working directory: /Users/poelstra.1/Desktop\n\n\n4.2 Uploading files to OSC\nTo upload files to OSC, use sftp’s put command.\nThe syntax is put &lt;local-path&gt; &lt;remote-path&gt;, and unlike with scp etc., you don’t need to include the address to the remote (because in an stfp session, you are simultaneously connected to both computers). But like with cp and scp, you’ll need the -r flag for recursive transfers, i.e. transferring a directory and its contents.\n# Upload fastqc.sh in a dir 'scripts' on your local computer to the PAS0471 Scratch dir:\nsftp&gt; put scripts/fastqc.sh /fs/scratch/PAS0471/sandbox\n\n# Use -r to transfer directories:\nsftp&gt; put -r scripts /fs/scratch/PAS0471/sandbox\n\n# You can use wildcards to upload multiple files:\nsftp&gt; put scripts/*sh /fs/scratch/PAS0471/sandbox\n\n\n\n\n\n\nsftp is primitive\n\n\n\nThe ~ shortcut to your Home directory does not work in sftp!\nsftp is generally quite primitive and you also cannot use, for example, tab completion or the recalling of previous commands with the up arrow.\n\n\n\n\n4.3 Downloading files from OSC\nTo download files from OSC, use the get command, which has the syntax get &lt;remote-path&gt; &lt;local-path&gt; (this is the other way around from put in that the remote path comes first, but the same in that both use the order &lt;source&gt; &lt;target&gt;, like cp and so on).\nFor example:\nsftp&gt; get /fs/scratch/PAS0471/mcic-scripts/misc/fastqc.sh .\n\nsftp&gt; get -r /fs/scratch/PAS0471/sandbox/ .\n\n\n4.4 Exiting\nWhen you’re done, you can type exit or press Ctrl+D to exit the sftp prompt."
  },
  {
    "objectID": "info/osc_transfer.html#globus",
    "href": "info/osc_transfer.html#globus",
    "title": "Transferring files to and from OSC",
    "section": "5 Globus",
    "text": "5 Globus\nThe second option for large transfers is Globus, which has a browser-based GUI, and is especially your best bet for very large transfers. Some advantages of using Globus are that:\n\nIt checks whether all files were transferred correctly and completely\nIt can pause and resume automatically when you e.g. turn off your computer for a while\nIt can be used to share files from OSC directly with collaborators even at different institutions.\n\nGlobus does need some setup, including the installation of a piece of software that will run in the background on your computer.\n\nGlobus installation and configuration instructions: Windows / Mac / Linux\nGlobus transfer instructions\nOSC’s page on Globus"
  },
  {
    "objectID": "info/osc_transfer.html#footnotes",
    "href": "info/osc_transfer.html#footnotes",
    "title": "Transferring files to and from OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut the initial setup for Globus is quite involved and a bit counterintuitive.↩︎\nFor simplicity, these commands are copying between local dirs, which is also possible with rsync.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RNAseq data analysis using the command line and R",
    "section": "",
    "text": "Sessions\n\n\n\nSession nr\nDate\nTopic & link\nPart\n\n\n\n\n1\n2023-07-14\nIntroduction\n1. Computing Skills\n\n\n2\n2023-07-21\nThe Ohio Supercomputer Center (OSC)\n1. Computing Skills\n\n\n3\n2023-07-28\nVS Code & Unix shell I\n1. Computing Skills\n\n\n4\n2023-08-04\nUnix shell II\n1. Computing Skills\n\n\n5\n2023-08-11\nShell scripts\n1. Computing Skills\n\n\n6\n2023-08-18\nShell scripts (cont.)\n1. Computing Skills\n\n\n7\n2023-08-25\nSoftware at OSC\n1. Computing Skills\n\n\n8\n2023-09-01\nSlurm jobs\n1. Computing Skills"
  },
  {
    "objectID": "info/osc_ssh.html#introduction",
    "href": "info/osc_ssh.html#introduction",
    "title": "Connecting to OSC through SSH",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis page will first go through the basics of connecting to OSC with SSH, using the ssh command.\nIf you use ssh frequently, or plan to do so, the next two sections will show you two useful tricks:\n\nAvoid being prompted for your OSC password\nSet up a shortcut for your SSH connection name.\n\nFinally, something more specific but incredibly useful if you like VS Code, and have it installed locally, is to SSH-tunnel VS Code to OSC.\n\n\n\n\n\n\nSSH and Unix shells on Windows\n\n\n\n\n\nIf you have a Windows computer, you can instead use a GUI-based SSH client like PuTTY.\nAlternatively, for an experience more similar to that if you did have a Unix-based operating system, you can install either of the following, which will enable you to get a terminal program that does run a Unix shell:\n\nWindows Subsystem for Linux (WSL) — the more involved option, this will basically run a Linux Virtual Machine on your computer.\nGit for Windows, which comes with a Unix (Bash) shell — “Git Bash”. To install this, download it from this page and install it using all the default settings for the installation, except:\n\nIn “Adjusting Your PATH Environment”, select “Use Git from Git Bash Only”.\nIn the prompt “Configuring the Line Ending Conversions”, choose “Checkout as-is, commit as-is”."
  },
  {
    "objectID": "info/osc_ssh.html#basic-ssh-connection-in-a-terminal",
    "href": "info/osc_ssh.html#basic-ssh-connection-in-a-terminal",
    "title": "Connecting to OSC through SSH",
    "section": "2 Basic SSH connection in a terminal",
    "text": "2 Basic SSH connection in a terminal\nTo connect to OSC or other remote computers without using a web portal like OnDemand, you can use SSH. You can do so via the ssh command if you have a Linux or a Mac computer, since these two operating systems are both Unix-based and have built-in terminals with Unix shells.\nHere, I’ll briefly demonstrate how to use the ssh command. On your own computer, open a terminal application and input the command ssh &lt;user&gt;@&lt;host&gt;, where:\n\n&lt;user&gt; should be replaced by your OSC username, and\n&lt;host&gt; should be replaced by the name of the computer you want to connect to:\n\npitzer.osc.edu to connect to the Pitzer cluster\nowens.osc.edu to connect to the Owens cluster\n\n\nFor example, if I (username jelmer) wanted to log in to the Pitzer cluster, I would use:\nssh jelmer@pitzer.osc.edu\nThe authenticity of host 'pitzer.osc.edu' can't be established.\nRSA key fingerprint is 2a:b6:f6:8d:9d:c2:f8:2b:8c:c5:03:06:a0:f8:59:12.\nAre you sure you want to continue connecting (yes/no)?\nIf this is the first time you are connecting to Pitzer via SSH, you’ll encounter a message similar to the one above. While the phrase “The authenticity of host ‘pitzer.osc.edu’ can’t be established.” sounds ominous, you will always get this warning when you attempt to connect to a remote computer for the first time, and you should type yes to proceed (you then won’t see this message again).\nYou should now be prompted for your password. Type it in carefully because no characters or even *s will appear on the screen, and then press Enter.\njelmer@pitzer.osc.edu's password:\nIf you entered your password correctly, your shell is now connected to OSC rather than operating on your own computer. That is, you’ll have shell access very much in the same way as when using the “Pitzer Shell Access” button on OSC OnDemand. (The key difference between SSH-ing in this way rather than using OnDemand is that the terminal is not running inside your browser, which can be convenient.)\n\n\n\n\n\n\nSSH shortcuts\n\n\n\nIf you use SSH a lot to connect to OSC, typing ssh &lt;username&gt;@pitzer.osc.edu every time and then providing your password can get pretty tedious. The next two sections will show you how to make this go faster."
  },
  {
    "objectID": "info/osc_ssh.html#avoid-being-prompted-for-your-osc-password",
    "href": "info/osc_ssh.html#avoid-being-prompted-for-your-osc-password",
    "title": "Connecting to OSC through SSH",
    "section": "3 Avoid being prompted for your OSC password",
    "text": "3 Avoid being prompted for your OSC password\nIf you take the following steps, you will no longer be promoted for your OSC password every time you log in to OSC using SSH.\nBoth steps should be done in a terminal on your local machine:\n\nGenerate a public-private SSH key-pair:\nssh-keygen -t rsa\nYou’ll get some output and will then be asked several questions, but in each case, you can just press Enter to select the default answer.\nTransfer the public key to OSC’s clusters:\n# Replace &lt;user&gt; by your username, e.g. \"ssh-copy-id jelmer@owens.osc.edu\"\nssh-copy-id &lt;user&gt;@owens.osc.edu\nssh-copy-id &lt;user&gt;@pitzer.osc.edu\n\nTest if it works by runnning:\n# Try connecting to Owens (once again, replace '&lt;user&gt;' by your username):\nssh &lt;user&gt;@owens.osc.edu\n\n# Try connecting to Pitzer (once again, replace '&lt;user&gt;' by your username):\nssh &lt;user&gt;@owens.osc.edu\n\n\n\n\n\n\nMore instructions\n\n\n\nSee also this Tecmint post in case you’re struggling."
  },
  {
    "objectID": "info/osc_ssh.html#use-a-shorter-name-for-your-ssh-connection",
    "href": "info/osc_ssh.html#use-a-shorter-name-for-your-ssh-connection",
    "title": "Connecting to OSC through SSH",
    "section": "4 Use a shorter name for your SSH connection",
    "text": "4 Use a shorter name for your SSH connection\nYou can easily set up alternative ways of referring to you SSH connection (i.e., “aliases”), such as shortening jelmer@pitzer.osc.edu to jp, by saving these aliases in a text file ~/.ssh/config, as shown below.\nThese two steps should both be done on your local machine:\n\nCreate a file called ~/.ssh/config:\ntouch ~/.ssh/config\nOpen the file in a text editor and add your alias(es) in the following format:\nHost &lt;arbitrary-alias-name&gt;    \n    HostName &lt;remote-address&gt;\n    User &lt;username&gt;\nFor instance, my file contains the following so as to connect to Pizer with jp and to Owens with jo:\nHost jp\n    HostName pitzer.osc.edu\n    User jelmer\n\nHost jo\n    HostName owens.osc.edu\n    User jelmer\n\n\nNow, you just need to use your, preferably very short, alias to log in — and if you did the previous no-password setup, you won’t even be prompted for your password!\nssh jp\nPerhaps even more conveniently, these shortcuts will also work with scp and rsync! For example:\nrsync ~/scripts op:/fs/scratch/PAS0471"
  },
  {
    "objectID": "info/osc_ssh.html#set-up-your-local-vs-code-to-ssh-tunnel-into-osc",
    "href": "info/osc_ssh.html#set-up-your-local-vs-code-to-ssh-tunnel-into-osc",
    "title": "Connecting to OSC through SSH",
    "section": "5 Set up your local VS Code to SSH tunnel into OSC",
    "text": "5 Set up your local VS Code to SSH tunnel into OSC\nIf you want to use VS Code to write code, have a shell, and interact with files at OSC directly, you don’t necessarily need to use the VS Code (Code Server) in OSC OnDemand. You can also make your local VS Code installation “SSH tunnel” into OSC.\nThis is a more convenient way of working because it’s quicker to start, will never run out of alotted time, and because you are not working inside a browser, you have more screen space and no keyboard shortcut interferences.\nThe set-up is pretty simple (see also these instructions if you get stuck), and should also work on Windows:\n\nIf necessary, install VS Code (instructions for Windows / Mac / Linux) on your computer, and open it.\nInstall the VS Code “Remote Development extension pack”: open the Extensions side bar (click the icon with the four squared in the far left), search for “Remote Development extension pack”, and click “Install”.\nOpen the Command Palette (F1 or Ctrl+ShiftP) and start typing “Remote SSH”.\nThen, select Remote-SSH: Connect to Host… and specify your SSH connection: e.g. ssh jelmer@pitzer.osc.edu (you’ll have to do this separately for Pitzer and Owens if you want to be able to connect to both).\nIf you did the no-password setup described above (recommended!), you shouldn’t be prompted for a password and VS Code will connect to OSC!\nIf you’re asked about the operating system of the host, select Linux, which is the operating system of the OSC clusters.\n\n\n\n\n\n\n\nWarning\n\n\n\nJust be aware that you’ll now be on a Login node (and not on a Compute node like when you use VS Code through OnDemand), so avoid running analyses directly in the terminal, and so on."
  },
  {
    "objectID": "info/about.html",
    "href": "info/about.html",
    "title": "About",
    "section": "",
    "text": "This website contains the material for a series of teaching sessions for grad students and postdocs of the Cruz-Monserrate lab in the fall of 2023.\nThese sessions focus on hands-on analysis of (short-read, bulk) RNAseq data with command-line tools and R using the computing resources at the Ohio Supercomputer Center (OSC).\nThis website and the teaching materials have been created by Jelmer Poelstra at the Molecular and Cellular Imaging Center (MCIC) of Ohio State University."
  },
  {
    "objectID": "modules/08_slurm.html",
    "href": "modules/08_slurm.html",
    "title": "Compute Jobs with Slurm",
    "section": "",
    "text": "We have so far been working on login nodes at OSC, but in order to run some actual analyses, you will need access to compute nodes.\nAutomated scheduling software allows hundreds of people with different requirements to access compute nodes effectively and fairly. For this purpose, OSC uses the Slurm scheduler (Simple linux utility for resource management).\nA temporary reservation of resources on compute nodes is called a compute job. What are the options to start a compute job at OSC?\nWhen running command-line programs for genomics analyses, batch jobs are the most useful and will be the focus of this module. We’ll also touch on interactive shell jobs, which can occasionally be handy and are requested and managed in a very similar way to batch jobs."
  },
  {
    "objectID": "modules/08_slurm.html#setup",
    "href": "modules/08_slurm.html#setup",
    "title": "Compute Jobs with Slurm",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/&lt;your-folder&gt; in the box Working Directory (replace &lt;your-folder&gt; by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    =&gt; Terminal =&gt; New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =&gt;   File   =&gt;   Open Folder and enter /fs/ess/scratch/PAS2250/&lt;your-folder&gt;."
  },
  {
    "objectID": "modules/08_slurm.html#interactive-shell-jobs",
    "href": "modules/08_slurm.html#interactive-shell-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "2 Interactive shell jobs",
    "text": "2 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. Working in an interactive shell job is operationally identical to working on a login node as we’ve been doing so far, but the difference is that it’s now okay to use significant computing resources. (How much and for how long depends on what you reserve.)\n\n2.1 Using srun\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command1, which we can use with --pty /bin/bash added to get an interactive Bash shell.\nHowever, if we run that command without additional options, we get an error:\n\nsrun --pty /bin/bash\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs the error message Must specify account for job tries to tell us, we need to indicate which OSC project (or as SLURM puts it, “account”) we want to use for this compute job. This is because an OSC project always has to be charged for the computing resources used during a compute job.\nTo specify the project/account, we can use the --account= option followed by the project number:\n\nsrun --account=PAS2250 --pty /bin/bash\n\n\nsrun: job 12431932 queued and waiting for resources\nsrun: job 12431932 has been allocated resources\n[…regular login info, such as quota, not shown…]\n[jelmer@p0133 PAS2250]$\n\nThere we go! First some Slurm scheduling info was printed to screen:\n\nInitially, the job is “queued”: that is, waiting to start.\nVery soon (usually!), the job is “allocated resources”: that is, computing resources such as a compute node are reserved for the job.\n\nThen:\n\nThe job starts and because we’ve reserved an interactive shell job, a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nMost importantly, we are no longer on a login node but on a compute node, as our prompt hints at: we switched from something like [jelmer@pitzer-login04 PAS2250]$ to the [jelmer@p0133 PAS2250]$ shown above.\nNote also that the job has a number (above: job 12431932): every compute job has such a unique identifier among all jobs by all users at OSC, and we can use this number to monitor and manage it. All of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nThe working directory stays the same\n\n\n\nBatch jobs start in the directory that they were submitted from: that is, your working directory remains the same.\n\n\n\n\n\n2.2 Compute job options\nThe --account= option is just one of out of many options we can use when reserving a compute job, but is the only one that always has to be specified (including for batch jobs and for Interactive Apps).\nDefaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1). These options are all specified in the same way for interactive and batch jobs, and we’ll dive into them below.\n\n\n\n\n\n\nQueueing times\n\n\n\nThe “bigger” (more time, more cores, more memory) our job is, the more likely it is that our job will be pending for an appreciable amount of time.\nSmaller jobs (requesting up to a few hours and cores) will almost always start running nearly instantly. Even big jobs (requesting a day or more, 10 or more cores) will often do so, but during busy times, you might have to wait for a while. That said, the only times I’ve had to wait for more than an hour or so was when I was requesting jobs with very large memory requirements (100s of GBs), which have to be submitted to a separate queue/“partition”."
  },
  {
    "objectID": "modules/08_slurm.html#intro-to-batch-jobs",
    "href": "modules/08_slurm.html#intro-to-batch-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "3 Intro to batch jobs",
    "text": "3 Intro to batch jobs\nWhen requesting batch jobs, we are asking the Slurm scheduler to run a script on a compute node.\nIn contrast to interactive shell jobs, we stay in our current shell when submitting a script, and the script will run on a compute node “out of sight”. Also, as we’ll discuss in more detail below:\n\nOutput from the script that would normally be printed to screen ends up in a file (!).\nDespite not being on the same node as our job, we can do things like monitoring whether the job is already/still running, and cancelling the job.\n\n\n\n\n\n\n\nScripts in other languages\n\n\n\nThe script that we submit can be in different languages but typically, including in all examples in this workshop, they are shell (Bash) scripts.\n\n\n\n\n3.1 The sbatch command\nWhereas we used Slurm’s srun command to start an interactive shell job, we use its sbatch command to submit a batch job. Recall from the Bash scripting module that we can run a Bash script as follows:\n\nbash scripts/printname.sh Jane Doe\n\nFirst name: Jane\nLast name: Doe\n\n\n\n\n\n\n\n\nCan’t find yesterday’s printname.sh script?\n\n\n\n\n\n\nOpen a new file in the VS Code editor (     =&gt;   File   =&gt;   New File) and save it as printname.sh\nCopy the code below into the script:\n\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\n\nThe above command ran the script on our current node, a login node. To instead submit the script to the Slurm queue, we would start by simply replacing bash by sbatch:\n\nsbatch scripts/printname.sh Jane Doe\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs we’ve learned, we always have to specify the OSC account when submitting a compute job. Conveniently, we can also specify Slurm/sbatch options inside our script, but first, let’s add the --account option on the command line:\n\nsbatch --account=PAS2250 scripts/printname.sh Jane Doe\n\n\nSubmitted batch job 12431935\n\n\n\n\n\n\n\nsbatch options and script arguments\n\n\n\n\n\nNote that we can use sbatch options and script arguments in one command, in the following order:\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\nBut both of these are optional:\n\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2250 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2250 printname.sh Jane Doe  # Both sbatch option and script arguments\n\n\n\n\n\n\n\n3.2 Adding sbatch options in scripts\nInstead of specifying Slurm/sbatch options on the command-line when we submit the script, we can also add these options inside the script.\nThis is handy because even though we have so far only seen the account= option, you often want to specify several options. That would lead to very long sbatch commands. Additionally, it can be practical to store a script’s typical Slurm options along with the script itself.\nWe add the options in the script using another type of special comment line akin to the shebang line, marked by #SBATCH. The equivalent of adding --account=PAS2250 after sbatch on the command line is a line in a script that reads #SBATCH --account=PAS2250.\nJust like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one such line to the printname.sh script, such that the first few lines read:\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n\nset -ueo pipefail\n\nAfter having added this to the script, we can run our earlier sbatch command without options:\n\nsbatch printname.sh Jane Doe\n\n\nSubmitted batch job 12431942\n\nAfter we submit the batch job, we immediately get our prompt back. Everything else (job queuing and running) will happen out of our immediate view. This allows us to submit many jobs at the same time — we don’t have to wait for other jobs to finish (or even to start).\n\n\n\n\n\n\nsbatch option precedence\n\n\n\nAny sbatch option provided on the command line will override the equivalent option provided inside the script. This is sensible: we can provide “defaults” inside the script, and change one or more of those when needed on the command line.\n\n\n\n\n\n\n\n\nRunning a script with #SBATCH in other contexts\n\n\n\nBecause #SBATCH lines are special comment lines, they will simply be ignored and not throw any errors when you run a script that contains them in other contexts: when not running them as a batch job at OSC, or even when running them on a computer without Slurm installed.\n\n\n\n\n\n3.3 Where does the output go?\nAbove, we saw that when we ran the printname.sh script directly, its output was printed to the screen, whereas when we submitted it as a batch job, all that was sprinted to screen was Submitted batch job 12431942. So where did our output go?\nOur output ended up in a file called slurm-12431942.out: that is, slurm-&lt;job-number&gt;.out. Since each job number is unique to a given job, your file would have a different number in its name. We might call this type of file a Slurm log file.\n\n\n\n\n\n\nAny idea why we might not want batch job output printed to screen, even if we could?\n\n\n\n\n\nThe power of submitting batch jobs is that you can submit many at once — e.g. one per sample, running the same script. If the output from all those scripts ends up on your screen, things become a big mess, and you have no lasting record of what happened.\n\n\n\nLet’s take a look at the contents of the Slurm log file with the cat command:\n\ncat slurm-12431942.out\n\n\nFirst name: Jane\nLast name: Doe\n\nThis file simply contains the output that we saw printed to screen before — nothing more and nothing less.\nIt’s important to conceptually distinguish two broad types of output that a script may have:\n\nOutput that is printed to screen when we directly run a script, such as what was produced by our echo statements, by any errors that may occur, and possibly by a program that we run in the script.2 As we saw, this output ends up in the Slurm log file when we submit the script as a batch job.\nOutput that we redirect to a file (&gt; myfile.txt) or output that a program we run in the script writes to file(s). This type of output will always end up in those very same files regardless of whether we run the script directly or as a batch job."
  },
  {
    "objectID": "modules/08_slurm.html#monitoring-batch-and-other-compute-jobs",
    "href": "modules/08_slurm.html#monitoring-batch-and-other-compute-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "4 Monitoring batch (and other compute) jobs",
    "text": "4 Monitoring batch (and other compute) jobs\n\n4.1 A sleepy script for practice\nLet’s use the following short script to practice monitoring and managing batch and other compute jobs.\nOpen a new file in the VS Code editor (     =&gt;   File   =&gt;   New File) and save it as scripts/sleep.sh, then copy the following into it:\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n\necho \"I will sleep for 30 seconds\" &gt; sleep.txt\nsleep 30s\necho \"I'm awake!\"\n\n\n\nOn Your Own: Batch job output recap\nIf you submit the script as a batch job using sbatch scripts/sleep.sh:\n\nHow many output files will this batch job produce?\nWhat will be in it/them?\nIn which directory will the file(s) appear?\nIn terms of output, what would have been different if we had run the script directly, i.e. using the command bash scripts/sleep.sh?\n\nYou can test your predictions by running the script, if you want.\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will produce 2 files\nThey will contain:\n\nsleep.txt: I will sleep for 30 seconds\nslurm-&lt;job-number&gt;.out: I'm awake!\n\nBoth files will end up in your current working directory.\nIf we had run the script directly, slept.txt would have been the same, but All done! would have been printed to screen.\n\n\n\n\n\n\n\n4.2 Checking the status of our batch job\nAfter we submit a job, it may be initially be queued (or pending), before the Slurm scheduler finds a “slot” for our job. Then, the job will start running, and at some point it will stop running, either because the script ran into and error or because it ran to completion.\nHow can we check the status of our batch job? We can do so using the Slurm command squeue:\n\nsqueue -u $USER -l\n\nIn the command above:\n\nOur user name is specified with the -u option (otherwise we would see everyone’s jobs) —\nWe use the environment variable $USER, which is a variable that’s always available and contains your user name, so that the very same code will work for everyone (you can also simply type your user name if that’s shorter or easier).\nWe’ve added the -l option to get more verbose output.\n\nLet’s try that — first we submit the script:\n\nsbatch scripts/sleep.sh\n\n\nSubmitted batch job 12431945\n\nWe may be able to catch the STATE being PENDING before the job starts:\nsqueue -u $USER -l\n# Fri Aug 19 07:23:19 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n#           12520046 serial-40 sleep.sh   jelmer  PENDING       0:00   1:00:00      1 (None)\nBut soon enough it should say RUNNING in the STATE column:\nsqueue -u $USER -l\n# Fri Aug 19 07:23:45 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n#           12520046 condo-osu sleep.sh   jelmer  RUNNING       0:12   1:00:00      1 p0133\nThe script should finish after 30 seconds (sleep 30s…), and after that, the squeue output will only show the header line with column names:\n\nsqueue -u $USER -l\n# Fri Aug 19 07:24:18 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON) \n\nOnce a job has finished running, it disappears from the squeue listing. So, the output above means that we have no running (or pending) jobs.\nBut we need to check our output file(s) to see if our script ran successfully!\n\ncat sleep.txt\n\n\nI will sleep for 30 seconds\n\n\ncat slurm-12520046.out\n\n\nI’m awake!\n\n\n\n\n4.3 Cancelling jobs (and other monitoring/managing commands)\nSometimes, you want to cancel one or more jobs, because you realize you made a mistake in the script or you used the wrong input files. You can do so using scancel:\n\nscancel 2979968        # Cancel job number 2979968\nscancel -u $USER       # Cancel all your jobs\n\n\n\n\n\n\n\nAt-home reading: Other commands and options\n\n\n\n\n\n\nCheck only a specific job by specifying the job ID, e.g 2979968:\n\nsqueue -j 2979968\n\nOnly show running (not pending) jobs:\n\nsqueue -u $USER -t RUNNING\n\nUpdate Slurm directives for a job that has already been submitted:\n\nscontrol update job=&lt;jobID&gt; timeLimit=5:00:00\n\nHold and release a pending (queued) job, e.g. when needing to update input file before it starts running:\n\nscontrol hold &lt;jobID&gt;        # Job won't start running until released\nscontrol release &lt;jobID&gt;     # Job is free to start\n\nYou can see more details about any running or finished jobs, including the amount of time it ran for:\n\nscontrol show job 2526085   # For job 2526085\n\n# UserId=jelmer(33227) GroupId=PAS0471(3773) MCS_label=N/A\n# Priority=200005206 Nice=0 Account=pas0471 QOS=pitzer-default\n# JobState=RUNNING Reason=None Dependency=(null)\n# Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n# RunTime=00:02:00 TimeLimit=01:00:00 TimeMin=N/A\n# SubmitTime=2020-12-14T14:32:44 EligibleTime=2020-12-14T14:32:44\n# AccrueTime=2020-12-14T14:32:44\n# StartTime=2020-12-14T14:32:47 EndTime=2020-12-14T15:32:47 Deadline=N/A\n# SuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-12-14T14:32:47\n# Partition=serial-40core AllocNode:Sid=pitzer-login01:57954\n# [...]"
  },
  {
    "objectID": "modules/08_slurm.html#common-sbatch-options",
    "href": "modules/08_slurm.html#common-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "5 Common sbatch options",
    "text": "5 Common sbatch options\n\n\n\n\n\n\nLong and and short option format\n\n\n\nMany SLURM options have a long format (--account=PAS2250) and a short format (-A PAS2250), which can generally be used interchangeably. For clarity, we’ll stick to long format options during this workshop.\n\n\n\n5.1 --account: The OSC project\nAs seen above. Always specify the project when submitting a batch job.\n\n\n5.2 --time: Time limit (“wall time”)\nSpecify the maximum amount of time your job will run for. Wall time is a term meant to distinguish it from, say “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\n\nYour job gets killed as soon as it hits the specified time limit!\nYou will only be charged for the time your job actually used.\nThe default time limit is 1 hour. Acceptable time formats include:\n\nminutes\nhours:minutes:seconds\ndays-hours\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\n\n\n#!/bin/bash\n#SBATCH --time=1:00:00\n\n\n\n\n\n\n\nAsk for more time\n\n\n\nIf you are uncertain about the time your job will take, ask for (much) more time than you think you will need. This is because queuing times are generally good at OSC and you won’t be charged for reserved-but-not-used time.\n\n\n\n\n5.3 --mem: RAM memory\nSpecify a maximum amount of RAM (Random Access Memory) that your job can use.\n\nThe default unit is MB (MegaBytes) — append G for GB.\nThe default amount is 4 GB per core that you reserve\nLike with the time limit, your job gets killed when it hits the memory limit.\n\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n\n\n\n\nDefault memory limits usually work\n\n\n\nIt is not that common to hit the memory limit, so I usually don’t specify it — unless the program reports needing lots of memory, or I got “out-of-memory” errors when trying to run the script before.\n\n\n\n\n5.4 Cores (& nodes and tasks)\nSpecify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing!\n\nSlurm for the most part uses “core” and “CPU” interchangeably3. More generally, “thread” is also commonly used interchangeably with core/CPU4.\n\n\nRunning a program that uses multiple threads/cores/CPUs (“multi-threading”) is common. In such cases, specify the number of threads/cores/CPUs n with --cpus-per-task=n (and keep --nodes and --ntasks at their defaults of 1).\nThe program you’re running may have an argument like --cores or --threads, which you should then set to n as well.\n\n\n\n\n\n\n\nUncommon cases\n\n\n\n\nOnly ask for &gt;1 node when a program is parallelized with e.g. “MPI”, which is uncommon in bioinformatics.\nFor jobs with multiple processes (tasks), use --ntasks=n or --ntasks-per-node=n.\n\n\n\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\n\n\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n\n\n\n5.5 --output: Slurm log files\nAs we saw above, by default, all output from a script that would normally5 be printed to screen will end up in a Slurm log file when we submit the script as a batch job. This file will be created in the directory from which you submitted the script, and will be called slurm-&lt;job-number&gt;.out, e.g. slurm-12431942.out.\nBut it is possible to change the name of this file. For instance, it can be useful to include the name of the program that the script runs, so that it’s easier to recognize this file later.\nWe can do this with the --output option, e.g. --output=slurm-fastqc.out if we were running FastQC.\nHowever, you’ll generally want to keep the batch job number in the file name too6. Since we won’t know the batch job number in advance, we need a trick here and that is to use %j, which represents the batch job number:\n\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n\n\n\n\n\n\n\nAt-home reading: stdout and stderr\n\n\n\n\n\nBy default, two output streams “standard output” (stdout) and “standard error” (stderr) are printed to screen and therefore also both end up in the same Slurm log file, but it is possible to separate them into different files.\nBecause stderr, as you might have guessed, often contains error messages, it could be useful to have those in a separate file. You can make that happen with the --error option, e.g. --error=slurm-fastqc-%j.err.\nHowever, reality is more messy: some programs print their main output not to a file but to standard out, and their logging output, errors and regular messages alike, to standard error. Yet other programs use stdout or stderr for all messages.\nI therefore usually only specify --output, such that both streams end up in that file."
  },
  {
    "objectID": "modules/08_slurm.html#addendum-table-with-sbatch-options",
    "href": "modules/08_slurm.html#addendum-table-with-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "6 Addendum: Table with sbatch options",
    "text": "6 Addendum: Table with sbatch options\nThis includes all the discussed options, and a couple more useful ones:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nProject to be billed\n-A PAS0471\n--account=PAS0471\nN/A\n\n\nTime limit\n-t 4:00:00\n--time=4:00:00\n1:00:00\n\n\nNr of nodes\n-N 1\n--nodes=1\n1\n\n\nNr of cores\n-c 1\n--cpus-per-task=1\n1\n\n\nNr of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr of tasks per node\n-\n--ntasks-per-node\n1\n\n\nMemory limit per node\n-\n--mem=4G\n(4G)\n\n\nLog output file (%j = job number)\n-o\n--output=slurm-fastqc-%j.out\n\n\n\nError output (stderr)\n-e\n--error=slurm-fastqc-%j.err\n\n\n\nJob name (displayed in the queue)\n-\n--job-name=fastqc\n\n\n\nPartition (=queue type)\n-\n--partition=longserial  --partition=hugemem\n\n\n\nGet email when job starts, ends, fails,  or all of the above\n-\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\n\nLet job begin at/after specific time\n-\n--begin=2021-02-01T12:00:00\n\n\n\nLet job begin after other job is done\n-\n--dependency=afterany:123456"
  },
  {
    "objectID": "modules/08_slurm.html#footnotes",
    "href": "modules/08_slurm.html#footnotes",
    "title": "Compute Jobs with Slurm",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther options: salloc works almost identically to srun, whereas sinteractive is an OSC convenience wrapper but with more limited options.↩︎\nTechnically, these are two different types of output, as we briefly touch on below: “standard output” and “standard error”.↩︎\nEven though technically, one CPU often contains multiple cores.↩︎\nEven though technically, one core often contains multiple threads.↩︎\nThat is, when we run the script directly, e.g. bash myscript.sh↩︎\nFor instance, we might be running the FastQC script multiple times, and otherwise those would all have the same name and be overwritten.↩︎"
  },
  {
    "objectID": "modules/02_osc.html#introduction",
    "href": "modules/02_osc.html#introduction",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis session will provide an introduction to supercomputers in general and to the Ohio Supercomputer Center (OSC) specifically.\n\n1.1 Supercomputers\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run.\nYou analyses need large numbers of CPUs or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs.\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working RNAseq data or other kinds of genomic data, many of these reasons apply. This can make it hard or simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\n1.2 The Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio (not The Ohio State University). It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources. At least for folks at OSU, using OSC is currently usually free in practice. Having such a good HPC resource available at no cost is not something we should take for granted — at many institutions, academics not only have have to pay for these kinds of resources, but those are often more limited and not as easy to access.\nIn upcoming sessions, we’ll continue to work at OSC, so you will get a fair bit of experience with using it. We’ll also have specific sessions dedicated to using VS Code at OSC, loading and installing software at OSC and using the SLURM job scheduler.\nOSC has three main websites:\n\nhttps://osc.edu: OSC’s general website, with lots of information about the supercomputers, the software that’s installed, and how to use OSC.\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: A site to manage your account and OSC Projects you are an admin for (login needed)."
  },
  {
    "objectID": "modules/02_osc.html#the-structure-of-a-supercomputer-center",
    "href": "modules/02_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 The Structure of a Supercomputer Center",
    "text": "2 The Structure of a Supercomputer Center\nLet’s start with some terminology, going from smaller things to bigger things:\n\nCore / Processor / CPU / Thread — Components of a computer that can each (semi-)indendepently be asked to perform a computing task like running a bioinformatics program. While these terms are not technically all synonyms, we can treat them as such for our purposes.\nNode — A single computer that is a part of a supercomputer and has dozens of cores (i.e., they tend to be more powerful than a personal laptop or desktop).\nSupercomputer / Cluster — Many computers connected by a high-speed network. (“Pitzer” and “Owens” are the two currently active ones at OSC.)\nSupercomputer Center — A facility like OSC that has one or more supercomputers.\n\n\n\n\nThis is what the Owens supercomputer at OSC physically looks like:\n\n\n\n\n\n\n\n\n\nLinux and Unix\n\n\n\nLike the vast majority of supercomputers, OSC’s run on the Linux operating system (as opposed to on MacOS or Windows). In turn, Linux is a Unix-based operating system like MacOS (but unlike Windows).\n\n\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two clusters!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\nLet’s take those in order.\n\n\n2.1 File Systems\nThere are 4 main file systems where you can store files at OSC: Home Directories, Project Directories, Scratch Directories, and Compute storage.\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/ 1\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/ 2\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\nCompute\n$TMPDIR\n1 TB\nNo\nAfter job completes\nCompute job\n\n\n\nYou’ll interact most with the Project directories: this is because for most files, you’ll want a permanent and backed-up location (i.e., not Scratch or Compute storage), and the Home directory offers relatively limited storage as well as challenges with file sharing.\n\n\n\n\n\n\nUnix terminology and environment variables\n\n\n\nWe’ll talk about all of this more in upcoming sessions, but to clarify some of the terms and concepts mentioned here:\n\n“Directory” (or “dir” for short) is a commonly used term in Unix that just means “folder”.\nIn the “Located within” column in the table above, the leading forward slash / signifies the system’s “root” (top-level) directory, and forward slashes are also used to separate directories (unlike in Windows, which uses backslashes).\nFile and directory locations on a computer are often referred to as “paths”.\n$TMPDIR is a so-called “environment variable” that contains the path to the Compute storage directory (in the Unix shell, all variables are referenced by putting a $ before their name, and environment variables are in all-caps). A variable is useful in this case, because the location of this storage space will vary depending on the compute node at which it’s located. Along similar lines, your Home directory’s path is stored in $HOME.\n\n\n\n\n2.1.1 Home Directory\nWhen you initially get an account with OSC, a Home directory is created for you, named with your OSC username. This directory will always be within /users/, and then, somewhat strangely, in a directory containing the name of the OSC Project you were first added to (and this will not change even if you’re no longer a member of that project, or if that project ceases to exist). For example, my Home directory is /users/PAS0471/jelmer.\nYou will only ever have one Home directory. You also cannot expand the standard 500 GB of storage — if you need more space, you should turn to your Project directories.\nIf possible, I recommend to use your Home directory only for some general files (like some software, tutorials and practice, general scripts and databases), and to use Project directories for all your research project data and results.\n\n\n2.1.2 Project Directories\nProject directories are linked to OSC projects, which are typically set up by PIs. They offer flexibility in terms of the amount of storage available, and also in terms of who can access files in the directory.\nBy default, all members of an OSC Project have “read access” (the ability to see and copy files) for all files in a project directory, which makes it suitable for collaborating on a research project. But rest assured: except for OSC staff, other people can never move, modify, or delete your files (i.e., they don’t have “write access”) — not even the admins (PIs) for the OSC Project in question.\nLike Home directories, Project directories are backed up daily. You don’t have direct access to the backups, but if you’ve accidentally deleted some important files (Linux has no thrash bin!), you can request them to be restored to the way they were on a specific date.\n\n\n\n\n\n\nFile Systems are shared among the clusters\n\n\n\nWhile OSC’s current two clusters, Owens and Pitzer, are largely separate, they do share the same File System. This means that you can access your files in the exact same way regardless of which supercomputer you have connected to.\nFor example, your Home directory can be accessed using the same path (in my case, /users/PAS0471/jelmer) on Pitzer and Owens.\n\n\n\n\n2.1.3 Temporary storage: Scratch and Compute\nEvery OSC Project also has a Scratch directory. The two main advantages of Scratch space are that it is effectively unlimited and that it has faster data read and write (“I/O”) speed than Home and Project space. However, it’s not backed up, and files that are unmodified for 90 days are automatically deleted. As such, Scratch storage is mostly useful for intermediate results that are likely not needed again and can be reproduced easily.3\nCompute storage space is even more fleeting: as soon as the compute “job” in question has stopped (e.g. your script has finished), these files will be deleted. We’ll talk a bit more about this type of storage later, as using them can save time for I/O-intensive analyses.\n\n\n\n\n2.2 Login Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, and they are shared among everyone and cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing.\nAttempting large computing efforts on these nodes risks taxing the limited resources on these nodes, and bogging things down for everyone. There are checks built in that limit what you are able to do on the login nodes (i.e. jobs running for longer than 20 minutes will be killed), but it’s best to just not push it at all. Any serious computing should be done on the compute nodes.\n\n\n\n2.3 Compute Nodes\nCompute nodes are really the powerhouse of the supercomputer, and this is where you run your data processing and analysis.\nYou can use compute nodes by putting in requests for resources, such as the number of nodes, cores, and for how long you will need them. Because many different users are sending such requests –i.e., for “compute jobs” or just “jobs”– all the time, there is software called a job scheduler (specifically, Slurm in case of OSC) that considers each request and assigns the necessary resources to the job as they become available.\n\n\n\n\n\n\nInteractive and batch use of compute nodes\n\n\n\nRequests for compute node jobs can be made either through the OnDemand website or with commands like sinteractive and sbatch.\nFor instance, when we start an RStudio session at OSC, we first have to fill out a little form with such a request, and then RStudio will run on a compute node. This is an example of using a compute node interactively — “you” are located on a compute node, and any R command you type will be executed there. More commonly for genomics work, you’ll be using compute nodes non-interactively, that is, through “batch jobs”. When doing so, you will write a script in advance and send it to the job scheduler, which will run the script on a compute node that “you” don’t go to at all.\nThe session Compute Jobs with Slurm is dedicated to this topic.\n\n\nCompute nodes come in different shapes and sizes. “Standard nodes” are by far the most numerous (e.g., Owens has 648 and Pitzer has 564 of them) and even those vary in size, from 28 cores per node (Owens) to 48 cores per node (the “expansion” part of Pitzer). Some examples of other types of nodes are ones with extra memory (largemem and hugemem) and ones that provide access to GPUs (Graphical Processing Units) rather than CPUs.\nFortunately, you don’t tend to have to think much about node types as you start using OSC, since Standard nodes are automatically picked by default, and those will serve you well for the vast of majority genomics analysis.4\n\n\n\n\n\n\nMemory versus storage\n\n\n\nWhen we talk about “memory”, this refers to RAM: the data that your computer has actively “loaded” or in use. For example, if you play a computer game or have many browser tabs open, your computer’s memory will be heavily used. Genomics programs sometimes load all of the input data from disk to memory for fast access, or hold a huge assembly graph in memory, and as such may need a lot of memory as well.\nDon’t confuse memory with file storage, the data that is on disk, some of which may have been unused for years.\n\n\n\n\n\n2.4 Putting it together\nAll these parts are connected together to create a supercomputer — for example, let’s take a look at the specs for Owens now that we understand the components a bit better:"
  },
  {
    "objectID": "modules/02_osc.html#connecting-to-osc-with-ondemand",
    "href": "modules/02_osc.html#connecting-to-osc-with-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 Connecting to OSC with OnDemand",
    "text": "3 Connecting to OSC with OnDemand\nThe classic way of connecting to supercomputers is using SSH, like with the ssh command in a Unix shell on your computer (see this reference page). However, OSC has pioneered the use of a web portal called OnDemand, which has since become more widely used among supercomputer centers.\nThe OSC OnDemand website, https://ondemand.osc.edu, then, allows you to access OSC resources through a web browser. When you go there, you first need to log in with your OSC (not OSU!) credentials. After that, you should see a landing page similar to the one below:\n\n\n\nThe main part of the page (below the logo) only contains some general OSC messages and updates — what we will focus on instead are some of the options in the blue bar along the top.\n\n\n3.1 File System Access\nLet’s start with Files. Hovering over this dropdown menu gives a list of directories you have access to. If you’re account is new, you might only have three: a Home directory, and a Project and Scratch directory for one OSC Project.\nFor every project you’re associated with, directories are added — I’m associated with quite a few different projects, so I have a long list under Files. I’ll select the Project directory for the MCIC’s main OSC Project, PAS0471, which is /fs/ess/PAS0471:\n\n\n\nOnce there, I can see a list of directories and files inside this Project directory, and I can click on the directories to explore the contents further.\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders: see the buttons across the top.\nAdditionally, there are Upload and Download buttons for uploading files from your computer to OSC, and downloading them from OSC to your computer. These are only suitable for relatively small transfers, roughly below 1 GB. Other options to transfer files to and from OSC are remote transfer commands like scp (also for smaller transfers), and SFTP or Globus for larger transfers. To learn more about these options, see the reference page on OSC file transfer.\n\n\n\n\n\n\nNote\n\n\n\nWe will skip the “Jobs” dropdown menu in the blue top bar, because in later sessions, we will learn to create, submit, and monitor compute jobs at the command line instead, which quickly becomes more efficient as you get the hang of it.\n\n\n\n\n\n3.2 System Status (in Clusters)\nMoving on to “Clusters”, we’ll start with the item at the bottom of that dropdown menu, “System Status”:\n\n\n\nThis page shows an overview of the current usage of the two clusters, which might help to decide which cluster you want to use and set some expectations for compute job waiting times:\n\n\n\n\n\n\n3.3 Unix Shell Access (in Clusters)\nInteracting with a supercomputer in a point-and-click manner only goes so far. Using a supercomputer effectively requires interacting with the system using a command-line (CLI) rather than a graphical user (GUI) interface.\nAgain under the Clusters option in the blue top bar, you can access a Unix shell either on Owens or Pitzer:\n\n\n\nI’m selecting a shell on the Pitzer supercomputer, which will open a new browser tab looking like this:\n\n\n\nWe most commonly interact with a supercomputer using a Unix shell, and we’ll learn about the basics of doing so in an upcoming session. However, we’ll mostly be accessing a Unix shell in a different manner, namely inside the VS Code text editor, which also gives us some additional functionality in a user-friendly way.\n\n\n3.4 Interactive Apps\nWe can get access to VS Code, as well as many other programs with GUIs such as RStudio, via the Interactive Apps dropdown menu (and the menu item next to that, My Interactive Sessions, will list the sessions that are currently active as well as finished ones).\n\n\n\n\n\n\n\n\n\n‘VS Code’ versus ‘Code Server’\n\n\n\nIn the list, the “VS Code” program is called “Code Server”, much like “RStudio” is called “RStudio Server”. They are the same programs but with minor edits to allow them to run remotely in a browser rather than as locally installed on your own computer.\n\n\n“Interactive Apps” like VS Code and RStudio run on compute nodes — therefore, we need to fill out a form and specify some details for our interactive compute job request:\n\nThe OSC Project that should be billed for the compute resource usage — a dropdown menu will list all Projects you are a member of.\nThe amount of time we want to make a reservation for — we’ll be kicked off as soon as that amount of time has passed!\nThe “working directory” (starting location in the file system) for the program, which we can type in the box or select with the “Select Path” button (the default is your Home directory, here referred to as $HOME).\nThe software version — the most recent available one should be automatically selected, and that is almost always what you’ll want.\n\n\n\n\nClick on Launch at the bottom and this will send your request to the compute job scheduler. First, your job will be “Queued” — that is, waiting for the job scheduler to allocate resources on the compute nodes to it:\n\n\n\nIn general, it should be granted resources within a few seconds (the card will then say “Starting”), and be ready for usage (“Running”) in another couple of seconds:\n\n\n\nThen, you can click on the blue Connect to VS Code button to open a new browser tab that runs VS Code. We’ll explore VS Code in the next session."
  },
  {
    "objectID": "modules/02_osc.html#in-closing",
    "href": "modules/02_osc.html#in-closing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 In Closing",
    "text": "4 In Closing\n\n4.1 Administrative miscellaneae\n\nRequesting & managing OSC Projects, and user accounts\nGenerally, only PIs request OSC projects, and they typically manage them as well. OSC has this page with more information on how to do so. Whoever manages an OSC Project can add both existing OSC users and new users to the Project, which will provide these users with access the project’s Project and Scratch directories, and the ability to specify this Project in association with compute node resource requests.\nWhen you get added to an OSC Project and don’t yet have an OSC account, you will automatically receive an email with a link that allows you to create an account. It is not possible to create an account before having been added to an OSC Project.\nBilling\nOSC will bill OSC Projects (not individual users), and only for the following two things:\n\nFile storage in the Project Storage file system\nCompute node usage by “core hour” (e.g. using 2 cores for 2 hours = 4 core hours)\n\nThe prices for academic usage are quite low (see this page for specifics), and importantly, at OSU, they are often covered at the department level such that individual PIs do not have to directly pay for this at all.\nWhen you use OSC, it’s good practice to acknowledge and cite OSC in your papers, see their citation page.\nFor many questions such as if you have problems with your account, have problems installing or using specific software, or don’t understand why your jobs keep failing, you can email OSC at oschelp@osc.edu. They are usually very quick to respond!\n\n\n\n\n4.2 Upcoming sessions on OSC\nToday, we have learned some of the basics of supercomputers and of accessing OSC. In separate sessions in this series, we will look at:\n\nUsing specific “Interactive Apps” (GUI-based programs):\n\nVS Code\nRStudio (TBA)\n\nLoading and installing command-line software at OSC\nSubmitting batch jobs using the SLURM scheduler\n\nAdditionally, there are pages with reference material (see the right side of the top menu bar of this side) on:\n\nFile transfer to and from OSC\nUsing OSC with SSH (rather than through OnDemand)\n\n\n\n\n4.3 OSC’s learning resources\nTo learn more about OSC, I would start with these short courses:\n\nOSC’s online asynchronous courses\n\nThis includes a number of short videos\nWhen I tried to access these last, it wasn’t always clear where to go after enrolling, and one of the two courses had even diseappeared from the list. But the website https://scarlet.instructure.com then listed the courses and provided access.\n\n\nThis series of pages is also useful:\n\nNew User Resource Guide\nSupercomputing FAQ\nHOWTOs (tutorials on specific topics)\nInfo on batch (non-interactive) compute jobs (rather technical)\nOSC “events” such as Office Hours\n\n\n\n\n4.4 Acknowledgements\nThis page uses material from an OSC Introduction written by Mike Sovic and from OSC’s Kate Cahill Software Carpentry introduction to OSC (outdated)."
  },
  {
    "objectID": "modules/02_osc.html#footnotes",
    "href": "modules/02_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr /fs/project/↩︎\nOr /fs/ess/scratch↩︎\nFor example, many genome and transcriptome assemblers output a lot of data, but you will only need a few files (like the assembly) for your next steps.↩︎\nSome examples where you might need a different type of node are genome or transcriptome assembly where you might need nodes with a lot of memory, or Oxford Nanopore sequence data basecalling where you might need GPUs.↩︎"
  },
  {
    "objectID": "modules/07_software.html",
    "href": "modules/07_software.html",
    "title": "Using Software at OSC",
    "section": "",
    "text": "So far, we have only used commands that are available in any Unix shell. But to actually analyze genomics data sets, we also need to use specialized bioinformatics software.\nMost software that is already installed at OSC must nevertheless be “loaded” (“activated”) before we can use it — and if our software of choice is not installed, we have to do so ourselves. We will cover those topics in this module."
  },
  {
    "objectID": "modules/07_software.html#setup",
    "href": "modules/07_software.html#setup",
    "title": "Using Software at OSC",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/&lt;your-folder&gt; in the box Working Directory (replace &lt;your-folder&gt; by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    =&gt; Terminal =&gt; New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =&gt;   File   =&gt;   Open Folder and enter /fs/ess/scratch/PAS2250/&lt;your-folder&gt;."
  },
  {
    "objectID": "modules/07_software.html#running-command-line-programs",
    "href": "modules/07_software.html#running-command-line-programs",
    "title": "Using Software at OSC",
    "section": "2 Running command-line programs",
    "text": "2 Running command-line programs\nAs pointed out in the introduction to the workshop, bioinformatics software (programs) that we use to analyze genomic data are typically run from the command line. That is, they have “command-line interfaces” (CLIs) rather than “graphical user interfaces” (GUIs), and are run using commands that are structurally very similar to how we’ve been using basic Unix commands.\nFor instance, we can run the program FastQC as follows, instructing it to process the FASTQ file sampleA.fastq.gz with default options:\n\nfastqc sampleA.fastq.gz       # Don't run\n\nSo, with all the scaffolding we have learned in the previous modules, we only need to make small modifications to have our scripts run command-line programs. But, we first need to load and/or install these programs.\n\n\n\n\n\n\nRunning inside a script or interactively\n\n\n\nLike any other command, we could in principle run the line of code above either in our interactive shell or from inside a script. In practice, it is better to do this in a script, especially at OSC, because:\n\nSuch programs typically take a while to run\nWe are not supposed to run processes that use significant resources on login nodes\nWe can run the same script simultaneously for different input files."
  },
  {
    "objectID": "modules/07_software.html#software-at-osc-with-lmod",
    "href": "modules/07_software.html#software-at-osc-with-lmod",
    "title": "Using Software at OSC",
    "section": "3 Software at OSC with Lmod",
    "text": "3 Software at OSC with Lmod\nOSC administrators manage software with the Lmod system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it.\n(That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.)\n\n3.1 Checking for available software\nThe OSC website has a list of software that has been installed at OSC. You can also search for available software in the shell using two subtly different commands:\n\nmodule spider lists modules that are installed.\nmodule avail lists modules that can be directly loaded, given the current environment (i.e., depending on which other software has been loaded).\n\nSimply running module spider or module avail would spit out complete lists — more usefully, we can add search terms as arguments to these commands:\n\nmodule spider python\n\n\n\n\n\npython:\n\n\n\n Versions:\n    python/2.7-conda5.2\n    python/3.6-conda5.2\n    python/3.7-2019.10\n\n\nmodule avail python\n\n\npython/2.7-conda5.2         python/3.6-conda5.2 (D)         python/3.7-2019.10\n\n\n\n\n\n\n\n(D) = default version\n\n\n\nThe (D) in the output above marks the default version of the program; that is, the version of the program that would be loaded if we don’t specify a version (see examples below).\n\n\n\n\n\n3.2 Loading software\nAll other Lmod software functionality is also accessed using module “subcommands” (we call module the command and e.g. spider the subcommand). For instance, to load a module:\n\n# Load a module:\nmodule load python              # Load the default version\nmodule load python/3.7-2019.10  # Load a specific version (copy from module spider output)\n\nTo check which modules have been loaded (the list includes automatically loaded modules):\n\nmodule list\n\n\nCurrently Loaded Modules:\n    1) xalt/latest       2) gcc-compatibility/8.4.0       3) intel/19.0.5       4) mvapich2/2.3.3       5) modules/sp2020\n\n\n\n\n\n\n\nUnloading modules\n\n\n\nOccasionally useful when running into conflicting (mutually incompatible) modules:\n\nmodule unload python        # Unload a module\nmodule purge                # Unload all modules\n\n\n\n\n\n\n3.3 A practical example\nLet’s load a very commonly used bioinformatics program that we will also use in examples later on: FastQC. FastQC performs quality control (hence: “QC”) on FASTQ files.\nFirst, let’s test that we indeed cannot currently use fastqc by running fastqc with the --help flag:\n\nfastqc --help\n\n\nbash: fastqc: command not found\n\n\n\n\n\n\n\nHelp!\n\n\n\nA solid majority of command-line programs can be run with with a --help (and/or -h) flag, and this is often a good thing to try first, since it will tell use whether we can use the program, and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether FastQC is available at OSC, and if so, in which versions:\n\nmodule avail fastqc\n\n\nfastqc/0.11.8\n\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be a reason to specify the version when we load FastQC?\n\n\n\n\n\nWhen we use it inside a script:\n\nThis would ensure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nIt will make it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\n\nmodule load fastqc/0.11.8\n\nAfter we have loaded the module, we can retry our --help attempt:\n\nfastqc --help | head     # I'm piping into head to avoid pages worth of output \n\n\n        FastQC - A high throughput sequence QC analysis tool\nSYNOPSIS\n    fastqc seqfile1 seqfile2 .. seqfileN\n\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n       [-c contaminant file] seqfile1 .. seqfileN"
  },
  {
    "objectID": "modules/07_software.html#when-software-isnt-installed-at-osc",
    "href": "modules/07_software.html#when-software-isnt-installed-at-osc",
    "title": "Using Software at OSC",
    "section": "4 When software isn’t installed at OSC",
    "text": "4 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than is available. The main options available to you in such a case are to:\n\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”1 at OSC and will often have difficulties with “dependencies”2.\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through module). #TODO: Refer to the software form in https://www.osc.edu/resources/getting_started/supercomputing_faq#req\nUse Apptainer / Singularity “containers”. Containers are self-contained software environments that include operating systems, akin to mini virtual machines.\nUse conda, which creates software environments that are activated like in the module system.\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally, for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nWe will teach conda here because it is easier to learn and use than containers, and because nearly all open-source bioinformatics software is available as a conda package.\n\n\n\n\n\n\nWhen to use containers instead of Conda\n\n\n\n\nIf you need to use software that requires a different Operating System (OS) or OS version than the one at OSC.\nIf you want or require even greater reproducibility and portability to create an isolated environment that can be exported and used anywhere."
  },
  {
    "objectID": "modules/07_software.html#using-conda",
    "href": "modules/07_software.html#using-conda",
    "title": "Using Software at OSC",
    "section": "5 Using conda",
    "text": "5 Using conda\nConda creates so-called environments in which you can install one or more software packages. As mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system – but the key difference is that we can create and manage these environments ourselves.\n\n\n\n\n\n\nUse one environment per program (as here) or one per research project\n\n\n\n\n\nBelow are two reasonable ways to organize your conda environments, and their advantages:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you’re working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nIts not recommended to simply install all programs across all projects in one environment. This doesn’t benefit reproducibility and your environment is likely to sooner or later stop functioning properly.\nA side note is that even when you want to install a single program, multiple programs are in fact nearly always installed: the programs that your target program depends on (“dependencies”).\n\n\n\n\n\n5.1 Loading the (mini)conda module\nWhile it is also fairly straightforward to install conda for yourself 3, we will use OSC’s system-wide installation of conda in this workshop. Therefore, we first need to use a module load command to make it available:\n\n# (The most common installation of conda is actually called \"miniconda\")\nmodule load miniconda3\n\n\n\n\n5.2 One-time conda configuration\nWe will also do some one-time configuration, which will set the conda “channels” (basically, software repositories) that we want to use when we install software. This config also includes setting relative priorities among channels, since one software package may be available from multiple channels.\nLike with module commands, conda commands consist of two parts, the conda command itself and a subcommand, such as config:\n\nconda config --add channels defaults     # Added first =&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last =&gt; highest priority\n\nLet’s check whether this configuration step worked:\n\nconda config --get channels\n\n\n–add channels ‘defaults’       # lowest priority\n–add channels ‘bioconda’\n–add channels ‘conda-forge’       # highest priority\n\n\n\n\n5.3 Example: Creating an environment for cutadapt\nTo practice using conda, we will now create a conda environment with the program cutadapt installed.\ncutadapt is a commonly used program to remove adapters or primers from sequence reads in FASTQ files — in particular, it is ubiquitous for primer removal in (e.g. 16S rRNA) microbiome metabarcoding studies. But there is no Lmod module on OSC for it, so if we want to use it, our best option is to resort to conda.\nHere is the command to create a new environment and install cutadapt into that environment:\n\nconda create -y -n cutadapt -c bioconda cutadapt   # Don't run this\n\nLet’s break the above command down:\n\ncreate is the conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation.\nFollowing the -n option, we can specify the name of the environment, so -n cutadapt means that we want our environment to be called cutadapt. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a channel from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe cutadapt at the end of the line simply tells conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\nSpecifying a version\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name. We do that below, and we also include the version in the environment name.\n Let’s run the command above and see if we can install cutadapt\n\nconda create -y -n cutadapt-4.1 -c bioconda cutadapt=4.1\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment:\n\n\n\n\n\n\n\nSee the full output when I ran this command\n\n\n\n\n\n\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.10.3\n  latest version: 4.13.0\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\n\n## Package Plan ##\n\n  environment location: /fs/project/PAS0471/jelmer/conda/cutadapt-TMP\n\n  added / updated specs:\n    - cutadapt=4.1\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    cutadapt-4.1               |  py310h1425a21_1         211 KB  bioconda\n    dnaio-0.9.1                |  py310h1425a21_1          80 KB  bioconda\n    libsqlite-3.39.2           |       h753d276_1         789 KB  conda-forge\n    openssl-3.0.5              |       h166bdaf_1         2.8 MB  conda-forge\n    pip-22.2.2                 |     pyhd8ed1ab_0         1.5 MB  conda-forge\n    python-isal-1.0.1          |  py310h5764c6d_0          47 KB  conda-forge\n    setuptools-65.0.2          |  py310hff52083_0         1.4 MB  conda-forge\n    sqlite-3.39.2              |       h4ff8645_1         788 KB  conda-forge\n    tzdata-2022c               |       h191b570_0         119 KB  conda-forge\n    xopen-1.6.0                |  py310hff52083_0          27 KB  conda-forge\n    xz-5.2.6                   |       h166bdaf_0         409 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         8.1 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu\n  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4\n  ca-certificates    conda-forge/linux-64::ca-certificates-2022.6.15-ha878542_0\n  cutadapt           bioconda/linux-64::cutadapt-4.1-py310h1425a21_1\n  dnaio              bioconda/linux-64::dnaio-0.9.1-py310h1425a21_1\n  isa-l              conda-forge/linux-64::isa-l-2.30.0-ha770c72_4\n  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.36.1-hea4e1c9_2\n  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5\n  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.1.0-h8d9b700_16\n  libgomp            conda-forge/linux-64::libgomp-12.1.0-h8d9b700_16\n  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0\n  libsqlite          conda-forge/linux-64::libsqlite-3.39.2-h753d276_1\n  libuuid            conda-forge/linux-64::libuuid-2.32.1-h7f98852_1000\n  libzlib            conda-forge/linux-64::libzlib-1.2.12-h166bdaf_2\n  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1\n  openssl            conda-forge/linux-64::openssl-3.0.5-h166bdaf_1\n  pbzip2             conda-forge/linux-64::pbzip2-1.1.13-0\n  pigz               conda-forge/linux-64::pigz-2.6-h27826a3_0\n  pip                conda-forge/noarch::pip-22.2.2-pyhd8ed1ab_0\n  python             conda-forge/linux-64::python-3.10.5-ha86cf86_0_cpython\n  python-isal        conda-forge/linux-64::python-isal-1.0.1-py310h5764c6d_0\n  python_abi         conda-forge/linux-64::python_abi-3.10-2_cp310\n  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0\n  setuptools         conda-forge/linux-64::setuptools-65.0.2-py310hff52083_0\n  sqlite             conda-forge/linux-64::sqlite-3.39.2-h4ff8645_1\n  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0\n  tzdata             conda-forge/noarch::tzdata-2022c-h191b570_0\n  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0\n  xopen              conda-forge/linux-64::xopen-1.6.0-py310hff52083_0\n  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0\n  zlib               conda-forge/linux-64::zlib-1.2.12-h166bdaf_2\n\n\n\nDownloading and Extracting Packages\nsqlite-3.39.2        | 788 KB    | ################################################################################################################################################################### | 100% \nxz-5.2.6             | 409 KB    | ################################################################################################################################################################### | 100% \ncutadapt-4.1         | 211 KB    | ################################################################################################################################################################### | 100% \nxopen-1.6.0          | 27 KB     | ################################################################################################################################################################### | 100% \nlibsqlite-3.39.2     | 789 KB    | ################################################################################################################################################################### | 100% \ndnaio-0.9.1          | 80 KB     | ################################################################################################################################################################### | 100% \npython-isal-1.0.1    | 47 KB     | ################################################################################################################################################################### | 100% \nsetuptools-65.0.2    | 1.4 MB    | ################################################################################################################################################################### | 100% \npip-22.2.2           | 1.5 MB    | ################################################################################################################################################################### | 100% \ntzdata-2022c         | 119 KB    | ################################################################################################################################################################### | 100% \nopenssl-3.0.5        | 2.8 MB    | ################################################################################################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate cutadapt-4.1\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\n\n\n\n\n\n\n\n5.4 Activating conda environments\nWhereas we use the term “load” for Lmod modules, we use “activate” for conda environments — it means the same thing. Oddly enough, the most foolproof way to activate a conda environment is to use source activate rather than the expected conda activate — for instance:\n\nsource activate cutadapt-4.1\n\n\n(cutadapt-4.1) [jelmer@pitzer-login03 PAS2250]$\n\n\n\n\n\n\n\nEnvironment indicator\n\n\n\nWhen we have an active conda environment, its name is conveniently displayed in our prompt, as depicted above.\n\n\nAfter we have activated the cutadapt environment, we should be able to actually use the program. To test this, we’ll again simply run it with a --help option:\n\ncutadapt --help | head     # I'm piping into head to avoid pages worth of output \n\n\ncutadapt version 4.1\nCopyright (C) 2010-2022 Marcel Martin marcel.martin@scilifelab.se\ncutadapt removes adapter sequences from high-throughput sequencing reads.\nUsage:\n        cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq\nFor paired-end reads:\n\n\n\n\n5.5 Creating an environment for any program\nMinor variations on the conda create command above can be used to install almost any program for which is conda package is available. However, you may be wondering how we would know:\n\nWhether the program is available and what its conda package’s name is\nWhich conda channel we should use\nWhich versions are available\n\nMy strategy to finding these things out is to simply Google the program name together with “conda”, e.g. cutadapt conda.\nLet’s see that in action:\n\n\n\n\nWe click on that first link (it should always be the first Google hit):\n\n\n\nI always take the top of the two example installation commands as a template, here: conda install -c bioconda cutadapt\nYou may notice the install subcommand, which we haven’t yet seen. This would install Cutadapt into the currently activated conda environment. Since our strategy here –and my general strategy– is to create a new environment each time you’re installing a program, the all-in-one command is to replace install by create -y -n &lt;env-name&gt;.\nThen, our full command (without version specification) again becomes:\n\nconda create -y -n cutadapt -c bioconda cutadapt\n\nTo see which version will be installed by default, and to see which older versions are available:\n\n\n\nFor almost any other program, this works exactly the same!\n\n\n5.6 Lines to add to your Bash script\nWhile you’ll typically want to do installation interactively and only need to do to it once (see note below), you should always include the necessary code to load/activate your programs in your shell scripts.\nWhen your program is in an Lmod module, this simply entails a module load call — e.g., for fastqc:\n\n#!/bin/bash\nset -ueo pipefail\n\n# Load software\nmodule load fastqc\n\nWhen your program is available in a conda environment, this entails a module load command to load conda itself, followed by a source activate command to load the relevant conda environment:\n\n#!/bin/bash\n\n# Load software\nmodule load miniconda3\nsource activate cutadapt-4.1\n\n# Strict/safe Bash settings \nset -ueo pipefail\n\n\n\n\n\n\n\nWarning\n\n\n\nWe’ve moved the set -ueo pipefail line below the source activate command, because the conda activation procedure can sometims throw “unbound variable” errors otherwise.\n\n\n\n\n\n\n\n\nInstall once, load/activate always\n\n\n\n\nProvided you don’t need to switch versions, you only need to install a program once. This is true also at OSC and also when using conda: your environments won’t disappear unless you delete them.\nIn every single “session” that you want to use a program via an Lmod module or a conda environment, you need to load/activate the program. So the line(s) to do so should always be in your script for that program."
  },
  {
    "objectID": "modules/07_software.html#addendum-a-few-other-useful-conda-commands",
    "href": "modules/07_software.html#addendum-a-few-other-useful-conda-commands",
    "title": "Using Software at OSC",
    "section": "6 Addendum: a few other useful conda commands",
    "text": "6 Addendum: a few other useful conda commands\n\nDeactivate the currently active conda environment:\n\nconda deactivate   \n\nActivate one environment and then “stack” an additional environment (a regular conda activate command would switch environments):\n\nsource activate cutadapt         # Now, the env \"cutadapt\" is active\nconda activate --stack multiqc   # Now, both \"cutadapt\" and \"multiqc\" are active\n\nRemove an environment entirely:\n\nconda env remove -n cutadapt\n\nList all your conda environments:\n\nconda env list\n\nList all packages (programs) installed in an environment:\n\nconda list -n cutadapt"
  },
  {
    "objectID": "modules/07_software.html#footnotes",
    "href": "modules/07_software.html#footnotes",
    "title": "Using Software at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen your personal computer asks you to “authenticate” while you are installing something, you are authenticating yourself as a user with administrator privileges. At OSC (and for OSU-managed personal computers, too!), you don’t have such privileges.↩︎\nOther software upon which the software that you are trying to install depends.↩︎\nAnd this is certainly worth considering if you find yourself using conda a lot, because the conda version at OSC is quite out-of-date.↩︎"
  },
  {
    "objectID": "modules/05-shell2.html#keyboard-shortcuts",
    "href": "modules/05-shell2.html#keyboard-shortcuts",
    "title": "The Unix Shell - Part II",
    "section": "1 Keyboard shortcuts",
    "text": "1 Keyboard shortcuts\n\n\n\n\n\n\n\nShortcut\nCommand\n\n\n\n\nTab\nTab completion! Files, commands, etc.  Double Tab to show options when  multiple are still available.\n\n\n⇧ / ⇩\nCycle through command history\n\n\nCTRL + R\nEnter characters to search for in the history  (repeat CTRL + R to keep going back,  ENTER to put command in prompt)\n\n\nCTRL + C\nAbort (kill) current process\n\n\nCTRL + D\nExit the current shell (/ interactive job) (exit)\n\n\nCTRL + Z\nSuspend (pause) a process, then use bg to move to background (fg)\n\n\nCtrl+Shift+C\nCopy\n\n\nCtrl+Shift+V\nPaste\n\n\nCtrl+A\nGo to beginning of line\n\n\nCtrl+E\nGo to end of line\n\n\nCtrl+U\nCut to beginning of line\n\n\nCtrl+K\nCut to end of line\n\n\nCtrl+W\nCut previous word\n\n\nCtrl+Y\nPaste previously cut element\n\n\nAlt+.\nPaste last argument of last command\n\n\n\n\nCtrl+C to kill the currently running command. Try this:\n# For each, see what happens, then press Ctrl + C\n$ sleep 60s\n\n$ echo \"Missing quote\"  # Omit closing quote!\nCtrl+D or typing exit will exit a shell.\nPress Ctrl+A to move to the beginning of the line,\nand add ls to the beginning: ls /fs/project/PAS1855.\nPress Enter (anywhere on the line!).\nPress ⇧ to get the previous command back on the prompt,\nand then press Ctrl+U to delete until the beginning of the line.\nCtrl+U actually cut the text: “Yank” it back with Ctrl+Y.\nCreate a directory for yourself using mkdir:\n$ mkdir /fs/ess/PAS1855/users/$USER\nMove into this dir using cd – after typing cd and a space, press Alt+.:\n$ cd /fs/ess/PAS1855/users/$USER"
  },
  {
    "objectID": "modules/05-shell2.html#wildcards",
    "href": "modules/05-shell2.html#wildcards",
    "title": "The Unix Shell - Part II",
    "section": "2 Wildcards",
    "text": "2 Wildcards\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets"
  },
  {
    "objectID": "modules/05-shell2.html#commands-for-file-organization",
    "href": "modules/05-shell2.html#commands-for-file-organization",
    "title": "The Unix Shell - Part II",
    "section": "3 Commands for file organization",
    "text": "3 Commands for file organization\n\n3.1 mkdir\nThe mkdir command allows you to create a new directory. Create one for yourself in the current directory … FINISH\nmkdir results scripts\nmkdir -p\n\n\n\n\n\n\nThe trouble with spaces\n\n\n\n\nBecause spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS1855/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\n\n\n\n\n\n3.2 cp\nThe data files in the data/fastq directory are FASTQ formatted files from an RNA experiment, and are what we’ll be analyzing as we go through the workshop. We’ll talk more about them soon, but for now, let’s make sure everyone has a copy of the data. We’ll copy the data directory and its contents into the new directory you just made.\nThe cp command allows you to copy files or directories from one location to another. It has 2 required arguments – what you want to copy, and where you want to copy it to.\nLet’s start with what we want to copy. It’s the data directory and all of its contents.\nNotice in the diagram above that data is at the same level in the directory structure as our current working directory, participants. This means using data as a relative path won’t work, because the computer looks down the directory structure (it will see the contents of ‘participants’). But there’s a way to deal with that. We can use .. to move us up a level in the directory structure.\ncp ...\nNotice we get a message that it omitted copying the directory data (which is what we wanted to copy). Indeed, the copy didn’t work (you can ls the contents of the target directory to check – it will still be empty). cp works in this simplest form with individual files, but not with directories that have contents inside them. If you want to copy a directory and all of its contents, we need one of those options that modify the behavior of the cp command. In this case, -r, which tells it to copy in a recursive manner.\n\n\n3.3 Command History\nAnd this is a good spot to introduce the Command History. At the prompt, try hitting the up arrow. A record of all your previous commands is kept, so you can scroll back through them. Use this to get the previous cp command, and then add the -r argument.\ncp -r ...\nls\n\n\n3.4 mv"
  },
  {
    "objectID": "modules/05-shell2.html#working-with-text-files",
    "href": "modules/05-shell2.html#working-with-text-files",
    "title": "The Unix Shell - Part II",
    "section": "4 Working With Text Files",
    "text": "4 Working With Text Files\nNow let’s start to explore our FASTQ files a bit. In preparation, it’s a good chance to practice a few of the commands we’ve seen so far.\n\nOn Your Own: Explore the Files\nSet your working directory to the data/fastq directory inside the folder you created for yourself. Then list the contents of that fastq directory. How many files are in there? See if you can get the sizes of each file.\n\n\nHint (click here)\n\nUse cd and a relative path (&lt;your_dir&gt;/data/fastq/) to change you working directory.\nOnce you’re there, use ls to list the contents of the current directory. Recall the option that we used above to give more detailed information about each file, or check out the man page for ls.\n\n\n\nSolutions (click here)\n\n\ncd &lt;your_dir&gt;/data\n\nls\n\nls -l\n\n\n\n\n\n4.1 Compressed Files\nYou might have noticed these files all have a .gz extension, indicating they are ‘gzip-compressed’. This is a common type of compression for large genomic-scale files. The fact that they’re compressed means we can’t just open them up and look inside – we need to uncompress them first. The gunzip command would allow us to do this: it uncompresses the file it’s given and writes the uncompressed version to a new file.\nWe could do this, but there’s another approach. FASTQ files can get big, and sometimes it helps to be able to keep them compressed as much as possible. It’s a good time for us to explore the pipe.\n\n\n\n4.2 | (pipe)\nWe talked earlier about that commands nearly always print their output is printed to the screen. But you can also redirect the output, and there are three primary ways to redirect it:\n\nWith &gt;, which is followed by the name of a text file the output will be written to\nWith &gt;&gt;, which is similar to &gt; but will append the output (that is, it won’t overwrite any existing content like &gt;)\nWith | (pipe), which takes the output of one command and “pipes” it as input for a subsequent command.\n\nLet’s try to preview the contents of one of the compressed files.\n\n\n4.3 head\nThe head command is a great way to preview the contents of a text file. By default, head prints the first 10 lines of a file. Since these are FASTQ files, let’s print 8 lines (a multiple of 4 – it will become clear why shortly). We can use the -n argument to specify the number of lines that will be returned.\nhead -n 8 x.fastq.gz\n\n\n\nThis isn’t what we want – we’re seeing the first 8 lines of the compressed files, which is not helpful.\n\n\n4.4 zcat\nThe zcat function prints human-readable contents of a gzip-compressed file to the screen. We can try running it on the file, but remember the file is pretty big – there are lots of lines of text in there that will all get printed to the screen. Instead, we can pipe the output of zcat to the head command.\nzcat x.fastq.gz | head -n 8\n\n\n\nMuch better – this is what the raw RNAseq data look like!\n\n\n\n\n\n\nWarning\n\n\n\nTo get the number of lines (= number sequences x 4 – see below) for a gzipped FASTQ file, it’s important to use zcat x.fastq.gz | wc -l instead of wc -l x.fastq.gz, because the compressed file does not have the same number of lines!\n\n\n\n\n\n4.5 FASTQ Format\nIf you’re not familiar with it, FASTQ is a very common format for genomic data files. The raw data produced by a high-throughput sequencer will almost certainly be returned to you in this format. These are plain text files, and each sequence that is read by the sequencer is represented by 4 lines:\n\nA name (header) line\nThe sequence itself\nA plus sign (+)\nQuality scores corresponding to each base position in the sequence\n\n\n\n4.6 wc\nSince each read in a FASTQ file is represented by 4 lines, we should expect the number of lines in each of the FASTQ files to be a multiple of 4. Let’s check one. The wc command stands for word count, but by default, it returns the number of words, lines, and characters in a file. The -l option tells it to return just the number of lines, so we’ll use it since that’s all we’re interested in right now. And remember, we’ll want to do this on the uncompressed data.\nzcat x.fastq.gz | wc -l\n\n\n\n\n\n\nNote\n\n\n\nwc -l file\nzcat file | wc -l\nwc -l &lt; file\n\n\n\n\n4.7 grep\ngrep allows you to search through a file for specific patterns of text and returns the matching lines. For example, let’s say we wanted to see what sequences in sample SRR7609467 contain the sequence “ACCGATACG”:\nzcat x.fastq.gz | grep \"ACCGATACG\"\n\n\n\n\nOn Your Own: Finding a Sequence\nHow many sequences in sample SRR7609467 contain the sequence “CCAGTA”?\n\n\nHint (click here)\n\nPipe the results of the grep to wc -l. Alternatively, check out the -c option to grep in the man page.\n\n\n\nSolutions (click here)\n\n\nzcat SRR7609467.fastq.gz | grep 'CCAGTA' | wc -l\n\nOR\n\nzcat SRR7609467.fastq.gz | grep -c 'CCAGTA'"
  },
  {
    "objectID": "modules/05-shell2.html#downloading-files-from-the-web",
    "href": "modules/05-shell2.html#downloading-files-from-the-web",
    "title": "The Unix Shell - Part II",
    "section": "5 Downloading Files from the Web",
    "text": "5 Downloading Files from the Web\nTo download a file to OSC, you can’t just open a web browser and download it directly to there. One way would be to download it to your own computer and then transfer it to OSC.\nA more direct approach is to use a command in your OSC Unix shell.\nTo analyze our RNAseq data, we’ll need two files related to our reference genome — the genome that we want to map our RNAseq reads to, and whose gene annotations will form the basis of the gene counts that we’ll get.\nSpecifically, we’ll need the nucleotide FASTA file with the genome assembly itself, and a so-called GFF file, a tabular file with the genomic coordinates and other information for genes and other genomic features.\n\nOn Your Own: Create a Directory\nCreate a new (empty) directory named reference that will later store the reference genome for our analyses. Put it in your own directory inside participants. Then make this reference directory your working directory.\n\n\nHint (click here)\n\nUse the mkdir command (and cd as necessary).\nRemember that .. moves you up/back one directory, and these can be combined. For example, ../../../ would move you up/back 3 directories.\n\n\n\nSolution (click here)\n\n\nmkdir ../../reference\n  \ncd ../../reference\n\nOR\n\ncd ../../\n  \nmkdir reference\n   \ncd reference\n\n\n\n\n5.1 wget\nwget is one command that allows you to download files from the web (curl is another very commonly used one, with much the same functionality).\nTo download a file to our working directory, all you need is to tell wget about the URL (web address) to the file you want to download.\n\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/499/845/GCF_000499845.1_PhaVulg1_0/GCF_000499845.1_PhaVulg1_0_genomic.fna.gz\n\n\nOn Your Own: Preview a FASTA file\nTry previewing the contents of the reference genome file you just downloaded.\n\n\nHint (click here)\n\nRemember, the file is gzip-compressed. Use zcat and pipe the results to head.\n\n\n\nSolution (click here)\n\n\nzcat Pvulg.fa.gz | head\n\n\n\nOK, now we’ve got our raw data (FASTQ) and our reference genome (FASTA). This is a good start in terms of getting ready to start analyzing the data. One more thing we can do now is try to understand a little bit about the samples themselves. There is a tab-separated text file named meta.tsv in the data/meta directory. et’s take a look at its contents…\n\n\n5.2 less\nless is a command that opens up a text file within your shell. Once you’re finished viewing the file, type q to quit and return to your prompt."
  },
  {
    "objectID": "modules/05-shell2.html#a-list-of-commonly-used-commands",
    "href": "modules/05-shell2.html#a-list-of-commonly-used-commands",
    "title": "The Unix Shell - Part II",
    "section": "6 A List of Commonly-used Commands",
    "text": "6 A List of Commonly-used Commands\nBelow is a list of common Unix commands: some truly ubiquitous ones, as well as ones dealing with files and data (but I’m omitting, e.g., the many let’s say “system managent” commands).\nThey are grouped into some general categories:\n\nNavigating in the Terminal\n\npwd: returns (prints) your working directory\ncd: change working directory\n\nViewing Files\n\ncat: print the entire contents of a file\nhead: print the first lines of a file\ntail: print the last lines of a file\nless: view the contents of a file in a “pager” (press q to quit/exit!)\n\nManaging/Organizing Files\n\nls: list contents of directory\nmkdir: create a new directory\nrm: remove/delete a file or directory\ncp: copy files/directories to a new location\nmv: move/rename files/directories to a new location\n\nWorking With Compressed Files\n\ngzip/gunzip: compress/uncompress a file with gzip compression (.gz)\nunzip: uncompress a zip (.zip) file\nzcat: print the contents of a compressed file to the screen\n\nAssessing Files\n\nmd5/shasum: check file integrity via “checksums” (fingerprints) for a file\ngrep: search a text file for lines containing a pattern of text\nwc: return number of lines, words, characters in a file\n\nEditing Files (or other data)\n\nsort\nuniq\ncut\ntr\nsed\nawk\n\nObtaining/Sharing Files\n\ncurl: download a file from online\nwget: download a file from online\n\nGetting Info About a Command\n\nman: get help (manual) for a command\n\nShell Features\n\nTab completion\nCommand History (up arrow)\nCtrl+c\n\nSpecial Notation\n\n|\n~\n.\n..\n$PATH\n$HOME\n\nWildcards\n\n*\n?\n[]\n^\n\n\nWhile it’s not an exhaustive list, getting a grasp on the commands and features above will go a long way in allowing you to work in the Unix shell. We should see nearly all of these in action during our sessions."
  },
  {
    "objectID": "modules/05-shell2.html#further-resources",
    "href": "modules/05-shell2.html#further-resources",
    "title": "The Unix Shell - Part II",
    "section": "7 Further resources",
    "text": "7 Further resources\n\nOSC’s UNIX Basics\nhttps://www.learnenough.com/command-line-tutorial\nhttps://cvw.cac.cornell.edu/Linux/\nhttp://www.ee.surrey.ac.uk/Teaching/Unix/\nhttps://www.udacity.com/course/linux-command-line-basics--ud595\nhttp://moo.nac.uci.edu/~hjm/How_Programs_Work_On_Linux.html"
  },
  {
    "objectID": "modules/06_scripts.html",
    "href": "modules/06_scripts.html",
    "title": "Shell Scripting",
    "section": "",
    "text": "Shell scripts (or to be slightly more precise, Bash scripts) enable us to run sets of commands non-interactively. This is especially beneficial or necessary when a set of commands:\nScripts form the basis for analysis pipelines and if we code things cleverly, it should be straightforward to rerun much of our project workflow:"
  },
  {
    "objectID": "modules/06_scripts.html#setup",
    "href": "modules/06_scripts.html#setup",
    "title": "Shell Scripting",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/&lt;your-folder&gt; in the box Working Directory (replace &lt;your-folder&gt; by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    =&gt; Terminal =&gt; New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =&gt;   File   =&gt;   Open Folder and enter /fs/ess/scratch/PAS2250/&lt;your-folder&gt;."
  },
  {
    "objectID": "modules/06_scripts.html#script-header-lines-and-zombie-scripts",
    "href": "modules/06_scripts.html#script-header-lines-and-zombie-scripts",
    "title": "Shell Scripting",
    "section": "2 Script header lines and zombie scripts",
    "text": "2 Script header lines and zombie scripts\n\n2.1 Shebang line\nWe use a so-called “shebang” line as the first line of a script to indicate which language our script uses. More specifically, this line tell the computer where to find the binary (executable) that will run our script.\nSuch a line starts with #!, basically marking it as a special type of comment. After that, we provide the location to the relevant program: in our case Bash, which is located at /bin/bash on Linux and Mac computers.\n#!/bin/bash\nAdding a shebang line is good practice in general, and is necessary when we want to submit our script to OSC’s Slurm queue, which we’ll do tomorrow.\n\n\n\n2.2 Bash script settings\nAnother line that is good practice to add to your Bash scripts changes some default settings to safer alternatives. The following two Bash default settings are bad ideas inside scripts:\nFirst, and as we’ve seen in the previous module, Bash does not complain when you reference a variable that does not exist (in other words, it does not consider that an error).\nIn scripts, this can lead to all sorts of downstream problems, because you very likely tried and failed to do something with an actual variable. Even more problematically, it can lead to potentially very destructive file removal:\n\n# Using a variable, we try to remove some temporary files whose names start with tmp_\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*     # DON'T TRY THIS!\n\n\n# Using a variable, we try to remove a temporary directory\ntempdir=output/tmp\nrm -rf $tmpdir/*      # DON'T TRY THIS!\n\n\n\n\n\n\n\nThe comments above specified the intent we had. What would have actually happened?\n\n\n\n\n\nIn both examples, there is a similar typo: temp vs. tmp, which means that we are referencing a (likely) non-existent variable.\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nIn the second example, along similar lines, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem (recall that a leading / in a path is a computer’s root directory).1 (-r makes the removal recursive and -f makes forces removal).\n\n\n\n\n\nSecond, a Bash script keeps running after encountering errors. That is, if an error is encountered when running line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, you might not notice an error somewhere in the middle if it doesn’t produce more errors downstream. But the downstream results from what we at that point might call a “zombie script” may still be completely wrong.\n\nThe following three settings will make your Bash scripts more robust and safer. With these settings, the script terminates, with an appropriate error message, if:\n\nset -u — An unset (non-existent) variable is referenced.\nset -e — Almost any error occurs.\nset -o pipefail — An error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\n\nset -u -e -o pipefail     # (For in a script - don't run in the terminal)\n\nOr even more concisely:\n\nset -ueo pipefail         # (For in a script - don't run in the terminal)\n\n\n\n\n2.3 Our header lines as a rudimentary script\nLet’s go ahead and start a script with the header lines that we have so far discussed.\n\nInside your personal directory within /fs/ess/scratch/PAS2250/participants, make a directory called scripts and one called sandbox (e.g. mkdir scripts sandbox, or use the VS Code menus.\nOpen a new file in the VS Code editor (     =&gt;   File   =&gt;   New File) and save it as printname.sh within the newly created scripts dir.\n\n\n\n\n\n\nShell scripts, including Bash scripts, most commonly have the extension .sh\n\n\n\n\n\n\nType the following lines in that script (not in your terminal!):\n\n#!/bin/bash\nset -ueo pipefail\n\n\nAlready now, we could run (execute) the script. One way of doing this is calling the bash command followed by the name of the script2:\n\nbash scripts/printname.sh\n\nDoing this won’t print anything to screen (or file). Since our script doesn’t have any output, that makes sense — no output can be a good sign, because it means that no errors were encountered."
  },
  {
    "objectID": "modules/06_scripts.html#command-line-arguments-for-scripts",
    "href": "modules/06_scripts.html#command-line-arguments-for-scripts",
    "title": "Shell Scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Calling a script with arguments\nWhen you call a script, you can pass it command-line arguments, such as a file to operate on.\nThis is much like when you provide a command like ls with arguments:\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\n\nLet’s see what this would look like with our printname.sh script and a fictional script fastqc.sh:\n\n# Run scripts without any arguments:\nbash fastqc.sh                            # (Fictional script)\nbash scripts/printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash fastqc.sh data/sampleA.fastq.gz      # 1 argument, a filename\nbash scripts/printname.sh John Doe        # 2 arguments, strings representing names\n\nIn the next section, we’ll see what happens when we pass arguments to a script on the command line.\n\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments are automatically available in placeholder variables.\nA first argument will be assigned to the variable $1, any second argument will be assigned to $2, any third argument will be assigned to $3, and so on.\n\n\n\n\n\n\nIn the calls to fastqc.sh and printname.sh above, what are the placeholder variables and their values?\n\n\n\n\n\nIn bash fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash scripts/printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\n\n\n\n\n\n\nPlaceholder variables are not automagically used\n\n\n\nArguments passed to a script are merely made available in placeholder variables — unless we explicitly include code in the script to do something with those variables, nothing else happens.\n\n\nLet’s add code to our printname.sh script to “process” any first and last name that are passed to the script as command-line arguments. For now, our script will simply echo the placeholder variables, so that we can see what happens:\n\n#!/bin/bash\nset -ueo pipefail\n\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNext, we’ll run the script, passing the arguments John and Doe:\n\nbash scripts/printname.sh John Doe\n\nFirst name: John\nLast name: Doe\n\n\n\n\nOn Your Own: Command-line arguments\nIn each case below, think about what might happen before you run the script. Then, run it, and if you didn’t make a successful prediction, try to figure out what happened instead.\n\nRun the script (scripts/printname.sh) without passing arguments to it.\nDeactivate (“comment out”) the line with set settings by inserting a # as the first character. Then, run the script again without passing arguments to it.\nDouble-quote John Doe when you run the script, i.e. run bash scripts/printname.sh \"John Doe\"\n\nTo get back to where we were, remove the # you inserted in the script in step 2 above.\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\n\n\nbash scripts/printname.sh\n\n\nprintname.sh: line 4: $1: unbound variable\n\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\n\n\nbash scripts/printname.sh\n\nFirst name: \nLast name: \n\n\nBeing commented out, the set line should read:\n\n#set -ueo pipefail\n\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\n\n\nbash scripts/printname.sh \"John Doe\"\n\nFirst name: John Doe\nLast name: \n\n\n\n\n\n\n\n\n3.3 Descriptive variable names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables as follows:\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nUsing descriptively named variables in your scripts has several advantages. It will make your script easier to understand for others and for yourself. It will also make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name\n$# contains the number of command-line arguments passed\n\n\n\n\n\nOn Your Own: A script to print a specific line\nWrite a script that prints a specific line (identified by line number) from a file.\n\nOpen a new file and save it as scripts/printline.sh\nStart with the shebang and set lines\nYour script takes two arguments: a file name ($1) and a line number ($2)\nCopy the $1 and $2 variables to descriptively named variables\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the top solution box.\nTest the script by printing line 4 from data/meta/meta.tsv.\n\n\n\n\n\n\n\nSolution: how to print a specific line number\n\n\n\n\n\nFor example, to print line 4 of data/meta/meta.tsv directly:\n\nhead -n 4 data/meta/meta.tsv | tail -n 1\n\nJust note that in the script, you’ll be using variables instead of the “hardcode values” 4 and data/meta/meta.tsv.\nHow this command works:\n\nhead -n 4 data/meta/meta.tsv will print the first 4 lines of data/meta/meta.tsv\nWe pipe those 4 lines into the tail command\nWe ask tail to just print the last line of its input, which will in this case be line 4 of the original input file.\n\n\n\n\n\n\n\n\n\n\nFull solution\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\nTo run the script and make it print the 4th line of meta.tsv:\n\nbash scripts/printline.sh data/meta/meta.tsv 4\n\nSRR7609471  beach   control 3   40982374    78.70"
  },
  {
    "objectID": "modules/06_scripts.html#script-variations-and-improvements",
    "href": "modules/06_scripts.html#script-variations-and-improvements",
    "title": "Shell Scripting",
    "section": "4 Script variations and improvements",
    "text": "4 Script variations and improvements\n\n4.1 A script to serve as a starting point\nWe’ve learned that the head command prints the first lines of a file, whereas the tail command prints the last lines. Sometimes it’s nice to be able to quickly see both ends of a file, so let’s write a little script that can do that for us, as a starting point for the next few modifications.\nOpen a new file, save it as scripts/headtail.sh, and add the following code to it:\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\n\nhead -n 2 \"$input_file\"\necho \"---\"\ntail -n 2 \"$input_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNext, let’s run our headtail.sh script:\n\nbash scripts/headtail.sh data/meta/meta.tsv\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n\n4.2 Redirecting output to a file\nSo far, the output of our scripts was printed to screen, e.g.:\n\nIn printnames.sh, we simply echo’d, inside sentences, the arguments passed to the script.\nIn headtail.sh, we printed the first and last few lines of a file.\n\nAll this output was printed to screen because that is the default output mode of Unix commands, and this works the same way regardless of whether those commands are run directly on the command line, or are run inside a script.\nAlong those same lines, we have already learned that we can “redirect” output to a file using &gt; (write/overwrite) and &gt;&gt; (append) when we run shell commands — and this, too, works exactly the same way inside a script.\n\nWhen working with genomics data, we commonly have files as input, and new/modified files as output. Let’s practice with this and modify our headtail.sh script so that it writes output to a file.\nWe’ll make the following changes:\n\nWe will have the script accept a second argument: the output file name3.\nWe will redirect the output of our head, echo, and tail commands to the output file. We’ll have to append (using &gt;&gt;) in the last two commands.\n\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\noutput_file=$2\n\nhead -n 2 \"$input_file\" &gt; \"$output_file\"\necho \"---\" &gt;&gt; \"$output_file\"\ntail -n 2 \"$input_file\" &gt;&gt; \"$output_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNow we run the script again, this time also passing the name of an output file:\n\nbash scripts/headtail.sh data/meta/meta.tsv sandbox/samples_headtail.txt\n\nThe script will no longer print any output to screen, and our output should instead be in sandbox/samples_headtail.txt:\n\n# Check that the file exists and was just modified:\nls -lh sandbox/samples_headtail.txt\n\n-rw-r--r--@ 1 poelstra.1  staff   197B Jul 19 12:15 sandbox/samples_headtail.txt\n\n\n\n# Print the contents of the file to screen\ncat sandbox/samples_headtail.txt\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n\n4.3 Report what’s happening\nIt is often useful to have your scripts “report” or “log” what is going on. Let’s keep thinking about a script that has file(s) as the main output, but instead of having no output printed to screen at all, we’ll print some logging output to screen. For instance:\n\nWhat is the date and time\nWhich arguments were passed to the script\nWhat are the output files\nPerhaps even summaries of the output.\n\nAll of this can help with troubleshooting and record-keeping.4 Let’s try this with our headtail.sh script.\n\n#!/bin/bash\nset -ueo pipefail\n\n## Copy placeholder variables\ninput_file=$1\noutput_file=$2\n\n## Initial logging \necho \"Starting script $0\"           # Print name of script\ndate                                # Print date & time\necho \"Input file:   $input_file\"\necho \"Output file:  $output_file\" \necho                                # Print empty line to separate initial & final logging\n\n## Print the first and last two lines to a separate file\nhead -n 2 \"$input_file\" &gt; \"$output_file\"\necho \"---\" &gt;&gt; \"$output_file\"\ntail -n 2 \"$input_file\" &gt;&gt; \"$output_file\"\n\n## Final logging\necho \"Listing the output file:\"\nls -lh \"$output_file\"\necho \"Done with script $0\"\ndate\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nA couple of notes about the lines that were added to the script above:\n\nPrinting the date at the end of the script as well will allow you to check for how long the script ran, which can be informative for longer-running scripts.\nPrinting the input and output files (and the command-line arguments more generally) can be particularly useful for troubleshooting\nWe printed a “marker line” like Done with script, indicating that the end of the script was reached. This is handy due to our set settings: seeing this line printed means that no errors were encountered.\nI also added some comment headers like “Initial logging” to make the script easier to read, and such comments can be made more extensive to really explain what is being done.\n\nLet’s run the script again:\n\nbash scripts/headtail.sh data/meta/meta.tsv sandbox/tmp.txt\n\nStarting script scripts/headtail.sh\nWed Jul 19 12:15:57 EDT 2023\nInput file:   data/meta/meta.tsv\nOutput file:  sandbox/tmp.txt\n\nListing the output file:\n-rw-r--r--@ 1 poelstra.1  staff   197B Jul 19 12:15 sandbox/tmp.txt\nDone with script scripts/headtail.sh\nWed Jul 19 12:15:57 EDT 2023\n\n\nThe script printed some details for the output file, but not its contents (that would have worked here, but is usually not sensible when working with genomics data). Let’s take a look, though, to make sure the script worked:\n\ncat sandbox/tmp.txt      # \"cat\" prints all of a file's contents\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n\n\n\n\necho, echo\n\n\n\nThe extensive reporting (echo-ing) may have seemed silly for our little script, but fairly extensive reporting (as well as testing, but that’s outside the scope of this workshop) can be very useful — and will be eventually a time-saver.\nThis is especially true for long-running scripts, or scripts that you often reuse and perhaps share with others.\n\n\n\n\nOn Your Own: A fanciful script\nModify your printline.sh script to:\n\nRedirect output to a file\nThis output file should not be “hardcoded” in the script, but its name should be passed as an argument to the script, like we did above with headtail.sh\nAdd a bit of reporting — echo statements, date, etc, along the lines of what we did above with headtail.sh\nAdd some comments to describe what the code in the script is doing\n\n\n\n\n\n\n\nThe original printline.sh script\n\n\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\n\n\n\n\n\n\n\n\n\n\n(One possible) solution\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n\n## Copy placeholder variables\ninput_file=$1\noutput_file=$2\nline_nr=$3\n\n## Initial logging \necho \"Starting script $0\"           # Print name of script\ndate                                # Print date & time\necho \"Input file:   $input_file\"\necho \"Output file:  $output_file\"\necho \"Line number:  $line_nr\"\necho                                # Print empty line to separate initial & final logging\n\n## Print 1 specific line from the input file and redirect to an output file\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1 &gt; $output_file\n\n## Final logging\necho \"Listing the output file:\"\nls -lh \"$output_file\"\necho \"Done with script $0\"\ndate\n\nTo run the script with the additional argument:\n\nbash scripts/printline.sh data/meta/meta.tsv sandbox/meta_line.tsv 4\n\nStarting script scripts/printline.sh\nWed Jul 19 12:15:57 EDT 2023\nInput file:   data/meta/meta.tsv\nOutput file:  sandbox/meta_line.tsv\nLine number:  4\n\nListing the output file:\n-rw-r--r--@ 1 poelstra.1  staff    42B Jul 19 12:15 sandbox/meta_line.tsv\nDone with script scripts/printline.sh\nWed Jul 19 12:15:57 EDT 2023"
  },
  {
    "objectID": "modules/06_scripts.html#footnotes",
    "href": "modules/06_scripts.html#footnotes",
    "title": "Shell Scripting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut note that at OSC, you would not be able to remove anything you’re not supposed to, since you don’t have the permissions to do so. On your own computer, this could be more genuinely dangerous, though even there, you would not be able to remove the operating system without specifically requesting “admin” rights.↩︎\nBecause our script has a shebang line, we could also execute the script without the bash command using ./printname.sh. However, this would also require us to “make the script executable”, which is beyond the scope of this workshop.↩︎\nOf course, we could also simply write the output to a predefined (“hardcoded”) file name such as out.txt, but in general, it’s better practice to keep this flexible via an argument.↩︎\nWe’ll see in the upcoming SLURM module that we when submit scripts to the OSC queue (rather than running them directly), the output of scripts that is normally printed to screen, will instead go to a sort of “log” file. So, your script’s reporting will end up in this file.↩︎"
  }
]