{
  "hash": "a515bf5269200e09991c9e8dc3bbb5c0",
  "result": {
    "markdown": "---\ntitle: \"Intro to the Ohio Supercomputer Center (OSC)\"\npagetitle: \"OSC Intro\"\nhighlight-style: github\nnumber-sections: true\nauthor: Jelmer Poelstra\ndate: 2023-07-18\n---\n\n::: {.cell}\n\n:::\n\n\n-----\n\n<p align=\"center\"><img src=img/osc_logo.png width=\"70%\"></p>\n\n<br>\n\n## Introduction\n\nThis session will provide an introduction to supercomputers in general and to\nthe Ohio Supercomputer Center (OSC) specifically.\n\n### Supercomputers\n\nA **supercomputer** (also known as a \"compute cluster\" or simply a \"**cluster**\")\nconsists of many computers that are connected by a high-speed network,\nand that can be accessed remotely by its users.\nIn more general terms,\nsupercomputers provide high-performance computing (**HPC**) resources.\n\nHere are some possible reasons to use a supercomputer instead of your own laptop\nor desktop:\n\n- Your analyses take a long time to run.\n- You analyses need large numbers of CPUs or a large amount of memory.\n- You need to run some analyses many times.\n- You need to store a lot of data.\n- Your analyses require specialized hardware, such as GPUs.\n- Your analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you're working RNAseq data or other kinds of genomic data,\nmany of these reasons apply.\nThis can make it hard or simply impossible to do all your\nwork on your personal workstation, and supercomputers provide a solution.\n\n### The Ohio Supercomputer Center (OSC)\n\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio\n(not The Ohio State University).\nIt has two supercomputers, lots of storage space, and an excellent infrastructure\nfor accessing these resources.\nAt least for folks at OSU, using OSC is currently usually [**free in practice**](#administrative-miscellaneae).\nHaving such a good HPC resource available at no cost is not something we should take\nfor granted &mdash;\nat many institutions,\nacademics not only have have to pay for these kinds of resources,\nbut those are often more limited and not as easy to access.\n\nIn **upcoming sessions**, we'll continue to work at OSC,\nso you will get a fair bit of experience with using it.\nWe'll also have specific sessions dedicated to\n[using VS Code at OSC](03_vscode.qmd),\n[loading and installing software at OSC](07_software.qmd) and\n[using the SLURM job scheduler](08_slurm.qmd).\n\nOSC has **three main websites**:\n\n- <https://osc.edu>: OSC's general website, with lots of information about the\n  supercomputers, the software that's installed, and how to use OSC.\n- <https://ondemand.osc.edu>: A web portal to use OSC resources through your browser (login needed).\n- <https://my.osc.edu>: A site to manage your account and OSC Projects you are an admin for (login needed).\n\n<br>\n\n## The Structure of a Supercomputer Center\n\nLet's start with some terminology, going from smaller things to bigger things:\n\n- **Core / Processor / CPU / Thread** &mdash;\n  Components of a computer that can each (semi-)indendepently be asked to perform\n  a computing task like running a bioinformatics program.\n  While these terms are not technically all synonyms, we can treat them as such for our purposes.\n- **Node** &mdash;\n  A single computer that is a part of a supercomputer and has dozens of cores\n  (i.e., they tend to be more powerful than a personal laptop or desktop).\n- **Supercomputer / Cluster** &mdash;\n  Many computers connected by a high-speed network.\n  (\"Pitzer\" and \"Owens\" are the two currently active ones at OSC.)\n- **Supercomputer Center** &mdash;\n  A facility like OSC that has one or more supercomputers.\n\n<p align=\"center\"><img src=img/terminology.png width=\"95%\"></p>\n\nThis is what the **Owens** supercomputer at OSC physically looks like:\n\n<p align=\"center\"><img src=img/owens.jpg width=\"70%\"></p>\n\n::: {.callout-note}\n#### Linux and Unix\nLike the vast majority of supercomputers,\nOSC's run on the **Linux operating system** (as opposed to on MacOS or Windows).\nIn turn, Linux is a **Unix-based** operating system like MacOS (but unlike Windows).\n:::\n\nWe can think of a supercomputer as having three main parts:\n\n- **File Systems**: Where files are stored (these are shared between the two clusters!)\n- **Login Nodes**: The handful of computers everyone shares after logging in\n- **Compute Nodes**: The many computers you can reserve to run your analyses\n\n<p align=\"center\"><img src=img/cluster_overview_ed.png width=\"95%\"></p>\n\nLet's take those in order.\n\n<br>\n\n### File Systems\n\nThere are 4 main file systems where you can store files at OSC:\nHome Directories, Project Directories, Scratch Directories, \nand Compute storage.\n\n| File system   | Located within        | Quota                 | Backed up?    | Auto-purged?          | One for each... |\n|---------------|-----------------------|-----------------------|---------------|-----------------------|-----------------|\n| **Home**      | `/users/`             | 500 GB / 1 M files    | Yes           | No                    | User            |\n| **Project**   | `/fs/ess/` [^1]       | Flexible              | Yes           | No                    | OSC Project     |\n| **Scratch**   | `/fs/scratch/` [^2]   | 100 TB                | No            | After 90 days         | OSC Project     |\n| **Compute**   | `$TMPDIR`             | 1 TB                  | No            | After job completes   | Compute job     |\n\n[^1]: Or `/fs/project/`\n[^2]: Or `/fs/ess/scratch`\n\nYou'll interact most with the **Project directories**:\nthis is because for most files, you'll want a permanent and backed-up location\n(i.e., not Scratch or Compute storage),\nand the Home directory offers relatively limited storage as well as challenges\nwith file sharing.\n\n::: {.callout-note}\n#### Unix terminology and environment variables\nWe'll talk about all of this more in upcoming sessions,\nbut to clarify some of the terms and concepts mentioned here:\n\n- \"**Directory**\" (or \"**dir**\" for short) is a commonly used term in Unix that\n  just means \"folder\".\n- In the \"Located within\" column in the table above,\n  the leading forward slash **`/`** signifies the system's\n  **\"root\" (top-level) directory**,\n  and forward slashes are also used to _separate directories_\n  (unlike in Windows, which uses backslashes).\n- File and directory locations on a computer are often referred to as \"**paths**\".\n- `$TMPDIR` is a so-called \"**environment variable**\" that contains the path\n  to the Compute storage directory\n  (in the Unix shell, all _variables_ are referenced by putting a `$` before their name,\n  and _environment_ variables are in all-caps).\n  A variable is useful in this case,\n  because the location of this storage space will vary depending on the compute\n  node at which it's located.\n  Along similar lines, your Home directory's path is stored in `$HOME`.\n:::\n\n#### Home Directory\n\nWhen you initially get an account with OSC, a Home directory is created for you,\nnamed with your OSC username.\nThis directory will always be within `/users/`, and then,\nsomewhat strangely, in a directory containing the name of the OSC Project\nyou were first added to \n(and this will not change even if you're no longer a member of that project,\nor if that project ceases to exist).\nFor example, my Home directory is `/users/PAS0471/jelmer`.\n\nYou will only ever have **one** Home directory.\nYou also **cannot expand** the standard 500 GB of storage &mdash;\nif you need more space,\nyou should turn to your _Project_ directories.\n\nIf possible,\nI recommend to use your Home directory only for some **general files**\n(like some software, tutorials and practice, general scripts and databases),\nand to use Project directories for all your research project data and results.\n\n#### Project Directories\n\nProject directories are linked to OSC projects, which are typically set up by PIs.\nThey offer **flexibility** in terms of the amount of storage available,\nand also in terms of who can access files in the directory.\n\nBy default, all members of an OSC Project have \"read access\"\n(the ability to see and copy files) for all files in a project directory,\nwhich makes it **suitable for collaborating** on a research project.\nBut rest assured:\nexcept for OSC staff, other people can never move, modify, or delete your files \n(i.e., they don't have \"write access\") &mdash;\nnot even the admins (PIs) for the OSC Project in question.\n\nLike Home directories, Project directories are **backed up** daily.\nYou don't have _direct_ access to the backups,\nbut if you've accidentally deleted some important files (Linux has no thrash bin!),\nyou can request them to be restored to the way they were on a specific date.\n\n::: {.callout-note}\n#### File Systems are shared among the clusters\nWhile OSC's current two clusters, **Owens** and **Pitzer**,\nare largely separate, they do share the same File System.\nThis means that you can access your files in the exact same way regardless\nof which supercomputer you have connected to.\n\nFor example, your Home directory can be accessed using the same path\n(in my case, `/users/PAS0471/jelmer`) on Pitzer and Owens.\n:::\n\n#### Temporary storage: Scratch and Compute\n\nEvery OSC Project also has a Scratch directory.\nThe two main advantages of Scratch space are that it is **effectively unlimited** and\nthat it has **faster** data read and write (\"I/O\") speed than Home and Project space.\nHowever, it's not backed up,\nand files that are unmodified for 90 days are automatically deleted.\nAs such, Scratch storage is mostly useful for intermediate results that are\nlikely not needed again and can be reproduced easily.[^4]\n\n[^4]: For example, many genome and transcriptome assemblers output a lot of data,\n      but you will only need a few files (like the assembly) for your next steps.\n\n**Compute** storage space is even more fleeting:\nas soon as the compute \"job\" in question has stopped (e.g. your script has finished),\nthese files will be deleted.\nWe'll talk a bit more about this type of storage later,\nas using them can save time for I/O-intensive analyses.\n\n<br>\n\n### Login Nodes\n\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer.\nThere are only a handful of them on each supercomputer,\nand they are shared among everyone and cannot be \"reserved\".\n\nAs such, login nodes are meant only to do things like organizing your files\nand creating scripts for compute jobs, and are **_not_ meant for any serious computing**.\n\nAttempting large computing efforts on these nodes risks taxing the limited resources\non these nodes, and bogging things down for everyone.\nThere are checks built in that limit what you are able to do on the login nodes\n(i.e. jobs running for longer than 20 minutes will be killed),\nbut it's best to just not push it at all.\nAny serious computing should be done on the compute nodes.\n\n<br>\n\n### Compute Nodes\n\nCompute nodes are really the powerhouse of the supercomputer,\nand this is where you run your data processing and analysis.\n\nYou can use compute nodes by putting in requests for resources,\nsuch as the number of nodes, cores, and for how long you will need them.\nBecause many different users are sending such requests\n--i.e., for \"compute jobs\" or just \"**jobs**\"--\nall the time, there is software called a **job scheduler**\n(specifically, _Slurm_ in case of OSC)\nthat considers each request and\nassigns the necessary resources to the job as they become available.\n\n::: {.callout-note}\n#### Interactive and batch use of compute nodes\n\nRequests for compute node jobs can be made either through the OnDemand website\nor with commands like `sinteractive` and `sbatch`.\n\nFor instance, when we start an RStudio session at OSC,\nwe first have to fill out a little form with such a request,\nand then RStudio will run on a compute node.\nThis is an example of using a compute node **interactively** &mdash;\n\"you\" are located on a compute node,\nand any R command you type will be executed there.\nMore commonly for genomics work,\nyou'll be using compute nodes **non-interactively, that is, through \"batch jobs\"**.\nWhen doing so, you will write a script in advance and send it to the job scheduler,\nwhich will run the script on a compute node that \"you\" don't go to at all.\n\nThe session [Compute Jobs with Slurm](08_slurm.qmd) is dedicated to this topic.\n:::\n\n**Compute nodes come in different shapes and sizes.**\n_\"Standard nodes\"_ are by far the most numerous\n(e.g., Owens has 648 and Pitzer has 564 of them) and even those vary in size,\nfrom 28 cores per node (Owens) to 48 cores per node (the \"expansion\" part of Pitzer).\nSome examples of other types of nodes are ones with extra memory (`largemem` and `hugemem`)\nand ones that provide access to GPUs (Graphical Processing Units) rather than CPUs.\n\nFortunately, you don't tend to have to think much about node types as you start using OSC,\nsince Standard nodes are automatically picked by default,\nand those will serve you well for the vast of majority genomics analysis.[^3]\n\n[^3]: Some examples where you might need a different type of node are\n      genome or transcriptome assembly where you might need nodes with a lot of memory,\n      or Oxford Nanopore sequence data basecalling where you might need GPUs.\n\n::: {.callout-caution}\n#### Memory versus storage\nWhen we talk about \"memory\", this refers to RAM:\nthe data that your computer has actively \"loaded\" or in use.\nFor example, if you play a computer game or have many browser tabs open,\nyour computer's memory will be heavily used.\nGenomics programs sometimes load all of the input data from disk to memory for\nfast access, or hold a huge assembly graph in memory,\nand as such may need a lot of memory as well.\n\nDon't confuse memory with file storage, the data that is on disk,\nsome of which may have been unused for years.\n:::\n\n<br>\n\n### Putting it together\n\nAll these parts are connected together to create a supercomputer &mdash;\nfor example, let's take a look at the specs for Owens now that we understand\nthe components a bit better:\n\n<p align=\"center\"><img src=img/owens_specs.png width=\"95%\"></p>\n\n<br>\n\n## Connecting to OSC with OnDemand\n\nThe classic way of connecting to supercomputers is using SSH,\nlike with the `ssh` command in a Unix shell on your computer\n(see [this reference page](../info/osc_ssh.qmd)).\nHowever, OSC has pioneered the use of a **web portal called OnDemand**,\nwhich has since become more widely used among supercomputer centers.\n\nThe OSC OnDemand website, <https://ondemand.osc.edu>,\nthen, allows you to access OSC resources through a web browser.\nWhen you go there, you first need to **log in** with your OSC (not OSU!) credentials.\nAfter that, you should see a landing page similar to the one below:\n\n<p align=\"center\"><img src=img/ondemand_home.png width=\"90%\"></p>\n\nThe main part of the page (below the logo)\nonly contains some general OSC messages and updates &mdash;\nwhat we will focus on instead are some of the options in the **blue bar along the top**.\n\n<br>\n\n### File System Access\n\nLet's start with **Files**.\nHovering over this dropdown menu gives a list of directories you have access to.\nIf you're account is new, you might only have three:\na Home directory, and a Project and Scratch directory for one OSC Project.\n\nFor every project you're associated with, directories are added &mdash;\nI'm associated with quite a few different projects, so I have a long list under Files.\nI'll select the Project directory for the MCIC's main OSC Project, `PAS0471`,\nwhich is `/fs/ess/PAS0471`:\n\n<p align=\"center\"><img src=img/ondemand_files.png width=\"30%\"></p>\n  \nOnce there, I can see a list of directories and files inside this Project directory,\nand I can click on the directories to explore the contents further.\n  \n<p align=\"center\"><img src=img/ondemand_projectdir.png width=\"90%\"></p>\n\nThis interface is **much like the file browser on your own computer**,\nso you can also create, delete, move and copy files and folders:\nsee the buttons across the top.\n\nAdditionally,\nthere are **Upload and Download buttons** for uploading files from your computer to OSC,\nand downloading them from OSC to your computer.\nThese are only suitable for relatively small transfers, roughly below 1 GB.\nOther options to transfer files to and from OSC are remote transfer commands\nlike `scp` (also for smaller transfers), and SFTP or Globus for larger transfers.\nTo learn more about these options,\nsee the [**reference page on OSC file transfer**](../info/osc_transfer.qmd).\n\n::: {.callout-note}\nWe will skip the \"**Jobs**\" dropdown menu in the blue top bar,\nbecause in later sessions,\nwe will learn to create, submit, and monitor compute jobs at the command line\ninstead, which quickly becomes more efficient as you get the hang of it.\n:::\n\n<br>\n\n### System Status (in Clusters)\n\nMoving on to \"**Clusters**\", \nwe'll start with the item at the bottom of that dropdown menu, \"**System Status**\":\n\n<p align=\"center\"><img src=img/ondemand_systemstatus_select.png width=\"50%\"></p>\n\nThis page shows an overview of the current usage of the two clusters,\nwhich might help to decide which cluster you want to use and\nset some expectations for compute job waiting times:\n\n<p align=\"center\"><img src=img/ondemand_systemstatus.png width=\"90%\"></p>\n\n<br>\n\n### Unix Shell Access (in Clusters)\n\nInteracting with a supercomputer in a point-and-click manner only goes so far.\nUsing a supercomputer effectively requires interacting with the system\nusing a command-line (**CLI**) rather than a graphical user (**GUI**) interface.\n\nAgain under the **Clusters** option in the blue top bar,\nyou can access a Unix shell either on Owens or Pitzer:\n\n<p align=\"center\"><img src=img/ondemand_shell_select.png width=\"50%\"></p>\n  \nI'm selecting a shell on the Pitzer supercomputer,\nwhich will open a new browser tab looking like this:\n\n<p align=\"center\"><img src=img/ondemand_shell2.png width=\"95%\"></p>\n\nWe most commonly interact with a supercomputer using a Unix shell,\nand we'll learn about the basics of doing so in an [upcoming session](04_shell.qmd).\nHowever, we'll mostly be accessing a Unix shell in a different manner,\nnamely **inside the VS Code text editor**,\nwhich also gives us some additional functionality in a user-friendly way.\n\n### Interactive Apps\n\nWe can get access to VS Code,\nas well as many other programs with GUIs such as RStudio,\nvia the **Interactive Apps** dropdown menu\n(and the menu item next to that, **My Interactive Sessions**,\nwill list the sessions that are currently active as well as finished ones).\n\n<p align=\"center\"><img src=img/ondemand_vscode_select.png width=\"32%\"></p>\n\n::: {.callout-note}\n#### 'VS Code' versus 'Code Server'  \nIn the list, the \"VS Code\" program is called \"Code Server\",\nmuch like \"RStudio\" is called \"RStudio Server\".\nThey are the same programs but with minor edits to allow them to run remotely\nin a browser rather than as locally installed on your own computer.\n:::\n\n\"Interactive Apps\" like VS Code and RStudio **run on compute nodes** &mdash;\ntherefore, we need to fill out a form and specify some details for our\ninteractive compute job request:\n\n- The **_OSC Project_** that should be billed for the compute resource usage &mdash;\n  a dropdown menu will list all Projects you are a member of.\n- The **_amount of time_** we want to make a reservation for &mdash;\n  we'll be kicked off as soon as that amount of time has passed!\n- The **\"_working directory_\"** (starting location in the file system) for the program,\n  which we can type in the box _or_ select with the \"Select Path\" button\n  (the default is your Home directory, here referred to as `$HOME`).\n- The **software version** &mdash;\n  the most recent available one should be automatically selected,\n  and that is almost always what you'll want.\n\n<p align=\"center\"><img src=img/ondemand_vscode_form.png width=\"60%\"></p>\n \nClick on **Launch** at the bottom and this will send your request to the compute\njob scheduler.\nFirst, your job will be \"Queued\" &mdash; that is, waiting for the job scheduler\nto allocate resources on the compute nodes to it:\n\n<p align=\"center\"><img src=img/ondemand_vscode_queued.png width=\"90%\"></p>\n\nIn general, it should be granted resources within a few seconds\n(the card will then say \"Starting\"),\nand be ready for usage (\"Running\") in another couple of seconds:\n\n<p align=\"center\"><img src=img/ondemand_vscode_running.png width=\"90%\"></p>\n\nThen, you can click on the blue **Connect  to VS Code** button\nto open a new browser tab that runs VS Code.\nWe'll explore VS Code in the [next session](03_vscode.qmd).\n\n<br>\n\n## In Closing\n\n### Administrative miscellaneae\n\n- **Requesting & managing OSC Projects, and user accounts**  \n  Generally, only PIs request **OSC projects**, and they typically manage them as well.\n  OSC has [this page with more information on how to do so](https://www.osc.edu/supercomputing/support/account).\n  Whoever manages an OSC Project can add both existing OSC users and new users\n  to the Project, which will provide these users with access the project's\n  Project and Scratch directories,\n  and the ability to specify this Project in association with compute node resource requests.\n  \n  When you get added to an OSC Project and don't yet have an OSC **account**,\n  you will automatically receive an email with a link that allows you to create an account.\n  It is not possible to create an account before having been added to an OSC Project.\n\n- **Billing**  \n  OSC will bill OSC Projects (not individual users),\n  and only for the following two things:\n\n  - _**File storage**_ in the Project Storage file system\n  - _**Compute node usage**_ by \"core hour\" (e.g. using 2 cores for 2 hours = 4 core hours)\n  \n  The prices for academic usage are quite low\n  (see [this page for specifics](https://www.osc.edu/content/academic_fee_model_faq)),\n  and importantly, at OSU, they are often _covered at the department level_ such that\n  individual PIs do not have to directly pay for this at all.\n\n- When you use OSC, it's good practice to **acknowledge and cite OSC** in your papers,\n  see their [citation page](https://www.osc.edu/resources/getting_started/citation). \n\n- For many questions such as if you have problems with your account,\n  have problems installing or using specific software,\n  or don't understand why your jobs keep failing,\n  you can **email OSC at <oschelp@osc.edu>**.\n  They are usually very quick to respond!\n\n<br>\n\n### Upcoming sessions on OSC\n\nToday, we have learned some of the basics of supercomputers and of\naccessing OSC. In separate sessions in this series, we will look at:\n\n- Using specific \"**Interactive Apps**\" (GUI-based programs):\n  - [_VS Code_](03_vscode.qmd)\n  - _RStudio_ (TBA)\n- [Loading and installing command-line software at OSC](07_software.qmd)\n- [Submitting batch jobs using the SLURM scheduler](08_slurm.qmd)\n\nAdditionally, there are pages with reference material (see the right side of the top\nmenu bar of this side) on:\n\n - [File transfer to and from OSC](../info/osc_transfer.qmd)\n - [Using OSC with SSH](../info/osc_ssh.qmd) (rather than through OnDemand)\n\n<br>\n\n### OSC's learning resources\n\nTo learn more about OSC, I would start with these short courses:\n\n- **[OSC's online asynchronous courses](https://www.osc.edu/supercomputing/training)**\n  - This includes a number of short videos\n  - When I tried to access these last, it wasn't always clear where to go\n    _after enrolling_, and one of the two courses had even diseappeared from the list.\n    But the website <https://scarlet.instructure.com> then listed the courses and\n    provided access.\n\nThis series of pages is also useful:\n\n- [New User Resource Guide](https://www.osc.edu/resources/getting_started/new_user_resource_guide)\n- [Supercomputing FAQ](https://www.osc.edu/resources/getting_started/supercomputing_faq)\n- [HOWTOs](https://www.osc.edu/resources/getting_started/howto) (tutorials on specific topics)\n- [Info on batch (non-interactive) compute jobs](https://www.osc.edu/supercomputing/batch-processing-at-osc) (rather technical)\n- [OSC \"events\" such as Office Hours](https://www.osc.edu/events)\n\n<br>\n\n### Acknowledgements\n\nThis page uses material from an\n[OSC Introduction written by Mike Sovic](https://mcic-osu.github.io/cl-workshop-22/modules/02-osc.html)\nand from OSC's Kate Cahill [Software Carpentry introduction to OSC](https://khill42.github.io/OSC_IntroHPC>)\n(outdated).\n\n<br>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}