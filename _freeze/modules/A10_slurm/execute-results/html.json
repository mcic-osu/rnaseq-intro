{
  "hash": "cd5ad5771d01ce2dc8995c356e5cc6fe",
  "result": {
    "markdown": "---\ntitle: \"Submitting and monitoring Slurm batch jobs\"\npagetitle: \"Slurm\"\nhighlight-style: github\nnumber-sections: true\nengine: knitr\nauthor: Jelmer Poelstra\ndate: 2023-09-15\n---\n\n::: {.cell}\n\n:::\n\n\n----\n\n<br>\n\n## Overview & setting up {-}\n\nWe have so far been working interactively at OSC,\nissuing our commands directly in the terminal.\nBut in order to run some actual genomics analyses,\nwe will want to run scripts non-interactively and submit them to the compute job\nqueue at OSC.\n  \nAutomated scheduling software allows hundreds of people with different\nrequirements to access compute nodes effectively and fairly.\nFor this purpose, OSC uses the **Slurm** scheduler\n(Simple Linux Utility for Resource Management).\n\nA temporary reservation of resources on compute nodes is called a **compute job**.\nWhat are the options to start a compute job at OSC?\n\n1. \"**Interactive Apps**\" &mdash; We can start programs with GUIs,\n   such as VS Code, RStudio or Jupyter Notebook on the OnDemand website,\n   and they will run in a browser window.\n2. **Interactive shell jobs** &mdash; Start a interactive shell on a compute node.\n3. **Batch (non-interactive) jobs** &mdash; Run a script on a compute node\n   without ever going to that node yourself.\n\nWe've already worked a lot with the VS Code Interactive App,\nand the at-home reading at the bottom of this page will talk about interactive\nshell jobs.\nBut what we're most interested in here is running **batch jobs**,\nwhich will be the focus of this session.\n\n#### Start VS Code and open your folder {-}\n\nAs always, we'll be working in VS Code &mdash;\nif you don't already have a session open, see below how to do so.\n\n**Make sure to open your `/fs/ess/PAS0471/<user>/rnaseq_intro` dir**,\neither by using the `Open Folder` menu item,\nor by clicking on this dir when it appears in the `Welcome` tab.\n\n:::{.callout-tip collapse=\"true\"}\n## Starting VS Code at OSC - with a Terminal (Click to expand)\n1. Log in to OSC's OnDemand portal at <https://ondemand.osc.edu>.\n\n2. In the blue top bar, select `Interactive Apps`\n   and then near the bottom of the dropdown menu, click `Code Server`.\n\n3. In the form that appears on a new page:\n   - Select an appropriate OSC project (here: `PAS0471`)\n   - For this session, select `/fs/ess/PAS0471` as the starting directory\n   - Make sure that `Number of hours` is at least `2`\n   - Click `Launch`.\n\n4. On the next page, once the top bar of the box has turned green\n   and says `Runnning`, click `Connect to VS Code`.\n\n<figure><p align=\"center\"><img src=img/osc-code-launch_ed.png width=\"80%\"></p></figure>\n\n5. Open a Terminal by clicking\n   &nbsp; {{< fa bars >}} &nbsp; => `Terminal` => `New Terminal`.\n   (Or use one of the keyboard shortcuts:\n   <kbd>Ctrl</kbd>+<kbd>\\`</kbd> (backtick) or\n   <kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>C</kbd>.)\n\n6. In the `Welcome` tab under `Recent`,\n   you should see your `/fs/ess/PAS0471/<user>/rnaseq_intro` dir listed:\n   click on that to open it.\n   Alternatively, use\n   &nbsp; {{< fa bars >}} &nbsp; => &nbsp; `File` &nbsp; => &nbsp; `Open Folder`\n   to open that dir in VS Code.\n:::\n\n:::{.callout-warning collapse=\"true\"}\n#### Don't have your own dir with the data? (Click to expand)\nIf you missed the last session, or deleted your `rnaseq_intro` dir entirely,\nrun these commands to get a (fresh) copy of all files you should have so far:\n\n```bash\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\n```\n\nAnd if you do have an `rnaseq_intro` dir,\nbut you want to start over because you moved or removed some of the files\nwhile practicing, then delete the dir before your run the commands above:\n\n```bash\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\n```\n\nYou should have at least the following files in this dir:\n\n```{.bash-out}\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── md5sums.txt\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md\n│   └── ref\n│       ├── GCF_000001405.40.fna\n│       ├── GCF_000001405.40.gtf\n```\n:::\n\n<br>\n\n## Getting started with Slurm batch jobs\n\nIn requesting _batch jobs_,\nwe are asking the Slurm scheduler to **run a script on a compute node**[^1].\nWhen doing so, we **stay in our current shell at our current node**\n(whether that's a login or compute node),\nand the script will run on a (different) compute node \"out of sight\".\nAlso, as we'll discuss in more detail below:\n\n- Script output that would normally be printed to screen (\"standard out\")\n  will end up in a \"log\" file\n- There are commands for e.g. _monitoring_ whether the job is\n  already/stillrunning, and _cancelling_ the job.\n\n[^1]: It _is_ also possible to directly submit a command or set of commands\n      using the `--wrap` option to `sbatch`, but we won't cover that here.\n\n<br>\n\n### The `sbatch` command\n\nWe use Slurm's **`sbatch` command to submit a batch job**.\nRecall from the Bash scripting session that we can run a Bash script as follows:\n\n```bash\nbash sandbox/printname.sh Jane Doe\n```\n```{.bash-out}\nFirst name: Jane\nLast name: Doe\n```\n\n:::{.callout-caution collapse=\"true\"}\n## Can't find yesterday's `printname.sh` script?\n\n- Open a new file in the `VS Code` editor\n  (&nbsp; {{< fa bars >}} &nbsp; => &nbsp; `File` &nbsp; => &nbsp; `New File`)\n  and save it as `printname.sh`\n- Copy the code below into the script:\n  \n```bash\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n```\n:::\n\nThe above command ran the script on our current node.\nTo instead submit the script to the Slurm queue,\nwe start by simply **replacing `bash` by `sbatch`**:\n\n```bash\nsbatch sandbox/printname.sh Jane Doe\n```\n``` {.bash-out}\nsrun: error: ERROR: Job invalid: Must specify account for job  \nsrun: error: Unable to allocate resources: Unspecified error\n```\n\nHowever, that didn't work.\nAs the error message \"_Must specify account for job_\" tries to tell us,\nwe need to indicate **which OSC Project** (or as Slurm puts it, \"account\")\nwe want to use for this compute job.\n\nTo specify the project/account,\nwe can use the `--account=` option followed by the OSC Project number:\n\n```bash\nsbatch --account=PAS0471 sandbox/printname.sh Jane Doe\n```\n```{.bash-out}\nSubmitted batch job 12431935\n```\n\nThis means that our job was successfully submitted \n(No further output will be printed to your screen - we'll talk more about that below).\nThe job has a **unique identifier** among all compute jobs by all users\nat OSC, and we can use this number to monitor and manage it.\nAll of us will therefore see a different job number pop up.\n\n<br>\n\n:::{.callout-note}\n#### `sbatch` options _and_ script arguments\n\nAs you perhaps noticed in the command above,\nwe can use `sbatch` options _and_ script arguments in one command,\nin the following order:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsbatch [sbatch-options] myscript.sh [script-arguments]\n```\n:::\n\n\nBut, depending on the details of the script itself, all combinations of using\n`sbatch` options and script arguments are possible:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS0471 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS0471 printname.sh Jane Doe  # Both sbatch option and script arguments\n```\n:::\n\n\nNot using the `--account` option, as in the first two examples above,\nis possible when we specify this option _inside the script_,\nas we'll see below.\n:::\n\n<br>\n\n### Adding `sbatch` options in scripts\n\nThe `--account=` option is just one of out of _many_ options we can use\nwhen reserving a compute job,\nbut is the only one that _always_ has to be specified\n(including for batch jobs and for Interactive Apps).\n\nDefaults exist for all other options,\nsuch as the amount of time (1 hour) and the number of cores (1).\nThese options are all specified in the same way for interactive and\nbatch jobs, and we'll dive into them below.\n\nInstead of specifying Slurm/`sbatch` options on the command-line when we submit\nthe script, we can also **add these options inside the script**.\n\nThis is handy because\neven though we have so far only seen the `account=` option,\nyou often want to specify several options.\nThat would lead to very long `sbatch` commands.\nAdditionally, it can be practical to store a script's typical Slurm options\nalong with the script itself, so you don't have to remember them.\n\nWe add the options in the script using another type of special comment line\nakin to the shebang line, marked by `#SBATCH`.\nThe equivalent of adding `--account=PAS0471` after `sbatch` on the command line\nis a line in  a script that reads `#SBATCH --account=PAS0471`.\n\nJust like the shebang line,\nthe `#SBATCH` line(s) should be at the top of the script.\nLet's add one such line to the `printname.sh` script,\nsuch that the first few lines read:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n#SBATCH --account=PAS0471\n\nset -ueo pipefail\n\n# (This is a partial script, don't run this directly in the terminal)\n```\n:::\n\n\nAfter having added this to the script,\nwe _can_ run our earlier `sbatch` command without options:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsbatch printname.sh Jane Doe\n```\n:::\n\n\n:::{.bash-out}\nSubmitted batch job 12431942\n:::\n\nAfter we submit the batch job, we **immediately get our prompt back**.\nEverything else (job queuing and running) will happen out of our immediate view.\nThis allows us to submit many jobs at the same time &mdash;\nwe don't have to wait for other jobs to finish (or even to start).\n\n:::{.callout-note}\n## `sbatch` option precedence\nAny `sbatch` option provided on the command line will override the equivalent\noption provided inside the script.\nThis is sensible: we can provide \"defaults\" inside the script,\nand change one or more of those when needed on the command line.\n:::\n\n:::{.callout-note}\n## Running a script with `#SBATCH` in other contexts\nBecause `#SBATCH` lines are special _comment_ lines,\nthey will simply be ignored and not throw any errors when you run a script that\ncontains them in other contexts: when not running them as a batch job at OSC,\nor even when running them on a computer without Slurm installed.\n:::\n\n<br>\n\n### Where does the output go?\n\nAbove, we saw that when we ran the `printname.sh` script directly,\nits output was printed to the screen,\nwhereas when we submitted it as a batch job,\nall that was sprinted to screen was `Submitted batch job 12431942`.\nSo where did our output go?\n\n**Our output ended up in a file** called `slurm-12431942.out`:\nthat is, `slurm-<job-number>.out`.\nSince each job number is unique to a given job,\nyour file would have a different number in its name.\nWe might call this type of file a **Slurm log file**.\n\n:::{.callout-caution collapse=\"true\"}\n## Any idea why we might not want batch job output printed to screen, even if we could? (Click to expand)\n\nThe power of submitting batch jobs is that you can submit many at once &mdash;\ne.g. one per sample, running the same script.\nIf the output from all those scripts ends up on your screen,\nthings become a big mess, _and_ you have no lasting record of what happened.\n:::\n\nLet's take a look at the contents of the Slurm log file with the `cat` command:\n\n```bash\n# (Replace the number in the file name with whatever you got! - check with 'ls')\ncat slurm-12431942.out\n```\n``` {.bash-out}\nFirst name: Jane  \nLast name: Doe\n```\n\nThis file simply contains the output of the script that was printed to screen \nwhen we ran it with `bash` &mdash; nothing more and nothing less.\n\nIt's important to conceptually distinguish between two overall types of output\nthat a script may have:\n\n- Output that is **printed to screen** when we directly run a script,\n  such as what was produced by our `echo` statements,\n  by any errors that may occur,\n  and possibly by a program that we run in the script.[^2]\n  As we saw, this output ends up in the **Slurm log file** when we submit\n  the script as a batch job.\n\n- Output of commands inside the script that we redirect to a file (`> myfile.txt`)\n  or that automatically goes to an output file rather than being printed to screen.\n  This type of output **will always end up in the very same files**\n  regardless of whether we run the script directly (with `bash`) or as a batch job\n  (with `sbatch`). \n\n[^2]: Technically, these are two different types of output,\n      as we briefly touch on below: \"standard output\" and \"standard error\".\n\n:::{.callout-tip}\n## The working directory stays the same\nBatch jobs start in the directory that they were submitted\nfrom: that is, your working directory remains the same.\n:::\n\n<br>\n\n## Monitoring batch jobs\n\n### A sleepy script for practice\n\nLet's use the following short script to practice monitoring and managing batch\nand other compute jobs.\nOpen a new file in the `VS Code` editor\n(&nbsp; {{< fa bars >}} &nbsp; => &nbsp; `File` &nbsp; => &nbsp; `New File`)\nand save it as `sandbox/sleep.sh`, then copy the following into it:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n#SBATCH --account=PAS0471\n\necho \"I will sleep for 30 seconds\" > sleep.txt\nsleep 30s\necho \"I'm awake!\"\n```\n:::\n\n\n:::{.exercise}\n\n### Your turn: Batch job output recap {-}\n\nPredict what would happen if you submit the `sleep.sh` script as a batch job \n(using `sbatch sandbox/sleep.sh`):\n\n1. How many output files will this batch job produce?\n2. What will be in each of those files?\n3. In which directory will the file(s) appear?\n4. In terms of output,\n   what would have been different if we had run the script directly,\n   i.e. using the command `bash sandbox/sleep.sh`?\n\nThen, can test your predictions by running the script.\n\n:::{.callout-tip collapse=\"true\"}\n## Solutions (Click to expand)\n\n1. The script will produce 2 files:\n    - `slurm-<job-number>.out`: The Slurm log file,\n       containing output that would have otherwise been printed to the screen\n    - `sleep.txt`: Containing the output that we redirected to this file in the script\n\n2. They will contain:\n    - `slurm-<job-number>.out`: _I'm awake!_\n    - `sleep.txt`: _\"I will sleep for 30 seconds\"_\n\n3. Both files will end up in your current working directory.\n   Slurm log files always go to the directory from which you submitted the job.\n   Slurm jobs also _run_ from the directory from which you submitted your job,\n   and since we redirected the output simply to `sleep.txt`,\n   that file was created in our working directory. \n\n4.  If we had run the script directly,\n    the `slept.txt` would have also been created with the same content,\n    but \"_All done!_\" would have been printed to the screen.\n:::\n:::\n\n<br>\n\n### Checking the status of our job\n\nAfter we submit a job, it may be initially be waiting to be allocated resources:\ni.e., it may be \"queued\" or \"pending\".\nThen, the job will start _running_, and at some point it will stop running,\neither because the script ran into and error, or because it completed.\n\nHow can we check the status of our batch job?\nWe can do so using the Slurm command **`squeue -u $USER -l`**, in which:\n\n- Our username is specified with the `-u` option\n  (without this, we would see _everyone's_ jobs)\n- We used the environment variable `$USER`\n  so that the very same code will work for everyone\n  (you can also simply type your username if that's shorter or easier).\n- We've added the `-l` (lowercase L, not the number 1!)\n  option to get more verbose (\"long\") output.\n\nLet's try that:\n\n```bash\nsqueue -u $USER -l\n```\n```{.bash-out}\nMon Aug 21 15:47:42 2023\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          23640814 condo-osu ondemand   jelmer  RUNNING       6:34   2:00:00      1 p0133\n```\n\nAfter a line with the date and time, and a header line,\nyou should see some information about a single compute job, as shown above:\n_this is the Interactive App job that runs VS Code_.\nThat's not a \"batch\" job, but it is a compute job, and all compute jobs are listed\nhere.\nIn the table, we can see the following pieces of information about each job:\n\n- `JOBID` &mdash; the job ID number,\n- `PARTITION` &mdash;\n  type of queue - here, we can tell it was submitted through OnDemand\n- The `NAME` of the job\n- The `USER` who submitted the job\n- The `STATE` of the job, which is usually either `PENDING` (queued) or `RUNNING`.\n  (As soon as a job finished, it will disappear from this list!)\n- `TIME` &mdash; for how long the job has been running (here as minutes:seconds)\n- The `TIME_LIMIT` &mdash; the amount of time you reserved for the job\n  (here as hours:minutes:seconds)\n- The number of `NODES` reserved for the job\n- `NODELIST(REASON)` &mdash;\n  When a job is running, this will show the ID of the node on which it is running.\n  When a job is pending, it will (sort of) say why it is pending.\n\nLet's also try this after submitting our `sleep.sh` script as a batch job:\n\n```bash\nsbatch sandbox/sleep.sh\n```\n``` {.bash-out}\nSubmitted batch job 12431945\n```\n\nWe _may_ be able to catch the `STATE` being `PENDING` before the job starts:\n\n```bash\nsqueue -u $USER -l\n```\n``` {.bash-out}\nMon Aug 21 15:48:26 2023\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          12520046 serial-40 sleep.sh   jelmer  PENDING       0:00   1:00:00      1 (None)\n          23640814 condo-osu ondemand   jelmer  RUNNING       7:12   2:00:00      1 p0133\n```\n\nBut soon enough it should say `RUNNING` in the `STATE` column:\n\n```sh\nsqueue -u $USER -l\n```\n``` {.bash-out}\nMon Aug 21 15:48:39 2023\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          12520046 condo-osu sleep.sh   jelmer  RUNNING       0:12   1:00:00      1 p0133\n          23640814 condo-osu ondemand   jelmer  RUNNING       8:15   2:00:00      1 p0133\n```\n\nThe script should finish after 30 seconds (our command was `sleep 30s`),\nafter which the job will immediately disappear from the `squeue` listing &mdash;\nonly pending and running jobs are shown: \n\n```bash\nsqueue -u $USER -l\n```\n``` {.bash-out}\nMon Aug 21 15:49:26 2023\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          23640814 condo-osu ondemand   jelmer  RUNNING       9:02   2:00:00      1 p0133\n```\n\nBut we need to check our output file(s) to see if our script ran _successfully_!\n\n```bash\ncat sleep.txt\n```\n``` {.bash-out}\nI will sleep for 30 seconds\n```\n\n```bash\ncat slurm-12520046.out\n```\n``` {.bash-out}\nI'm awake!\n```\n\n<br>\n\n### Cancelling jobs (and other monitoring/managing commands)\n\nSometimes, you want to cancel one or more jobs,\nbecause you realize you made a mistake\nin the script or you used the wrong input files.\nYou can do so using `scancel`:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nscancel 2979968        # Cancel job number 2979968\nscancel -u $USER       # Cancel all your jobs\n```\n:::\n\n\n:::{.callout-note}\n## Other job monitoring commands and options\n\n- Check only a specific job by specifying the job ID, e.g `2979968`:\n\n\n  ::: {.cell}\n  \n  ```{.bash .cell-code}\n  squeue -j 2979968\n  ```\n  :::\n\n\n- Only show running (not pending) jobs:\n  \n\n  ::: {.cell}\n  \n  ```{.bash .cell-code}\n  squeue -u $USER -t RUNNING\n  ```\n  :::\n\n\n- Update Slurm directives for a job that has already been submitted\n  (this can only been done _before_ the job has started running):\n\n\n  ::: {.cell}\n  \n  ```{.bash .cell-code}\n  scontrol update job=<jobID> timeLimit=5:00:00\n  ```\n  :::\n\n  \n- Hold and release a pending (queued) job,\n  e.g. when needing to update input file before it starts running:\n\n  ```bash\n  scontrol hold jobID        # Job won't start running until released\n  scontrol release jobID     # Job is free to start\n  ```\n\n- You can see more details about any running or finished jobs,\n  *including the amount of time it ran for*:\n  \n\n  ::: {.cell}\n  \n  ```{.bash .cell-code}\n  scontrol show job 2526085   # For job 2526085\n  \n  # UserId=jelmer(33227) GroupId=PAS0471(3773) MCS_label=N/A\n  # Priority=200005206 Nice=0 Account=pas0471 QOS=pitzer-default\n  # JobState=RUNNING Reason=None Dependency=(null)\n  # Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n  # RunTime=00:02:00 TimeLimit=01:00:00 TimeMin=N/A\n  # SubmitTime=2020-12-14T14:32:44 EligibleTime=2020-12-14T14:32:44\n  # AccrueTime=2020-12-14T14:32:44\n  # StartTime=2020-12-14T14:32:47 EndTime=2020-12-14T15:32:47 Deadline=N/A\n  # SuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-12-14T14:32:47\n  # Partition=serial-40core AllocNode:Sid=pitzer-login01:57954\n  # [...]\n  ```\n  :::\n\n:::\n\n<br>\n\n## Common `sbatch` options\n\nFirst off, note that many Slurm options have a corresponding long\n(`--account=PAS0471`) and short format (`-A PAS0471`),\nwhich can generally be used interchangeably.\nFor clarity, we'll stick to long format options here.\n\n### `--account`: The OSC project\n\nAs seen above. _Always_ specify the project when submitting a batch job.\n\n### `--time`: Time limit (\"wall time\")\n\nUse the `--time` option to specify the maximum amount of time your job will run for.\n**Your job gets killed as soon as it hits the specified time limit!**\n\n_Wall time_ is a term meant to distinguish it from, say \"core hours\":\nif a job runs for 2 hour and used 8 cores,\nthe wall time was 2 hours and the number of core hours was 2 x 8 = 16.\nSome notes:\n\n- You will only be charged for the time your job *actually used*,\n  not what you reserved.\n- The default time limit is 1 hour. Acceptable time formats include:\n  - `minutes` (e.g. `60` => 60 minutes)\n  - `hours:minutes:seconds` (e.g. `1:00:00` => 60 minutes)\n  - `days-hours` (e.g. `2-12` => two-and-a-half days)\n- For single-node jobs, up to 168 hours (7 days) can be requested.\n  If that's not enough, you can request access to the `longserial` queue\n  for jobs of up to 336 hours (14 days).\n\nAn example, where we ask for 1 hour:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n#SBATCH --time=1:00:00\n```\n:::\n\n\n:::{.callout-note}\n## When in doubt, ask for more time\nIf you are uncertain about how much time your job will take\n(i.e., how long it will take for your script / the program in your script\nto finish),\nthen ask for more or even much more time than you think you will need.\nThis is because queuing times are generally not long at OSC,\n_and_ because you won't be charged for reserved-but-not-used time.\n\nThat said, in general,\nthe \"bigger\" (more time, more cores, more memory) your job is,\nthe more likely it is that it will be pending for an appreciable amount of time.\nSmaller jobs (requesting up to a few hours and cores) will\n**almost always start running nearly instantly**.\nEven big jobs (requesting, say, a day or more and 10 or more cores)\nwill often do so, but during busy times, you might have to wait for a while.\n:::\n\n::: {.exercise}\n#### Your turn: exceed the time limit {-}\n\nModify the `sleep.sh` script\nto make it run longer than the time you request for it with `--time`.\n(Take into account that it does not seem to be possible to effectively request\na job that runs for less than 1 minute.)\n\nIf you succeed in exceeding the time limit, an error message will be printed &mdash;\nwhere do you think it will go?\nAfter waiting for the job to be killed after 60 seconds,\ncheck if you were correct and what the exact error message is.\n\n::: {.callout-tip collapse=\"true\"}\n#### Solution (Click to expand)\n\nThis script would do the trick,\nwhere we request 1 minute of walltime while we let the script sleep for 80 seconds:\n\n```bash\n#!/bin/bash\n#SBATCH --account=PAS0471\n#SBATCH --time=1\n\necho \"I will sleep for 80 seconds\" > sleep.txt\nsleep 80s\necho \"I'm awake!\"\n```\n\nThis would result in the following type of error:\n\n``` {.bash-out}\nslurmstepd: error: *** JOB 23641567 ON p0133 CANCELLED AT 2023-08-21T16:35:24 DUE TO TIME LIMIT ***\n```\n:::\n:::\n\n<br>\n\n### `--mem`: RAM memory\n\nUse the `--mem` option to specify the maximum amount of RAM (Random Access Memory)\nthat your job can use:\n\n- The default amount is 4 GB per core that you reserve.\n  **This is often enough**, so it is fairly common to omit the `--mem` option.\n- The default unit is MB (MegaBytes) &mdash; append `G` for GB\n  (i.e. `100` means 100 MB, `10G` means 10 GB).\n- Like with the time limit, your job gets killed when it hits the memory limit.\n  Whereas you get a very clear Slurm error message when you hit the time limit\n  (as seen in the exercise above),\n  hitting the memory limit can result in a variety of errors,\n  but look for keywords such as `Killed`, `Out of Memory` / `OOM`,\n  and `Core Dumped`, as well as actual \"dumped cores\" in your working dir\n  (large files with names like `core.<number>`, these can be deleted).\n\nFor example, to request 20 GB of RAM:\n\n```sh\n#!/bin/bash\n#SBATCH --mem=20G\n```\n\n<br>\n\n### Cores (& nodes and tasks)\n\nThere are several options to specify the number of nodes (≈ computers),\ncores, or \"tasks\" (processes). \nThese are separate but related options, and this is where things can get confusing!\n\n- Slurm for the most part uses \"**core**\" and \"**CPU**\" interchangeably[^3].\n  More generally, \"**thread**\" is *also* commonly used interchangeably \n  with core/CPU[^4].\n\n[^3]: Even though technically, one CPU often contains multiple cores.\n[^4]: Even though technically, one core often contains multiple threads.\n\n- Running a program that uses multiple threads/cores/CPUs (\"multi-threading\")\n  is common.\n  In such cases, specify the number of threads/cores/CPUs `n` with\n  **`--cpus-per-task=n`**\n  (and keep `--nodes` and `--ntasks` at their defaults of 1).\n  \n  The program you're running may have an argument like `--cores` or `--threads`,\n  which you should then set to `n` as well.\n  \n:::{.callout-note}\n## Uncommon cases\n\n- Only ask for **more than one node** when a program is parallelized with\n  e.g. \"MPI\", which is uncommon in bioinformatics.\n- For jobs with multiple processes (tasks),\n  use `--ntasks=n` or `--ntasks-per-node=n` &mdash; also quite rare!\n:::  \n\n| Resource/use                  | short    | long                    | default\n|-------------------------------|----------|-------------------------|:--------:| \n| **Nr. of cores/CPUs/threads (per task)**    | `-c 1`   | `--cpus-per-task=1`     | 1\n| Nr. of \"tasks\" (processes) | `-n 1`   | `--ntasks=1`            | 1\n| Nr. of tasks per node      | -        | `--ntasks-per-node=1`   | 1\n| Nr. of nodes               | `-N 1`   | `--nodes=1`             | 1\n\nAn example, where we ask for 2 CPUs/cores/threads:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n```\n:::\n\n\n<br>\n\n### `--output`: Slurm log files\n\nAs we saw above, by default, all output from a script that would normally\nbe printed to screen will end up in a Slurm log file when we submit the script\nas a batch job.\nThis file will be created in the directory from which you submitted the script,\nand will be called `slurm-<job-number>.out`, e.g. `slurm-12431942.out`.\n\nBut it is possible to change the name of this file.\nFor instance, it can be useful to include the **name of the program**\nthat the script runs,\nso that it's easier to recognize this file later.\n\nWe can do this with the `--output` option,\ne.g. `--output=slurm-fastqc.out` if we were running FastQC.\n\nHowever,\nyou'll generally want to keep the batch job number in the file name too[^6].\nSince we won't know the batch job number in advance, we need a trick here &mdash;\nand that is to use\n**`%j`, which represents the batch job number**:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n```\n:::\n\n\n[^6]: For instance, we might be running the FastQC script multiple times,\n      and otherwise those would all have the same name and be overwritten.\n      \n:::{.callout-note}\n## `stdout` and `stderr`\n\nBy default, two output streams \"standard output\" (`stdout`) and\n\"standard error\" (`stderr`) are printed to screen and therefore also\nboth end up in the same Slurm log file,\nbut it is possible to separate them into different files.\n\nBecause `stderr`, as you might have guessed, often contains error messages,\nit could be useful to have those in a separate file.\nYou can make that happen with the `--error` option,\ne.g. `--error=slurm-fastqc-%j.err`.\n\nHowever, reality is more messy:\nsome programs print their main output not to a file but to standard out,\nand their logging output, errors and regular messages alike, to standard error.\nYet other programs use `stdout` or `stderr` for _all_ messages.\n\n**I therefore usually only specify `--output`, such that both streams**\n**end up in that file.**\n:::\n\n<br>\n\n## At-home reading: `sbatch` option overview & interactive jobs {-}\n\n### Table with `sbatch` options {-}\n\nThis includes all the discussed options, and a couple more useful ones:\n\n| Resource/use                  | short      | long                 | default\n|-------------------------------|------------|----------------------|:---------:|\n| Project to be billed          | `-A PAS0471` | `--account=PAS0471`    | _N/A_\n| Time limit                    | `-t 4:00:00` | `--time=4:00:00`      | 1:00:00\n| Nr of nodes                   | `-N 1`       | `--nodes=1`            | 1\n| Nr of cores                   | `-c 1`       | `--cpus-per-task=1`    | 1\n| Nr of \"tasks\" (processes)     | `-n 1`      | `--ntasks=1`           | 1\n| Nr of tasks per node          | -          | `--ntasks-per-node`   | 1\n| Memory limit per node         | -          | `--mem=4G`             | *(4G)*\n| Log output file (%j = job number)    | `-o`       |  `--output=slurm-fastqc-%j.out`\n| Error output (*stderr*)              | `-e`       | `--error=slurm-fastqc-%j.err`\n| Job name (displayed in the queue)    | -        | `--job-name=fastqc`\n| Partition (=queue type)              | -        | `--partition=longserial` <br> `--partition=hugemem`\n| Get email when job starts, ends, fails, <br> or all of the above | -        | `--mail-type=START` <br> `--mail-type=END` <br> `--mail-type=FAIL` <br> `--mail-type=ALL`\n| Let job begin at/after specific time | -        | `--begin=2021-02-01T12:00:00`\n| Let job begin after other job is done | -      | `--dependency=afterany:123456`\n\n<br>\n\n### Interactive shell jobs {-}\n\nInteractive shell jobs will grant you interactive shell access on a compute node.\nWorking in an interactive shell job is operationally identical to working on\na login node as we've been doing so far, but\n**the difference is that it's now okay to use significant computing resources**.\n(How much and for how long depends on what you reserve.)\n\n#### Using `srun` {-}\n\nA couple of different commands can be used to start an interactive shell job.\nI prefer the general `srun` command[^1],\nwhich we can use with `--pty /bin/bash` added to get an interactive Bash shell.\n\n[^1]: Other options: `salloc` works almost identically to `srun`,\n      whereas `sinteractive` is an OSC convenience wrapper but with more\n      limited options.\n     \n\n::: {.cell}\n\n```{.bash .cell-code}\nsrun --account=PAS0471 --pty /bin/bash\n```\n:::\n\n\n:::{.bash-out}\nsrun: job 12431932 queued and waiting for resources  \nsrun: job 12431932 has been allocated resources\n\n[...regular login info, such as quota, not shown...]\n\n[jelmer@p0133 PAS0471]$\n:::\n\nThere we go! First some Slurm scheduling info was printed to screen:\ninitially, the job was queued, and then it was \"allocated resources\":\nthat is, computing resources such as a compute node were reserved for the job.\nAfter that:\n\n- The job starts and because we've reserved an _interactive_ shell job,\n  a new Bash shell is initiated:\n  for that reason, we get to see our regular login info once again.\n\n- We have now moved to the **compute node** at which our interactive job is running,\n  so you should have a different `p` number in your prompt\n  (And if you were on a login node before\n  -but this is never the case if you are running VS Code through OnDemand-,\n  your prompt switched from something like `[jelmer@pitzer-login04 PAS0471]$`).\n",
    "supporting": [
      "A10_slurm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}